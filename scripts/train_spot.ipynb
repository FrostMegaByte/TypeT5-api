{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from typing import *\n",
    "\n",
    "from spot.utils import proj_root, get_data_dir\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = get_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment configurations\n",
    "\n",
    "import torch\n",
    "\n",
    "from spot.data import (\n",
    "    SrcDataset,\n",
    "    get_dataset_name,\n",
    "    load_src_datasets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from spot.model import CtxArgs, DecodingArgs, ModelSPOT, ModelWrapper\n",
    "from copy import copy\n",
    "from spot.train import TrainingConfig, TypeCheckArgs\n",
    "\n",
    "config = TrainingConfig(quicktest=False, all_labels=True)\n",
    "train_R1: bool = True\n",
    "load_R0: bool = True\n",
    "load_critic: bool = False\n",
    "gpu_id = 0\n",
    "TypeCheckSettings.temp_path = f\"GPU-{gpu_id}\"\n",
    "\n",
    "project_name = \"test-SPOT\" if config.quicktest else \"SPOT\"\n",
    "train_ctx_args = config.train_ctx_args()\n",
    "tc_args = TypeCheckArgs(check_in_isolation=config.check_in_isolation)\n",
    "\n",
    "max_tokens_per_file = config.ctx_size\n",
    "dec_args = DecodingArgs(\n",
    "    sampling_max_tokens=8 * max_tokens_per_file,\n",
    "    ctx_args=config.dec_ctx_args(),\n",
    "    max_workers=20,\n",
    ")\n",
    "\n",
    "\n",
    "datasets_name = get_dataset_name(\n",
    "    drop_comments=config.drop_comments,\n",
    "    all_labels=config.all_labels,\n",
    ")\n",
    "\n",
    "r0_model_name = \"R0-model--\" + config.as_name()\n",
    "\n",
    "src_datasets = load_src_datasets(\n",
    "    datadir,\n",
    "    datasets_name,\n",
    "    data_reduction=config.data_reduction,\n",
    "    quicktest=config.quicktest,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from spot.train import ModelTrainingArgs, train_spot_model, TypeCheckArgs\n",
    "import wandb\n",
    "\n",
    "train_args = ModelTrainingArgs(\n",
    "    train_ctx_args,\n",
    "    dec_args,\n",
    "    train_max_tokens=max_tokens_per_file,\n",
    "    eval_max_tokens=2 * max_tokens_per_file,\n",
    "    max_epochs=2,\n",
    "    tc_args=tc_args,\n",
    ")\n",
    "\n",
    "if not load_R0:\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=r0_model_name,\n",
    "        config=config.as_dict(),\n",
    "        dir=str(datadir),\n",
    "    )\n",
    "    r0_wrapper, r0_extra = train_spot_model(\n",
    "        src_datasets,\n",
    "        r0_model_name,\n",
    "        train_args=train_args,\n",
    "        record_batches=train_R1,\n",
    "        gpus=[gpu_id],\n",
    "        quicktest=config.quicktest,\n",
    "        use_small_model=config.use_small_model,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "from spot.utils import pickle_load, pickle_dump\n",
    "\n",
    "r0_wrapper = ModelWrapper.from_pretrained(\n",
    "    datadir / f\"checkpoints/lit-saved/{r0_model_name}\"\n",
    ")\n",
    "# if train_R1:\n",
    "    # r0_extra = pickle_load(datadir / f\"checkpoints/lit-saved/{r0_model_name}/extra.pkl\")\n",
    "    # r1_src_datasets: dict[str, SrcDataset] = r0_extra[\"R1-src_datasets\"]\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "r0_wrapper.to(device)\n",
    "r0_wrapper.args.do_sample = False\n",
    "print(r0_wrapper.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test DAgger\n",
    "from spot.dagger import DAggerModel\n",
    "from spot.utils import print_limited, display, pretty_print_dict\n",
    "\n",
    "ex_data = src_datasets[\"test\"][1:10]\n",
    "dmodel = DAggerModel(r0_wrapper)\n",
    "\n",
    "metrics = dmodel.eval_on_data(src_datasets[\"test\"])\n",
    "pretty_print_dict(metrics)\n",
    "\n",
    "display(dmodel.t_logger.as_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the DAgger model\n",
    "from spot.dagger import DAggerModel, DAggerArgs\n",
    "from spot.utils import display, pretty_print_dict\n",
    "\n",
    "dmodel = DAggerModel(r0_wrapper)\n",
    "dmodel.t_logger.clear()\n",
    "dmodel.train_on_data(src_datasets, DAggerArgs())\n",
    "display(dmodel.t_logger.as_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dmodel.t_logger.as_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.train import evaluate_model\n",
    "from spot.utils import PickleCache\n",
    "from spot.visualization import display_persist, dict_widget\n",
    "\n",
    "r0_cache = PickleCache(datadir / f\"checkpoints/lit-saved/{r0_model_name}/eval_cache\")\n",
    "r0_eval = evaluate_model(\n",
    "    r0_wrapper,\n",
    "    None,\n",
    "    src_datasets[\"test\"],\n",
    "    eval_cache=r0_cache,\n",
    "    tc_args=train_args.tc_args,\n",
    ")\n",
    "r0_accs = r0_eval[0][1].accuracies\n",
    "display_persist(dict_widget(r0_accs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close wandb\n",
    "from spot.utils import pretty_show_dict\n",
    "from spot.visualization import string_to_html\n",
    "import wandb\n",
    "\n",
    "\n",
    "def wandb_string(s: str):\n",
    "    return wandb.Html(string_to_html(s))\n",
    "\n",
    "\n",
    "if not load_R0:\n",
    "    for i, e in enumerate(r0_eval):\n",
    "        wandb.log({f\"test/R{i}\": wandb_string(pretty_show_dict(e[1].accuracies))})\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the code with inlined predictions as HTML\n",
    "\n",
    "from spot.visualization import export_preds_on_code, display_persist, proj_root\n",
    "\n",
    "export_preds = False\n",
    "\n",
    "if export_preds:\n",
    "    pr = r0_eval[0][1]\n",
    "    sub_ids = range(0, len(pr.chunks), 10)\n",
    "    export_preds_on_code(\n",
    "        pr.chunks[sub_ids],\n",
    "        [pr.predictions[i] for i in sub_ids],\n",
    "        {},\n",
    "        export_to=proj_root() / \"R0_predictions\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the critic\n",
    "from spot.critic import (\n",
    "    CriticModel,\n",
    "    ModelSPOT,\n",
    "    train_critic_model,\n",
    "    CriticTrainArgs,\n",
    "    get_critic_name,\n",
    ")\n",
    "from spot.utils import pickle_load, run_long_task, PickleCache\n",
    "from spot.train import R1_srcs_from_extra, R1_srcs_from_model\n",
    "import wandb\n",
    "\n",
    "critic_new_data = True\n",
    "critic_no_feedback = False\n",
    "critic_name = get_critic_name(critic_no_feedback, critic_new_data, config)\n",
    "\n",
    "with run_long_task(f\"Training Critic: {critic_name}\", notify=not load_critic):\n",
    "    critic_train_args = CriticTrainArgs(\n",
    "        ctx_args=train_ctx_args,\n",
    "        train_max_tokens=max_tokens_per_file,\n",
    "        eval_max_tokens=2 * max_tokens_per_file,\n",
    "        max_epochs=1,\n",
    "    )\n",
    "\n",
    "    critic_tc_args = tc_args._replace(no_feedback=critic_no_feedback)\n",
    "    critic_cache = PickleCache(\n",
    "        datadir / f\"checkpoints/lit-saved/CriticData-{critic_name}\"\n",
    "    )\n",
    "    # critic_cache.remove(\"src_datasets\")\n",
    "    critic_src_datasets: dict[str, SrcDataset]\n",
    "\n",
    "    if critic_new_data:\n",
    "        # use sampling to increase example diversity\n",
    "        r0_wrapper.args.do_sample = True\n",
    "        r0_wrapper.args.top_p = 0.9\n",
    "\n",
    "    critic_src_datasets = critic_cache.cached(\n",
    "        \"src_datasets\",\n",
    "        lambda: {\n",
    "            k: v.inline_predictions(as_comment=False)\n",
    "            for k, v in (\n",
    "                R1_srcs_from_model(\n",
    "                    r0_wrapper,\n",
    "                    src_datasets,\n",
    "                    critic_tc_args,\n",
    "                )\n",
    "                if critic_new_data\n",
    "                else R1_srcs_from_extra(\n",
    "                    r0_wrapper,\n",
    "                    src_datasets,\n",
    "                    extra=pickle_load(\n",
    "                        datadir / f\"checkpoints/lit-saved/{r0_model_name}/extra.pkl\"\n",
    "                    ),\n",
    "                    tc_args=critic_tc_args,\n",
    "                )\n",
    "            ).items()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if not load_critic:\n",
    "        wandb.init(\n",
    "            project=project_name,\n",
    "            name=critic_name,\n",
    "            config=config.as_dict(),\n",
    "            dir=str(datadir),\n",
    "        )\n",
    "        critic, critic_extra = train_critic_model(\n",
    "            critic_src_datasets,\n",
    "            critic_train_args,\n",
    "            critic_name,\n",
    "            gpus=[gpu_id],\n",
    "            quicktest=config.quicktest,\n",
    "            use_early_stop=False,\n",
    "            use_small_model=config.use_small_model,\n",
    "        )\n",
    "        # critic.save_pretrained(\"CriticSaved\")\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained critic\n",
    "from spot.utils import pickle_load, pickle_dump\n",
    "from spot.critic import CriticModel\n",
    "\n",
    "critic = CriticModel.load(datadir / f\"checkpoints/lit-saved/{critic_name}\")\n",
    "if train_R1 and (\"r1_src_datasets\" not in globals()):\n",
    "    r0_extra = pickle_load(datadir / f\"checkpoints/lit-saved/{r0_model_name}/extra.pkl\")\n",
    "    r1_src_datasets: dict[str, SrcDataset] = r0_extra[\"R1-src_datasets\"]\n",
    "\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "critic.to(device)\n",
    "print(\"Critic loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show critic performance\n",
    "\n",
    "from spot.visualization import visualize_preds_on_code, pretty_print_dict\n",
    "\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "critic.to(device)\n",
    "r1_testset = critic_src_datasets[\"test\"]\n",
    "critic_eval = critic.eval_on_src_dataset(\n",
    "    r1_testset, train_ctx_args, dec_args.sampling_max_tokens\n",
    ")\n",
    "nicer_preds = [[f\"{x:.1%}\" for x in xs] for xs in critic_eval[1]]\n",
    "pretty_print_dict(critic_eval[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The performance achieved by always predicting true or random values\n",
    "\n",
    "from spot.utils import not_none, pretty_print_dict\n",
    "from spot.type_check import normalize_type\n",
    "from spot.critic import CriticModel\n",
    "import random\n",
    "\n",
    "\n",
    "def dummy_performance(dataset: SrcDataset, pred_f):\n",
    "    targets = list[bool]()\n",
    "    for s in dataset.all_srcs:\n",
    "        for p, t in zip(not_none(s.prev_types).values(), s.types):\n",
    "            targets.append(normalize_type(t) == normalize_type(p))\n",
    "\n",
    "    preds = [pred_f() for _ in range(len(targets))]\n",
    "    return CriticModel.compute_metrics(preds, targets)\n",
    "\n",
    "\n",
    "pretty_print_dict(dummy_performance(r1_testset, lambda: True))\n",
    "pretty_print_dict(dummy_performance(r1_testset, lambda: random.choice([True, False])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import DefaultTokenizer, decode_tokens, np\n",
    "\n",
    "\n",
    "def chunk_has_fdbk(tks):\n",
    "    return \"/* error:\" in decode_tokens(tks)\n",
    "\n",
    "\n",
    "test_chunks = r1_src_datasets[\"test\"].to_chunks(DefaultTokenizer, dec_args.ctx_args)\n",
    "fraction_chunks_with_fdbk = np.mean(\n",
    "    [chunk_has_fdbk(tks) for tks in test_chunks.data[\"input_ids\"]]\n",
    ")\n",
    "print(\"Fraction of chunks with feedback:\", fraction_chunks_with_fdbk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking mypy feedbacks\n",
    "from spot.visualization import show_feedback_stats\n",
    "\n",
    "if train_R1:\n",
    "    error_groups = show_feedback_stats(r1_src_datasets[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize feedback samples\n",
    "\n",
    "from spot.utils import seq_flatten, add_line_numbers\n",
    "from spot.visualization import code_inline_type_masks, visualize_sequence, display\n",
    "\n",
    "\n",
    "if train_R1:\n",
    "    to_display = []\n",
    "    for xs in error_groups[\"return-value\"]:  # seq_flatten(error_groups.values()):\n",
    "        src = xs[1]\n",
    "        code = code_inline_type_masks(src.origin_code, src.types)\n",
    "        to_display.append(\n",
    "            f\"feedback: {xs[0]}\\n\" + \"=========code=========\\n\" + add_line_numbers(code)\n",
    "        )\n",
    "    if len(to_display) > 0:\n",
    "        display(visualize_sequence(to_display))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R1 training\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from spot.data import SrcDataset, get_dataset_name\n",
    "from spot.model import CtxArgs, DecodingArgs, ModelSPOT, ModelWrapper\n",
    "\n",
    "load_R1 = False\n",
    "r1_model_name = \"R1-model--\" + config.as_name()\n",
    "\n",
    "if not load_R1:\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=r1_model_name,\n",
    "        config=config.as_dict(),\n",
    "        dir=str(datadir),\n",
    "    )\n",
    "\n",
    "    r1_train_args = copy(train_args)\n",
    "    r1_train_args.max_epochs = 1\n",
    "\n",
    "    r1_wrapper, r1_extra = train_spot_model(\n",
    "        r1_src_datasets,\n",
    "        r1_model_name,\n",
    "        train_args=r1_train_args,\n",
    "        gpus=[gpu_id],\n",
    "        record_batches=False,\n",
    "        quicktest=config.quicktest,\n",
    "        use_early_stop=False,\n",
    "        use_small_model=config.use_small_model,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model and evaluate\n",
    "from spot.train import evaluate_model\n",
    "from spot.visualization import visualize_dicts\n",
    "\n",
    "r1_wrapper = ModelWrapper.from_pretrained(\n",
    "    datadir / f\"checkpoints/lit-saved/{r1_model_name}\"\n",
    ")\n",
    "r1_wrapper.to(device)\n",
    "\n",
    "r1_cache = PickleCache(datadir / f\"checkpoints/lit-saved/{r1_model_name}/eval_cache\")\n",
    "r1_cache.clear()\n",
    "r1_eval = evaluate_model(\n",
    "    r0_wrapper,\n",
    "    r1_wrapper,\n",
    "    src_datasets[\"test\"],\n",
    "    tc_args=tc_args,\n",
    "    eval_cache=r1_cache,\n",
    ")\n",
    "visualize_dicts([x[1].accuracies for x in r1_eval])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.visualization import export_preds_on_code, display_persist, proj_root\n",
    "\n",
    "eval_to_viz = r1_eval[1][1]\n",
    "sub_ids = range(0, len(eval_to_viz.chunks), 10)\n",
    "export_preds_on_code(\n",
    "    eval_to_viz.chunks[sub_ids],\n",
    "    [eval_to_viz.predictions[i] for i in sub_ids],\n",
    "    {},\n",
    "    export_to=proj_root() / \"caches/R1_predictions\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.visualization import visualize_conf_matrix\n",
    "\n",
    "visualize_conf_matrix({n: x[1] for n, x in zip([\"R0\", \"R1\"], r1_eval)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.utils import pretty_show_dict\n",
    "\n",
    "if not load_R1:\n",
    "    for i, e in enumerate(r1_eval):\n",
    "        wandb.log({f\"test/R{i}\": wandb_string(pretty_show_dict(e[1].accuracies))})\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "from spot.visualization import visualize_preds_on_code\n",
    "\n",
    "round = 1\n",
    "pred_dataset = r1_eval[round][1].chunks\n",
    "visualize_preds_on_code(pred_dataset, r1_eval[round][1].predictions, dict())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
