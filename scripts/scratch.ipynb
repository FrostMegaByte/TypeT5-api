{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def gen_stub(m, rm_comments=..., rm_imports=...): ...\n",
      "def remove_empty_lines(m): ...\n",
      "OMIT = ...\n",
      "T = TypeVar(\"T\")\n",
      "U = typing.TypeVar(\"U\")\n",
      "@dataclass\n",
      "class ClassNamespace:\n",
      "    all_elems: ... = ...\n",
      "    declared_elems: ... = ...\n",
      "class StubGenerator(cst.CSTTransformer):\n",
      "    def __init__(self): ...\n",
      "    def register_elem(self, name, declared): ...\n",
      "    def visit_ClassDef(self, node): ...\n",
      "    def leave_ClassDef(self, node, updated): ...\n",
      "    def leave_FunctionDef(self, node, updated): ...\n",
      "    def leave_Annotation(self, node, updated): ...\n",
      "    def leave_Param(self, node, updated): ...\n",
      "    def leave_AnnAssign(self, node, updated): ...\n",
      "    def leave_Assign(self, node, updated): ...\n",
      "    def leave_Attribute(self, node, updated): ...\n",
      "    ns_stack: ...\n",
      "class EmptyLineRemove(cst.CSTTransformer):\n",
      "    def on_leave(self, node, updated): ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, stub_from_module\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''\n",
    "from .utils import *\n",
    "from .data import remove_comments, remove_imports\n",
    "import spot\n",
    "\n",
    "\n",
    "def gen_stub(m: cst.Module, rm_comments=True, rm_imports=True) -> cst.Module:\n",
    "    \"\"\"Removes all comments and docstrings.\"\"\"\n",
    "    if rm_comments:\n",
    "        m = remove_comments(m)\n",
    "    if rm_imports:\n",
    "        m, _ = remove_imports(m)\n",
    "    m = m.visit(StubGenerator())\n",
    "    m = remove_empty_lines(m)\n",
    "    spot.fly.attach(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "def remove_empty_lines(m: cst.Module) -> cst.Module:\n",
    "    m = m.visit(EmptyLineRemove())\n",
    "    return m\n",
    "\n",
    "\n",
    "OMIT = cst.SimpleStatementSuite([cst.Expr(cst.Ellipsis())])\n",
    "T = TypeVar(\"T\")\n",
    "U = typing.TypeVar(\"U\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClassNamespace:\n",
    "    all_elems: set[str] = field(default_factory=set)\n",
    "    declared_elems: set[str] = field(default_factory=set)\n",
    "\n",
    "\n",
    "class StubGenerator(cst.CSTTransformer):\n",
    "    \"\"\"Generate a stub module from a Python module.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ns_stack = list[ClassNamespace]()\n",
    "        self.ns_stack = 1\n",
    "\n",
    "    def register_elem(self, name: str, declared: bool):\n",
    "        if self.ns_stack:\n",
    "            s = self.ns_stack[-1]\n",
    "            s.all_elems.add(name)\n",
    "            if declared:\n",
    "                s.declared_elems.add(name)\n",
    "\n",
    "    def visit_ClassDef(self, node: cst.ClassDef):\n",
    "        self.ns_stack.append(ClassNamespace())\n",
    "\n",
    "    def leave_ClassDef(self, node, updated: cst.ClassDef):\n",
    "        s = self.ns_stack.pop()\n",
    "        to_declare = s.all_elems.difference(s.declared_elems)\n",
    "        if to_declare:\n",
    "            more_stmts = [cst.parse_statement(f\"{n}: ...\") for n in to_declare]\n",
    "            new_stmts = list(updated.body.body) + more_stmts\n",
    "            updated = updated.with_changes(\n",
    "                body=updated.body.with_changes(body=new_stmts)\n",
    "            )\n",
    "        return updated\n",
    "\n",
    "    def leave_FunctionDef(self, node, updated: cst.FunctionDef):\n",
    "        self.register_elem(updated.name.value, True)\n",
    "        return updated.with_changes(body=OMIT, returns=None)\n",
    "\n",
    "    def leave_Annotation(self, node, updated: cst.Annotation):\n",
    "        return updated.with_changes(annotation=cst.Ellipsis())\n",
    "\n",
    "    def leave_Param(self, node, updated: cst.Param):\n",
    "        if updated.default is not None:\n",
    "            updated = updated.with_changes(default=cst.Ellipsis())\n",
    "        return updated.with_changes(annotation=None)\n",
    "\n",
    "    def leave_AnnAssign(self, node, updated: cst.AnnAssign):\n",
    "        if updated.value is not None:\n",
    "            updated = updated.with_changes(value=cst.Ellipsis())\n",
    "        return updated\n",
    "\n",
    "    def leave_Assign(self, node, updated: cst.AnnAssign):\n",
    "        return updated.with_changes(value=cst.Ellipsis())\n",
    "\n",
    "    def leave_Attribute(self, node, updated: cst.Assign):\n",
    "        match updated:\n",
    "            case cst.Attribute(\n",
    "                value=cst.Name(value=\"self\"),\n",
    "                attr=cst.Name(value=elem_name),\n",
    "            ):\n",
    "                self.register_elem(elem_name, False)\n",
    "        return updated\n",
    "\n",
    "\n",
    "class EmptyLineRemove(cst.CSTTransformer):\n",
    "    def on_leave(self, node, updated):\n",
    "        if hasattr(updated, \"leading_lines\") and updated.leading_lines:\n",
    "            return updated.with_changes(leading_lines=[])\n",
    "        return updated\n",
    "\n",
    "'''\n",
    "\n",
    "ex_m = cst.parse_module(ex_code)\n",
    "print(stub_from_module(ex_m).code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "    body=[\n",
       "        SimpleStatementLine(\n",
       "            body=[\n",
       "                ImportFrom(\n",
       "                    module=Name(\n",
       "                        value='utils',\n",
       "                        lpar=[],\n",
       "                        rpar=[],\n",
       "                    ),\n",
       "                    names=[\n",
       "                        ImportAlias(\n",
       "                            name=Name(\n",
       "                                value='x',\n",
       "                                lpar=[],\n",
       "                                rpar=[],\n",
       "                            ),\n",
       "                            asname=None,\n",
       "                            comma=Comma(\n",
       "                                whitespace_before=SimpleWhitespace(\n",
       "                                    value='',\n",
       "                                ),\n",
       "                                whitespace_after=SimpleWhitespace(\n",
       "                                    value=' ',\n",
       "                                ),\n",
       "                            ),\n",
       "                        ),\n",
       "                        ImportAlias(\n",
       "                            name=Name(\n",
       "                                value='y',\n",
       "                                lpar=[],\n",
       "                                rpar=[],\n",
       "                            ),\n",
       "                            asname=AsName(\n",
       "                                name=Name(\n",
       "                                    value='z',\n",
       "                                    lpar=[],\n",
       "                                    rpar=[],\n",
       "                                ),\n",
       "                                whitespace_before_as=SimpleWhitespace(\n",
       "                                    value=' ',\n",
       "                                ),\n",
       "                                whitespace_after_as=SimpleWhitespace(\n",
       "                                    value=' ',\n",
       "                                ),\n",
       "                            ),\n",
       "                            comma=MaybeSentinel.DEFAULT,\n",
       "                        ),\n",
       "                    ],\n",
       "                    relative=[],\n",
       "                    lpar=None,\n",
       "                    rpar=None,\n",
       "                    semicolon=MaybeSentinel.DEFAULT,\n",
       "                    whitespace_after_from=SimpleWhitespace(\n",
       "                        value=' ',\n",
       "                    ),\n",
       "                    whitespace_before_import=SimpleWhitespace(\n",
       "                        value=' ',\n",
       "                    ),\n",
       "                    whitespace_after_import=SimpleWhitespace(\n",
       "                        value=' ',\n",
       "                    ),\n",
       "                ),\n",
       "            ],\n",
       "            leading_lines=[],\n",
       "            trailing_whitespace=TrailingWhitespace(\n",
       "                whitespace=SimpleWhitespace(\n",
       "                    value='',\n",
       "                ),\n",
       "                comment=None,\n",
       "                newline=Newline(\n",
       "                    value=None,\n",
       "                ),\n",
       "            ),\n",
       "        ),\n",
       "    ],\n",
       "    header=[\n",
       "        EmptyLine(\n",
       "            indent=True,\n",
       "            whitespace=SimpleWhitespace(\n",
       "                value='',\n",
       "            ),\n",
       "            comment=None,\n",
       "            newline=Newline(\n",
       "                value=None,\n",
       "            ),\n",
       "        ),\n",
       "    ],\n",
       "    footer=[],\n",
       "    encoding='utf-8',\n",
       "    default_indent='    ',\n",
       "    default_newline='\\n',\n",
       "    has_trailing_newline=True,\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_code = '''\n",
    "from utils import x, y as z\n",
    "'''\n",
    "\n",
    "cst.parse_module(import_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests.test_static_analysis/test_import_normalization\n",
      "\t spot.static_analysis/to_abs_import_path \n",
      "tests.test_static_analysis/test_namespace_resolution\n",
      "\t spot.static_analysis/ModuleNamespace.from_modules \n",
      "\t spot.static_analysis/ModuleNamespace.resolve_path   (maybe)\n",
      "tests.test_static_analysis/test_usage_analysis\n",
      "\t spot.static_analysis/PythonProject.from_modules \n",
      "\t spot.static_analysis/PythonModule.from_cst \n",
      "\t spot.static_analysis/UsageAnalysis.__init__ \n",
      "\t spot.static_analysis/ProjectPath.from_str \n",
      "\t spot.utils/assert_eq \n"
     ]
    }
   ],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).caller2callees.items():\n",
    "    if caller.module == \"tests.test_static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.callee, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.tokenized_src import PreprocessArgs, proj_root\n",
    "from spot.function_dataset import repo_to_tk_srcs, dataset_from_repos\n",
    "\n",
    "srcs = repo_to_tk_srcs(proj_root(), PreprocessArgs(drop_env_types=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "map srcs_to_chunks: 100%|██████████| 329/329 [00:00<00:00, 808.49it/s] \n",
      "verify_labels: 100%|██████████| 334/334 [00:00<00:00, 26409.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from spot.data import SrcDataset, CtxArgs\n",
    "\n",
    "sdata = SrcDataset(proj_root(), srcs)\n",
    "ctx_args = CtxArgs(1024, 128, 256, 512)\n",
    "cdata = sdata.to_chunks(ctx_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= file: tests.test_type_env/test_annotation_collection ========\n",
      "# spot.type_env\n",
      "def collect_annots_info(code: cst.Module | cst.MetadataWrapper) -> list[AnnotInfo]:\n",
      "    collector = AnnotCollector()\n",
      "    if not isinstance(code, cst.MetadataWrapper):\n",
      "        code = cst.MetadataWrapper(code)\n",
      "    code.visit(collector)\n",
      "    return collector.annots_info\n",
      "# spot.type_env\n",
      "def annot_path(*segs: str) -> AnnotPath:\n",
      "    return AnnotPath(plist(segs, reverse=True))\n",
      "# spot.utils\n",
      "def read_file(path) -> str:\n",
      "    with open(path, \"r\") as f:\n",
      "        return f.read()\n",
      "# tests.test_type_env\n",
      "def test_annotation_collection():\n",
      "    parsed = cst.parse_module(read_file(\"data/code/env_code_2.py\"))\n",
      "    annots = collect_annots_info(parsed)\n",
      "    annot_paths = [(a.path, a.cat) for a in annots]\n",
      "    correct_annot_paths: <mask> = [\n",
      "        (annot_path(\"fib\", \"n\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"fib\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"foo\", \"bar\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"foo\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"Bar\", \"z\"), AnnotCat.ClassAtribute),\n",
      "        (annot_path(\"Bar\", \"w\"), AnnotCat.ClassAtribute),\n",
      "        (annot_path(\"Bar\", \"__init__\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"Bar\", \"__init__\", \"x\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"Bar\", \"__init__\", \"self.x\"), AnnotCat.ClassAtribute),\n",
      "        (annot_path(\"Bar\", \"__init__\", \"self.y\"), AnnotCat.ClassAtribute),\n",
      "        (annot_path(\"Bar\", \"reset\", \"w0\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"Bar\", \"reset\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"Bar\", \"foo\", \"z\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"Bar\", \"foo\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"bar\"), AnnotCat.GlobalVar),\n",
      "    ]\n",
      "    for pair in correct_annot_paths:\n",
      "        assert pair in annot_paths\n",
      "    for pair in annot_paths:\n",
      "        assert pair in correct_annot_paths\n",
      "\n",
      "======= file: tests.test_type_env/test_type_normalization ========\n",
      "# tests.test_type_env\n",
      "def test_type_normalization():\n",
      "    equiv_pairs: <mask> = [\n",
      "        (\"list[int]\", \"List[int]\"),\n",
      "        (\"dict[str, list]\", \"Dict[str, List]\"),\n",
      "        (\"'Foo[int]'\", \"Foo[int]\"),\n",
      "        (\"typing.Union[str, List]\", \"typing.Union[list, str]\"),\n",
      "        (\"typing.Union[str, typing.Union[str, int]]\", \"str | int\"),\n",
      "        (\"typing.Union[str, float, typing.Union[str, int]]\", \"str | int | float\"),\n",
      "        (\"Union[str, float, None]\", \"Optional[Union[str, float]]\"),\n",
      "        (\"str | None\", \"Optional[str]\"),\n",
      "        (\"Any | None\", \"Optional\"),\n",
      "        (\"List[Any]\", \"List\"),\n",
      "        (\"Dict[Any, Any]\", \"Dict\"),\n",
      "    ]\n",
      "\n",
      "    for a, b in equiv_pairs:\n",
      "        ta = parse_type_str(a)\n",
      "        tb = parse_type_str(b)\n",
      "        assert normalize_type(ta) == normalize_type(tb)\n",
      "\n",
      "    nonequiv_pairs: <mask> = [\n",
      "        (\"Union[str, int]\", \"Union[str, list]\"),\n",
      "        (\"typing.List[str]\", \"t.List[str]\"),\n",
      "        (\"tuple[str, int]\", \"tuple[int, str]\"),\n",
      "        (\"Dict[str, Any]\", \"Dict\"),\n",
      "    ]\n",
      "\n",
      "    for a, b in nonequiv_pairs:\n",
      "        ta = parse_type_str(a)\n",
      "        tb = parse_type_str(b)\n",
      "        assert normalize_type(ta)!= normalize_type(tb)\n",
      "\n",
      "======= file: scripts.train_dagger/wandb_string ========\n",
      "# spot.visualization\n",
      "def string_to_html(s: str) -> str:\n",
      "    return f\"<div style='white-space: pre-wrap; line-height: 1.2; font-family: monospace, monospace;'>{s}</div>\"\n",
      "# scripts.train_dagger\n",
      "def wandb_string(s: <mask>):\n",
      "    return wandb.Html(string_to_html(s))\n",
      "\n",
      "======= file: scripts.train_model/wandb_string ========\n",
      "# spot.visualization\n",
      "def string_to_html(s: str) -> str:\n",
      "    return f\"<div style='white-space: pre-wrap; line-height: 1.2; font-family: monospace, monospace;'>{s}</div>\"\n",
      "# scripts.train_model\n",
      "def wandb_string(s: <mask>):\n",
      "    return wandb.Html(string_to_html(s))\n",
      "\n",
      "======= file: scripts.temp.type_check.code/fib ========\n",
      "# scripts.temp.type_check.code\n",
      "def fib(n: <mask>) -> <mask>:\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for src in srcs[-5:]:\n",
    "    print(f\"======= file: {src.file} ========\")\n",
    "    src.print_code(max_lines=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quicktest': bool,\n",
       " 'func_only': bool,\n",
       " 'pre_args': spot.tokenized_src.PreprocessArgs,\n",
       " 'data_reduction': int,\n",
       " 'check_in_isolation': bool,\n",
       " 'all_labels': bool,\n",
       " 'inline_prev_gold': bool,\n",
       " 'ctx_size': int,\n",
       " 'left_margin': int,\n",
       " 'preamble_size': int,\n",
       " 'right_margin': int,\n",
       " 'train_max_labels': int,\n",
       " 'dec_max_labels': int,\n",
       " 'use_small_model': bool,\n",
       " 'modifications': str}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.train import TrainingConfig\n",
    "\n",
    "TrainingConfig(quicktest=True).__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TrainingConfig(imports_in_preamble=False)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.data import PreprocessArgs, repr_modified_args\n",
    "\n",
    "repr_modified_args(TrainingConfig(pre_args=PreprocessArgs(imports_in_preamble=False)), flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(proj'root.file3/usage1', CodeRange(start=CodePosition(line=6, column=4), end=CodePosition(line=6, column=9)), QualifiedName(name='gf', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file3/usage1', CodeRange(start=CodePosition(line=7, column=4), end=CodePosition(line=7, column=8)), QualifiedName(name='C', source=<QualifiedNameSource.IMPORT: 1>))\n"
     ]
    }
   ],
   "source": [
    "from spot.static_analysis import cst, PythonModule, compute_module_usages, PythonProject\n",
    "\n",
    "code1 = \"\"\"\n",
    "# root.file1\n",
    "\n",
    "# global function\n",
    "def gf(x):\n",
    "    return x * x\n",
    "\n",
    "# with inner function\n",
    "def gf_with_inner(x):\n",
    "    def inner(y):\n",
    "        return y * y\n",
    "    return inner(x)\n",
    "\n",
    "# class\n",
    "class C:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def foo(self, y):\n",
    "        return self.x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x + 1\n",
    "    \n",
    "\"\"\"\n",
    "code2 = \"\"\"\n",
    "# root.file2\n",
    "from .file1 import gf\n",
    "from root.file1 import gf_with_inner\n",
    "import root.file1\n",
    "import root.file1 as f1\n",
    "\n",
    "def usage1(x):\n",
    "    gf(x) + root.file1.C(5)\n",
    "    foo(5)\n",
    "\n",
    "def usage2(x):\n",
    "    def inner():\n",
    "        1 + gf_with_inner(x)\n",
    "    return inner()\n",
    "\n",
    "def usage_method1(x):\n",
    "    x = f1.C(5)\n",
    "    1 + x.foo(3)\n",
    "\n",
    "def usage_method2(x):\n",
    "    (1 + f1.C(5)).foo(3)\n",
    "\n",
    "def usage_local():\n",
    "    usage1(3)\n",
    "    UsageClass(4)\n",
    "\n",
    "@f1.C(1)\n",
    "def usage_dec():\n",
    "    pass\n",
    "\n",
    "class UsageClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = gf_with_inner(x)\n",
    "        self.y = self.foo(5)\n",
    "\n",
    "    def foo(self, y):\n",
    "        return usage_local(f1.gf(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x\n",
    "\n",
    "class SubClass(UsageClass):\n",
    "    def use(self):\n",
    "        self.foo(5)\n",
    "        f1.C.s_method(5)\n",
    "\"\"\"\n",
    "\n",
    "code3 = \"\"\"\n",
    "# root.file3\n",
    "from .file1 import *\n",
    "\n",
    "def usage1(x):\n",
    "    gf(5)\n",
    "    C(5)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "project = PythonProject.from_modules(\n",
    "        [\n",
    "            PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
    "            PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
    "            PythonModule.from_cst(cst.parse_module(code3), \"root.file3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "for u in compute_module_usages(project.modules[\"root.file3\"]):\n",
    "    print(str(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root.file1': {'gf': proj'root.file1/gf',\n",
       "  'C': proj'root.file1/C',\n",
       "  'gf_with_inner': proj'root.file1/gf_with_inner'},\n",
       " 'root.file2': {'gf': proj'root.file1/gf',\n",
       "  'gf_with_inner': proj'root.file1/gf_with_inner',\n",
       "  'usage2': proj'root.file2/usage2',\n",
       "  'UsageClass': proj'root.file2/UsageClass',\n",
       "  'usage_method1': proj'root.file2/usage_method1',\n",
       "  'usage_method2': proj'root.file2/usage_method2',\n",
       "  'usage_local': proj'root.file2/usage_local',\n",
       "  'SubClass': proj'root.file2/SubClass',\n",
       "  'usage1': proj'root.file2/usage1',\n",
       "  'usage_dec': proj'root.file2/usage_dec'},\n",
       " 'root.file3': {'gf': proj'root.file1/gf',\n",
       "  'C': proj'root.file1/C',\n",
       "  'gf_with_inner': proj'root.file1/gf_with_inner',\n",
       "  'usage1': proj'root.file3/usage1'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.static_analysis import build_project_namespaces\n",
    "build_project_namespaces(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(proj'root.file3/usage1',\n",
       "  CodeRange(start=CodePosition(line=6, column=4), end=CodePosition(line=6, column=9)),\n",
       "  QualifiedName(name='gf', source=<QualifiedNameSource.IMPORT: 1>)),\n",
       " (proj'root.file3/usage1',\n",
       "  CodeRange(start=CodePosition(line=7, column=4), end=CodePosition(line=7, column=8)),\n",
       "  QualifiedName(name='C', source=<QualifiedNameSource.IMPORT: 1>))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.static_analysis import compute_module_usages\n",
    "\n",
    "compute_module_usages(project.modules[\"root.file3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local name: gf_with_inner.<locals>.inner\n",
      "Segs: ['gf_with_inner', '<locals>', 'inner']\n",
      "Local name: usage2.<locals>.inner\n",
      "Segs: ['usage2', '<locals>', 'inner']\n",
      "Local name: usage_method1.<locals>.x.foo\n",
      "Segs: ['usage_method1', '<locals>', 'x', 'foo']\n",
      "Case 3\n",
      "Local name: <method>.foo\n",
      "Local name: usage1\n",
      "Segs: ['usage1']\n",
      "Case 1\n",
      "Local name: UsageClass\n",
      "Segs: ['UsageClass']\n",
      "Case 2\n",
      "Local name: UsageClass.__init__.<locals>.self.foo\n",
      "Segs: ['UsageClass', 'foo']\n",
      "Case 1\n",
      "Local name: usage_local\n",
      "Segs: ['usage_local']\n",
      "Case 1\n",
      "Local name: SubClass.use.<locals>.self.foo\n",
      "Segs: ['SubClass', 'foo']\n",
      "Case 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FunctionUsage(caller=proj'root.file2/SubClass.use', callee=proj'root.file1/C.foo', call_site=CodeRange(start=CodePosition(line=42, column=8), end=CodePosition(line=42, column=19)), is_certain=False),\n",
       " FunctionUsage(caller=proj'root.file2/SubClass.use', callee=proj'root.file2/UsageClass.foo', call_site=CodeRange(start=CodePosition(line=42, column=8), end=CodePosition(line=42, column=19)), is_certain=False)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.static_analysis import UsageAnalysis, build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n",
    "\n",
    "analysis = UsageAnalysis(project)\n",
    "analysis.caller2callees[ProjectPath(\"root.file2\", \"SubClass.use\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@wraps(function)\n",
      "def catch_permission_denied(function):\n",
      "    import some.inner.imports\n",
      "    @wraps(function)\n",
      "    def decorated(x: <mask>, y: <mask>) -> <mask>:\n",
      "        try:\n",
      "            return function(*args, **kwargs)\n",
      "\n",
      "        except InsufficientPrivilege as error:\n",
      "            LOG.error(\"Forbidden: %s\", error) \n",
      "            raise Forbidden()\n",
      "\n",
      "    return decorated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc4db6cf4c24dc09970460392d6c744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='preamble', min=1), IntSlider(value=100, description='le…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(ctx_size, preamble, left, right, max_labels=max_labels, inline_prev_gold=inline_prev)\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0,1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.data import GitRepo, ModuleRemapUnpickler\n",
    "from spot.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    collect_annotations,\n",
    "    mypy_checker,\n",
    ")\n",
    "from spot.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"spot.data\" if n == \"spot.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "from spot.utils import get_data_dir\n",
    "\n",
    "model_dir = \"Salesforce/codet5-base\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9077, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from spot.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- model output ----\n",
      "<pad><s><extra_id_0>int<extra_id_1>int<extra_id_2>int<extra_id_3>int<extra_id_4>int, y : int<extra_id_5>int<extra_id_6>Optional[int]<extra_id_7>int<extra_id_8>int<extra_id_9>Bar[int, int, int, float, float]</s>\n",
      "---- checker_feedback ----\n",
      "env_code_2.py:20:14: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n",
      "env_code_2.py:32:29: error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  [arg-type]\n",
      "env_code_2.py:35:6: error: \"Bar\" expects no type arguments, but 5 given  [type-arg]\n",
      "Found 3 errors in 1 file (checked 1 source file)\n",
      "\n",
      "---- new input ----\n",
      "# Env example 2: some existing annotations\n",
      "\n",
      "from typing import *\n",
      "\n",
      "\n",
      "def fib(n: /* int */<extra_id_0>):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 1) + fib(n - 2)\n",
      "\n",
      "\n",
      "def foo(bar: /* int */<extra_id_1>):\n",
      "    return fib(bar)\n",
      "\n",
      "\n",
      "class Bar:\n",
      "    z: /* int */<extra_id_2> = /* error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  */\"hello\"\n",
      "    w: /* int */<extra_id_3>\n",
      "\n",
      "    def __init__(self, x: /* Any */<extra_id_4>):\n",
      "        self.x: /* int */<extra_id_5> = x\n",
      "        self.y: /* Optional[int] */<extra_id_6> = None\n",
      "        self.reset(self.z)\n",
      "\n",
      "    def reset(self, w0):\n",
      "        self.w = w0\n",
      "\n",
      "    def foo(self, z: /* int */<extra_id_7>) -> /* int */<extra_id_8>:\n",
      "        return self.x + len(/* error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  */z)\n",
      "\n",
      "\n",
      "bar: /* Bar[int, int, int, float, float] *//* error: \"Bar\" expects no type arguments, but 5 given  */<extra_id_9> = Bar(3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import patch_code_with_extra\n",
    "\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r[\"model_result\"][\"predicted_types\"])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountedAcc(16.23%, count=573)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from spot.utils import Path, run_long_task, DefaultTokenizer, not_none, CountedAcc\n",
    "from spot import proj_root\n",
    "from spot.function_dataset import guess_src_root\n",
    "\n",
    "datadir = Path(not_none(os.getenv(\"datadir\")))\n",
    "repos_dir = datadir / \"SPOT-data/repos/\"\n",
    "\n",
    "repos_split_path = proj_root() /  \"data/repos_split.pkl\"\n",
    "with repos_split_path.open(\"rb\") as f:\n",
    "    repos_split = pickle.load(f)\n",
    "\n",
    "root_is_src = list[bool]()\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd = repo.repo_dir(repos_dir)\n",
    "    root_is_src.append(guess_src_root(rd).name == \"src\")\n",
    "\n",
    "CountedAcc(sum(root_is_src), len(root_is_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_projects: 573\n",
      "src_in_root: 93\n",
      "package_in_root: 203\n",
      "setup_in_root: 107\n",
      "weird_repos: 170\n"
     ]
    }
   ],
   "source": [
    "src_in_root = 0\n",
    "package_in_root = 0\n",
    "setup_in_root = 0\n",
    "n_proj = 0\n",
    "\n",
    "weird_repos = []\n",
    "setup_files = []\n",
    "\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    n_proj += 1\n",
    "    files = list(rd.iterdir())\n",
    "    if rd / \"src\" in files:\n",
    "        src_in_root += 1\n",
    "    elif rd / (pname := rd.name.split(\"__\")[-1]) in files:\n",
    "        package_in_root += 1\n",
    "    elif rd / \"setup.cfg\" in files:\n",
    "        setup_in_root += 1\n",
    "        setup_files.append(rd / \"setup.cfg\")\n",
    "    else:\n",
    "        weird_repos.append(repo)\n",
    "\n",
    "print(\"n_projects:\", n_proj)\n",
    "print(\"src_in_root:\", src_in_root)\n",
    "print(\"package_in_root:\", package_in_root)\n",
    "print(\"setup_in_root:\", setup_in_root)\n",
    "print(\"weird_repos:\", len(weird_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: downloaded/tiangolo__uvicorn-gunicorn-docker\n",
      "scripts\n",
      ".gitignore\n",
      "mypy.ini\n",
      "README.md\n",
      "tests\n",
      ".github\n",
      ".mypy_cache\n",
      ".git\n",
      "docker-images\n",
      "pyproject.toml\n",
      "LICENSE\n",
      "Repo: downloaded/uwbmrb__BMRBDep\n",
      ".gitignore\n",
      "install.sh\n",
      "ADIT-NMR Testing.ods\n",
      "README.md\n",
      "FrontEnd\n",
      "deploy.sh\n",
      "BackEnd\n",
      "nginx_configuration_example.conf\n",
      "upgrade.sh\n",
      "apache_configuration_example.conf\n",
      ".mypy_cache\n",
      ".git\n",
      "wsgi.conf\n",
      "installation.md\n",
      ".editorconfig\n",
      "Dockerfile\n",
      "build_docker.sh\n",
      "run_locally.sh\n",
      ".dockerignore\n",
      "Repo: downloaded/jfcherng__Sublime-VisualizeZeroWidthChars\n",
      "messages\n",
      "dependencies.json\n",
      "docs\n",
      ".flake8\n",
      "boot.py\n",
      "scripts\n",
      ".gitignore\n",
      "typings\n",
      "mypy.ini\n",
      ".python-version\n",
      "messages.json\n",
      "README.md\n",
      "menus\n",
      "plugin\n",
      ".github\n",
      ".mypy_cache\n",
      ".git\n",
      "pyproject.toml\n",
      "VisualizeZeroWidthChars.sublime-settings\n",
      "CHANGELOG.md\n",
      "requirements.txt\n",
      ".gitattributes\n",
      "LICENSE\n",
      ".editorconfig\n",
      "Repo: downloaded/chaosdorf__mpd-mqtt-gateway\n",
      ".gitignore\n",
      "gateway.py\n",
      ".github\n",
      "Pipfile\n",
      ".mypy_cache\n",
      "server.py\n",
      ".git\n",
      "Dockerfile\n",
      "Pipfile.lock\n",
      "Repo: downloaded/Celeo__Preston\n",
      "preston\n",
      ".gitignore\n",
      "README.md\n",
      "pytest.ini\n",
      "tests\n",
      ".github\n",
      ".mypy_cache\n",
      "poetry.lock\n",
      ".git\n",
      "pyproject.toml\n",
      "LICENSE\n",
      ".vscode\n",
      "Repo: downloaded/andrewscwei__rbc-statement-parser\n",
      "parse_csv.py\n",
      ".gitignore\n",
      "__pycache__\n",
      "README.md\n",
      "parse_tbl.py\n",
      "Pipfile\n",
      ".mypy_cache\n",
      ".git\n",
      "config\n",
      "LICENSE\n",
      "parse_pdf.py\n",
      "utils.py\n",
      "Pipfile.lock\n",
      "Repo: downloaded/devonhollowood__adventofcode\n",
      "2016\n",
      "README.md\n",
      "2019\n",
      "2017\n",
      ".mypy_cache\n",
      "2015\n",
      ".git\n",
      "LICENSE\n",
      "2018\n",
      "2021\n",
      "2020\n",
      "Repo: downloaded/tedle__uitabot\n",
      "scripts\n",
      ".gitignore\n",
      "web-client\n",
      "README.md\n",
      "CONFIG.md\n",
      "config.example.json\n",
      "COMMANDS.md\n",
      ".github\n",
      "bot\n",
      ".mypy_cache\n",
      ".git\n",
      "CHANGELOG.md\n",
      ".gitattributes\n",
      "LICENSE\n",
      "Repo: downloaded/JohnStrunk__ocs-monkey\n",
      "build.sh\n",
      "chaos_runner.py\n",
      ".gitignore\n",
      "mypy.ini\n",
      "log_gather.py\n",
      ".travis\n",
      "README.md\n",
      "setup-env.sh\n",
      "kube.py\n",
      ".github\n",
      "util.py\n",
      "event.py\n",
      ".mypy_cache\n",
      ".git\n",
      "tox.ini\n",
      "osio-workload\n",
      "workload_runner.py\n",
      "requirements.txt\n",
      "oc_in_cluster.sh\n",
      "test_kube.py\n",
      "osio.py\n",
      "LICENSE\n",
      "Dockerfile\n",
      "failure.py\n",
      "helm\n",
      "conftest.py\n",
      "log_gather_ocs.py\n",
      "failure_ocs.py\n",
      ".dockerignore\n",
      "Repo: downloaded/jmanuel1__material-search\n",
      "scripts\n",
      ".gitignore\n",
      "elements\n",
      "README.md\n",
      "package.json\n",
      "images\n",
      "manifest.json\n",
      "robots.txt\n",
      ".mypy_cache\n",
      "polymer.json\n",
      ".git\n",
      "gulpfile.js\n",
      "requirements.txt\n",
      "index.html\n",
      ".gitattributes\n",
      "yarn.lock\n",
      "favicon.ico\n",
      "styles\n",
      "test\n",
      "LICENSE.md\n"
     ]
    }
   ],
   "source": [
    "for repo in weird_repos[:10]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    print(\"Repo:\", rd.relative_to(repos_dir))\n",
    "    for f in rd.iterdir():\n",
    "        print(f.relative_to(rd))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
