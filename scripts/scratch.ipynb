{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def gen_stub(m, rm_comments=..., rm_imports=...): ...\n",
      "def remove_empty_lines(m): ...\n",
      "OMIT = ...\n",
      "@dataclass\n",
      "class ClassNamespace:\n",
      "    all_elems: ... = ...\n",
      "    declared_elems: ... = ...\n",
      "class StubGenerator(cst.CSTTransformer):\n",
      "    def __init__(self): ...\n",
      "    def register_elem(self, name, declared): ...\n",
      "    def visit_ClassDef(self, node): ...\n",
      "    def leave_ClassDef(self, node, updated): ...\n",
      "    def leave_FunctionDef(self, node, updated): ...\n",
      "    def leave_Annotation(self, node, updated): ...\n",
      "    def leave_Param(self, node, updated): ...\n",
      "    def leave_AnnAssign(self, node, updated): ...\n",
      "    def leave_Assign(self, node, updated): ...\n",
      "    def leave_Attribute(self, node, updated): ...\n",
      "    ns_stack: ...\n",
      "class EmptyLineRemove(cst.CSTTransformer):\n",
      "    def on_leave(self, node, updated): ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, stub_from_module\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''\n",
    "from .utils import *\n",
    "from .data import remove_comments, remove_imports\n",
    "import spot\n",
    "\n",
    "\n",
    "def gen_stub(m: cst.Module, rm_comments=True, rm_imports=True) -> cst.Module:\n",
    "    \"\"\"Removes all comments and docstrings.\"\"\"\n",
    "    if rm_comments:\n",
    "        m = remove_comments(m)\n",
    "    if rm_imports:\n",
    "        m, _ = remove_imports(m)\n",
    "    m = m.visit(StubGenerator())\n",
    "    m = remove_empty_lines(m)\n",
    "    spot.fly.attach(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "def remove_empty_lines(m: cst.Module) -> cst.Module:\n",
    "    m = m.visit(EmptyLineRemove())\n",
    "    return m\n",
    "\n",
    "\n",
    "OMIT = cst.SimpleStatementSuite([cst.Expr(cst.Ellipsis())])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClassNamespace:\n",
    "    all_elems: set[str] = field(default_factory=set)\n",
    "    declared_elems: set[str] = field(default_factory=set)\n",
    "\n",
    "\n",
    "class StubGenerator(cst.CSTTransformer):\n",
    "    \"\"\"Generate a stub module from a Python module.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ns_stack = list[ClassNamespace]()\n",
    "        self.ns_stack = 1\n",
    "\n",
    "    def register_elem(self, name: str, declared: bool):\n",
    "        if self.ns_stack:\n",
    "            s = self.ns_stack[-1]\n",
    "            s.all_elems.add(name)\n",
    "            if declared:\n",
    "                s.declared_elems.add(name)\n",
    "\n",
    "    def visit_ClassDef(self, node: cst.ClassDef):\n",
    "        self.ns_stack.append(ClassNamespace())\n",
    "\n",
    "    def leave_ClassDef(self, node, updated: cst.ClassDef):\n",
    "        s = self.ns_stack.pop()\n",
    "        to_declare = s.all_elems.difference(s.declared_elems)\n",
    "        if to_declare:\n",
    "            more_stmts = [cst.parse_statement(f\"{n}: ...\") for n in to_declare]\n",
    "            new_stmts = list(updated.body.body) + more_stmts\n",
    "            updated = updated.with_changes(\n",
    "                body=updated.body.with_changes(body=new_stmts)\n",
    "            )\n",
    "        return updated\n",
    "\n",
    "    def leave_FunctionDef(self, node, updated: cst.FunctionDef):\n",
    "        self.register_elem(updated.name.value, True)\n",
    "        return updated.with_changes(body=OMIT, returns=None)\n",
    "\n",
    "    def leave_Annotation(self, node, updated: cst.Annotation):\n",
    "        return updated.with_changes(annotation=cst.Ellipsis())\n",
    "\n",
    "    def leave_Param(self, node, updated: cst.Param):\n",
    "        if updated.default is not None:\n",
    "            updated = updated.with_changes(default=cst.Ellipsis())\n",
    "        return updated.with_changes(annotation=None)\n",
    "\n",
    "    def leave_AnnAssign(self, node, updated: cst.AnnAssign):\n",
    "        if updated.value is not None:\n",
    "            updated = updated.with_changes(value=cst.Ellipsis())\n",
    "        return updated\n",
    "\n",
    "    def leave_Assign(self, node, updated: cst.AnnAssign):\n",
    "        return updated.with_changes(value=cst.Ellipsis())\n",
    "\n",
    "    def leave_Attribute(self, node, updated: cst.Assign):\n",
    "        match updated:\n",
    "            case cst.Attribute(\n",
    "                value=cst.Name(value=\"self\"),\n",
    "                attr=cst.Name(value=elem_name),\n",
    "            ):\n",
    "                self.register_elem(elem_name, False)\n",
    "        return updated\n",
    "\n",
    "\n",
    "class EmptyLineRemove(cst.CSTTransformer):\n",
    "    def on_leave(self, node, updated):\n",
    "        if hasattr(updated, \"leading_lines\") and updated.leading_lines:\n",
    "            return updated.with_changes(leading_lines=[])\n",
    "        return updated\n",
    "\n",
    "'''\n",
    "\n",
    "ex_m = cst.parse_module(ex_code)\n",
    "print(stub_from_module(ex_m).code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot.static_analysis import PythonModule, compute_module_usages, PythonProject\n",
    "\n",
    "pmod = PythonModule.from_cst(ex_m, \"spot.stub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{proj'spot.critic/CriticModel.eval_on_src_dataset': [proj'spot.model/dynamic_dataloader'],\n",
      " proj'spot.critic/TrainCriticModelWrapper.configure_optimizers': [proj'spot.train/_configure_optimizers'],\n",
      " proj'spot.critic/to_critic_dataset': [proj'spot.type_check/normalize_type',\n",
      "                                       proj'spot.type_check/normalize_type'],\n",
      " proj'spot.critic/train_critic_model': [proj'spot.model/dynamic_dataloader'],\n",
      " proj'spot.dagger/DAggerEvalResult.accuracies': [proj'spot.data/src_preds_to_accuracies'],\n",
      " proj'spot.dagger/DAggerModel.train_on_data': [proj'spot.train/_configure_optimizers',\n",
      "                                               proj'spot.type_check/normalize_type',\n",
      "                                               proj'spot.type_check/normalize_type'],\n",
      " proj'spot.dagger/get_typechecked_src': [proj'spot.tokenized_src/feedbacks_to_tokenized_src'],\n",
      " proj'spot.dagger/src_to_batch': [proj'spot.data/chunk_from_src'],\n",
      " proj'spot.data/GitRepo.collect_annotations': [proj'spot.type_env/collect_annots_info'],\n",
      " proj'spot.data/type_accuracies': [proj'spot.type_check/remove_top_optional',\n",
      "                                   proj'spot.type_check/remove_top_optional'],\n",
      " proj'spot.debug_critic/check_delta': [proj'spot.type_check/normalize_type',\n",
      "                                       proj'spot.type_check/normalize_type'],\n",
      " proj'spot.debug_critic/inspect_critic_on_beams': [proj'spot.type_check/normalize_type',\n",
      "                                                   proj'spot.type_check/normalize_type'],\n",
      " proj'spot.decode/collect_type_errors_in_project': [proj'spot.data/code_to_check_from_preds'],\n",
      " proj'spot.decode/incr_inference_with_feedback': [proj'spot.data/chunk_from_src',\n",
      "                                                  proj'spot.type_check/normalize_type',\n",
      "                                                  proj'spot.type_check/normalize_type',\n",
      "                                                  proj'spot.model/dynamic_dataloader'],\n",
      " proj'spot.decode/select_candidates_using_critic': [proj'spot.model/dynamic_dataloader'],\n",
      " proj'spot.decode/select_candidates_using_oracle': [proj'spot.type_check/normalize_type',\n",
      "                                                    proj'spot.type_check/normalize_type'],\n",
      " proj'spot.decode/to_critic_inputs': [proj'spot.data/src_to_chunks_'],\n",
      " proj'spot.model/DatasetPredResult.accuracies': [proj'spot.data/preds_to_accuracies'],\n",
      " proj'spot.model/ModelWrapper.predict_on_batch': [proj'spot.data/output_ids_as_types'],\n",
      " proj'spot.tokenized_src/feedbacks_to_tokenized_src': [proj'spot.type_env/collect_user_annotations',\n",
      "                                                       proj'spot.type_check/parse_type_str'],\n",
      " proj'spot.tokenized_src/preprocess_code': [proj'spot.type_env/collect_user_annotations',\n",
      "                                            proj'spot.type_env/apply_annotations'],\n",
      " proj'spot.train/R1_srcs_from_ckpts': [proj'spot.data/R1_srcs_from_preds'],\n",
      " proj'spot.train/R1_srcs_from_extra': [proj'spot.data/R1_srcs_from_preds',\n",
      "                                       proj'spot.data/R1_srcs_from_preds'],\n",
      " proj'spot.train/R1_srcs_from_model': [proj'spot.data/R1_srcs_from_preds',\n",
      "                                       proj'spot.data/preds_to_accuracies'],\n",
      " proj'spot.train/evaluate_model': [proj'spot.data/R1_srcs_from_preds'],\n",
      " proj'spot.train/train_spot_model': [proj'spot.model/dynamic_dataloader',\n",
      "                                     proj'spot.model/dynamic_dataloader'],\n",
      " proj'spot.visualization/export_preds_on_code': [proj'spot.type_check/normalize_type',\n",
      "                                                 proj'spot.type_check/normalize_type'],\n",
      " proj'spot.visualization/show_feedback_stats': [proj'spot.utils/pretty_print_dict',\n",
      "                                                proj'spot.utils/groupby'],\n",
      " proj'spot.visualization/visualize_chunk': [proj'spot.type_check/normalize_type',\n",
      "                                            proj'spot.type_check/normalize_type'],\n",
      " proj'spot.visualization/visualize_conf_matrix': [proj'spot.type_check/normalize_type',\n",
      "                                                  proj'spot.type_check/normalize_type'],\n",
      " proj'spot.visualization/visualize_preds_on_code': [proj'spot.type_check/normalize_type',\n",
      "                                                    proj'spot.type_check/normalize_type']}\n"
     ]
    }
   ],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root() / \"src\")\n",
    "pprint(compute_project_usages(proj), width=50, compact=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spot.__init__': PythonModule(n_functions=0, n_classes=0),\n",
      " 'spot.critic': PythonModule(n_functions=5, n_classes=5),\n",
      " 'spot.dagger': PythonModule(n_functions=6, n_classes=6),\n",
      " 'spot.data': PythonModule(n_functions=23, n_classes=8),\n",
      " 'spot.debug_critic': PythonModule(n_functions=4, n_classes=0),\n",
      " 'spot.decode': PythonModule(n_functions=16, n_classes=5),\n",
      " 'spot.model': PythonModule(n_functions=1, n_classes=3),\n",
      " 'spot.static_analysis': PythonModule(n_functions=2, n_classes=8),\n",
      " 'spot.tokenized_src': PythonModule(n_functions=8, n_classes=7),\n",
      " 'spot.train': PythonModule(n_functions=8, n_classes=3),\n",
      " 'spot.type_check': PythonModule(n_functions=11, n_classes=6),\n",
      " 'spot.type_env': PythonModule(n_functions=9, n_classes=11),\n",
      " 'spot.utils': PythonModule(n_functions=44, n_classes=10),\n",
      " 'spot.visualization': PythonModule(n_functions=35, n_classes=0)}\n"
     ]
    }
   ],
   "source": [
    "from spot.static_analysis import PythonProject, proj_root\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(PythonProject.from_root(proj_root() / \"src\").modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(proj'root.file2/usage1', CodeRange(start=CodePosition(line=9, column=4), end=CodePosition(line=9, column=9)), QualifiedName(name='.file1.gf', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage1', CodeRange(start=CodePosition(line=9, column=12), end=CodePosition(line=9, column=27)), QualifiedName(name='root.file1.C', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage2', CodeRange(start=CodePosition(line=14, column=12), end=CodePosition(line=14, column=28)), QualifiedName(name='root.file1.gf_with_inner', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage2', CodeRange(start=CodePosition(line=15, column=11), end=CodePosition(line=15, column=18)), QualifiedName(name='usage2.<locals>.inner', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/usage_method1', CodeRange(start=CodePosition(line=18, column=8), end=CodePosition(line=18, column=15)), QualifiedName(name='root.file1.C', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage_method1', CodeRange(start=CodePosition(line=19, column=8), end=CodePosition(line=19, column=16)), QualifiedName(name='usage_method1.<locals>.x.foo', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/usage_method2', CodeRange(start=CodePosition(line=22, column=4), end=CodePosition(line=22, column=24)), QualifiedName(name='<method>.foo', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/usage_method2', CodeRange(start=CodePosition(line=22, column=9), end=CodePosition(line=22, column=16)), QualifiedName(name='root.file1.C', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage_local', CodeRange(start=CodePosition(line=25, column=4), end=CodePosition(line=25, column=13)), QualifiedName(name='usage1', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/usage_local', CodeRange(start=CodePosition(line=26, column=4), end=CodePosition(line=26, column=17)), QualifiedName(name='UsageClass', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/UsageClass.__init__', CodeRange(start=CodePosition(line=30, column=17), end=CodePosition(line=30, column=33)), QualifiedName(name='root.file1.gf_with_inner', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/UsageClass.__init__', CodeRange(start=CodePosition(line=31, column=8), end=CodePosition(line=31, column=19)), QualifiedName(name='UsageClass.__init__.<locals>.self.foo', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/UsageClass.foo', CodeRange(start=CodePosition(line=34, column=15), end=CodePosition(line=34, column=36)), QualifiedName(name='usage_local', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/UsageClass.foo', CodeRange(start=CodePosition(line=34, column=27), end=CodePosition(line=34, column=35)), QualifiedName(name='root.file1.gf', source=<QualifiedNameSource.IMPORT: 1>))\n"
     ]
    }
   ],
   "source": [
    "code1 = \"\"\"\n",
    "# root.file1\n",
    "\n",
    "# global function\n",
    "def gf(x):\n",
    "    return x * x\n",
    "\n",
    "# with inner function\n",
    "def gf_with_inner(x):\n",
    "    def inner(y):\n",
    "        return y * y\n",
    "    return inner(x)\n",
    "\n",
    "# class\n",
    "class C:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def foo(self, y):\n",
    "        return self.x + y\n",
    "    \n",
    "\"\"\"\n",
    "code2 = \"\"\"\n",
    "# root.file2\n",
    "from .file1 import gf\n",
    "from root.file1 import gf_with_inner\n",
    "import root.file1\n",
    "import root.file1 as f1\n",
    "\n",
    "def usage1(x):\n",
    "    gf(x) + root.file1.C(5)\n",
    "    foo(5)\n",
    "\n",
    "def usage2(x):\n",
    "    def inner():\n",
    "        1 + gf_with_inner(x)\n",
    "    return inner()\n",
    "\n",
    "def usage_method1(x):\n",
    "    x = f1.C(5)\n",
    "    1 + x.foo(3)\n",
    "\n",
    "def usage_method2(x):\n",
    "    (1 + f1.C(5)).foo(3)\n",
    "\n",
    "def usage_local():\n",
    "    usage1(3)\n",
    "    UsageClass(4)\n",
    "\n",
    "class UsageClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = gf_with_inner(x)\n",
    "        self.foo(5)\n",
    "\n",
    "    def foo(self, y):\n",
    "        return usage_local(f1.gf(1))\n",
    "\"\"\"\n",
    "\n",
    "project = PythonProject.from_modules(\n",
    "    [\n",
    "        PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for u in compute_module_usages(project.modules[\"root.file2\"]):\n",
    "    print(str(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@wraps(function)\n",
      "def catch_permission_denied(function):\n",
      "    import some.inner.imports\n",
      "    @wraps(function)\n",
      "    def decorated(x: <mask>, y: <mask>) -> <mask>:\n",
      "        try:\n",
      "            return function(*args, **kwargs)\n",
      "\n",
      "        except InsufficientPrivilege as error:\n",
      "            LOG.error(\"Forbidden: %s\", error) \n",
      "            raise Forbidden()\n",
      "\n",
      "    return decorated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc4db6cf4c24dc09970460392d6c744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='preamble', min=1), IntSlider(value=100, description='le…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(ctx_size, preamble, left, right, max_labels=max_labels, inline_prev_gold=inline_prev)\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0,1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SrcDataset.from_repos() got an unexpected keyword argument 'drop_comments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jiayi/Projects/SPOT/scripts/scratch.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspot\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m SrcDataset\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspot\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m DefaultTokenizer, proj_root\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m simple_dataset \u001b[39m=\u001b[39m SrcDataset\u001b[39m.\u001b[39;49mfrom_repos(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=4'>5</a>\u001b[0m     proj_root() \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m     [proj_root() \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdata/code\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=6'>7</a>\u001b[0m     DefaultTokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=7'>8</a>\u001b[0m     drop_comments\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=8'>9</a>\u001b[0m     max_workers\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=9'>10</a>\u001b[0m     label_ratio\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/scratch.ipynb#ch0000004vscode-remote?line=10'>11</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: SrcDataset.from_repos() got an unexpected keyword argument 'drop_comments'"
     ]
    }
   ],
   "source": [
    "from spot.data import SrcDataset\n",
    "from spot.utils import DefaultTokenizer, proj_root\n",
    "\n",
    "simple_dataset = SrcDataset.from_repos(\n",
    "    proj_root() / \"data\",\n",
    "    [proj_root() / \"data/code\"],\n",
    "    DefaultTokenizer,\n",
    "    drop_comments=True,\n",
    "    max_workers=10,\n",
    "    label_ratio=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca56a5acd8c2432dbdb94fe691dcca3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "type_check_src:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedbacks:\n",
      "MypyFeedback(position=CodePosition(line=14, column=12), message='Incompatible return value type (got \"str\", expected \"int\") ', error_code='return-value')\n",
      "======= New code =======\n",
      " 1|  from typing import Any # SPOT\n",
      " 2|  from typing import Any\n",
      " 3|  \n",
      " 4|  def fib(n: str) -> Any:\n",
      " 5|      if n == 0:\n",
      " 6|          return 0\n",
      " 7|      elif n == 1:\n",
      " 8|          return 1\n",
      " 9|      else:\n",
      "10|          return fib(n-1) + fib(n-2)\n",
      "11|  \n",
      "12|  def t_add(x: str, y: str) -> int:\n",
      "13|      r = x + y\n",
      "14|      return r\n",
      "15|  \n",
      "16|  x: int = fib(3)\n",
      "17|  bad_y: str = 1\n",
      "Feedbacks:\n",
      "MypyFeedback(position=CodePosition(line=5, column=5), message='Argument 1 to \"fib\" has incompatible type \"int\"; expected \"str\" ', error_code='arg-type')\n",
      "======= New code =======\n",
      "1|  from typing import Any # SPOT\n",
      "2|  from bad_code_1 import fib\n",
      "3|  \n",
      "4|  i: int = 4\n",
      "5|  fib(i)\n",
      "6|  \n",
      "Feedbacks:\n",
      "MypyFeedback(position=CodePosition(line=4, column=10), message='Incompatible types in assignment (expression has type \"int\", variable has type \"str\") ', error_code='assignment')\n",
      "======= New code =======\n",
      "1|  from typing import Any # SPOT\n",
      "2|  from dummy.dummy_1 import f_int\n",
      "3|  \n",
      "4|  s: str = f_int(2)\n",
      "5|  \n"
     ]
    }
   ],
   "source": [
    "file2preds = {\n",
    "    (proj_root() / \"data/code/bad_code_1.py\"): {\n",
    "        1: \"str\",\n",
    "        2: \"str\",\n",
    "    },\n",
    "    (proj_root() / \"data/code/bad_code_2.py\"): {\n",
    "        0: \"int\",\n",
    "    },\n",
    "    (proj_root() / \"data/code/dummy/dummy_2.py\"): {\n",
    "        0: \"str\",\n",
    "    },\n",
    "}\n",
    "fdbks = simple_dataset._get_type_checker_feedback_iso(\n",
    "    file2preds,\n",
    "    max_workers=20,\n",
    ")\n",
    "for f in fdbks:\n",
    "    f.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedbacks:\n",
      "======= New code =======\n",
      "from typing import Any # SPOT\n",
      "from bad_code_1 import fib\n",
      "\n",
      "i: int = 4\n",
      "fib(i)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.data import Path, type_check_src, type_check_src_in_project\n",
    "\n",
    "src_to_check = simple_dataset.get_src_by_file(Path(\"bad_code_2.py\"))\n",
    "type_check_src(src_to_check, {0: \"int\"}).pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedbacks:\n",
      "MypyFeedback(position=CodePosition(line=5, column=5), message='Argument 1 to \"fib\" has incompatible type \"int\"; expected \"str\" ', error_code='arg-type')\n",
      "======= New code =======\n",
      "from typing import Any # SPOT\n",
      "from bad_code_1 import fib\n",
      "\n",
      "i: int = 4\n",
      "fib(i)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.data import type_check_src, type_check_src_in_project\n",
    "import shutil\n",
    "\n",
    "src_to_check = simple_dataset.get_src_by_file(Path(\"bad_code_2.py\"))\n",
    "temp_dir = proj_root() / \"mypy_temp/test_dir\"\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "type_check_src_in_project(\n",
    "    src_to_check,\n",
    "    {0: \"int\"},\n",
    "    project_files=(proj_root() / \"data/code\").glob(\"**/*.py\"),\n",
    "    project_root=(proj_root() / \"data/code\"),\n",
    "    temp_dir=temp_dir,\n",
    ").pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.data import GitRepo, ModuleRemapUnpickler\n",
    "from spot.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    collect_annotations,\n",
    "    mypy_checker,\n",
    ")\n",
    "from spot.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"spot.data\" if n == \"spot.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "from spot.utils import get_data_dir\n",
    "\n",
    "model_dir = \"Salesforce/codet5-base\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "inference_dir = Path(\"data/code_output/inference\")\n",
    "if inference_dir.exists():\n",
    "    shutil.rmtree(inference_dir)\n",
    "inference_dir.mkdir(parents=True)\n",
    "write_file(inference_dir / \"env_code_1.py\", read_file(\"data/code/env_code_1.py\"))\n",
    "write_file(inference_dir / \"env_code_2.py\", read_file(\"data/code/env_code_2.py\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9077, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from spot.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- model output ----\n",
      "<pad><s><extra_id_0>int<extra_id_1>int<extra_id_2>int<extra_id_3>int<extra_id_4>int, y : int<extra_id_5>int<extra_id_6>Optional[int]<extra_id_7>int<extra_id_8>int<extra_id_9>Bar[int, int, int, float, float]</s>\n",
      "---- checker_feedback ----\n",
      "env_code_2.py:20:14: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n",
      "env_code_2.py:32:29: error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  [arg-type]\n",
      "env_code_2.py:35:6: error: \"Bar\" expects no type arguments, but 5 given  [type-arg]\n",
      "Found 3 errors in 1 file (checked 1 source file)\n",
      "\n",
      "---- new input ----\n",
      "# Env example 2: some existing annotations\n",
      "\n",
      "from typing import *\n",
      "\n",
      "\n",
      "def fib(n: /* int */<extra_id_0>):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 1) + fib(n - 2)\n",
      "\n",
      "\n",
      "def foo(bar: /* int */<extra_id_1>):\n",
      "    return fib(bar)\n",
      "\n",
      "\n",
      "class Bar:\n",
      "    z: /* int */<extra_id_2> = /* error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  */\"hello\"\n",
      "    w: /* int */<extra_id_3>\n",
      "\n",
      "    def __init__(self, x: /* Any */<extra_id_4>):\n",
      "        self.x: /* int */<extra_id_5> = x\n",
      "        self.y: /* Optional[int] */<extra_id_6> = None\n",
      "        self.reset(self.z)\n",
      "\n",
      "    def reset(self, w0):\n",
      "        self.w = w0\n",
      "\n",
      "    def foo(self, z: /* int */<extra_id_7>) -> /* int */<extra_id_8>:\n",
      "        return self.x + len(/* error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  */z)\n",
      "\n",
      "\n",
      "bar: /* Bar[int, int, int, float, float] *//* error: \"Bar\" expects no type arguments, but 5 given  */<extra_id_9> = Bar(3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import patch_code_with_extra\n",
    "\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r[\"model_result\"][\"predicted_types\"])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted_types': [int,\n",
       "  int,\n",
       "  int,\n",
       "  int,\n",
       "  int,\n",
       "  int,\n",
       "  None,\n",
       "  int,\n",
       "  int,\n",
       "  Bar[int, int, int, float, float]],\n",
       " 'generation': '<pad><s><extra_id_0>int<extra_id_1>int<extra_id_2>int<extra_id_3>int<extra_id_4>int<extra_id_5>int<extra_id_6>None<extra_id_7>int<extra_id_8>int<extra_id_9>Bar[int, int, int, float, float]</s>'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_model_with_extra(code: str, num_beams=16):\n",
    "    input_ids = tokenizer.encode(code, return_tensors=\"pt\").to(device)\n",
    "    dec = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_target_length,\n",
    "        num_beams=num_beams,\n",
    "    )[0]\n",
    "    return {\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "    }\n",
    "\n",
    "\n",
    "run_model_with_extra(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "@dataclass\n",
      "class GitRepo:\n",
      "    author:<extra_id_0>\n",
      "    name:<extra_id_1>\n",
      "    url:<extra_id_2>\n",
      "    stars:<extra_id_3>\n",
      "    forks:<extra_id_4>\n",
      "\n",
      "    def authorname(self):\n",
      "        return self.author + \"__\" + self.name\n",
      "\n",
      "    def repo_dir(self, repos_dir:<extra_id_5>) -><extra_id_6>:\n",
      "        return repos_dir / \"downloaded\" / self.authorname()\n",
      "\n",
      "    def download(self, repos_dir:<extra_id_7>, timeout=None) -><extra_id_8>:\n",
      "        pass\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Replace all types to predict with special tokens\n",
    "print(tokenizer.decode(result[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Ċ', '@', 'data', 'class', 'Ċ', 'class', 'ĠGit', 'Repo', ':', 'Ċ', 'ĠĠĠ', 'Ġauthor', ':', '<extra_id_0>', 'Ċ', 'ĠĠĠ', 'Ġname', ':', '<extra_id_1>', 'Ċ', 'ĠĠĠ', 'Ġurl', ':', '<extra_id_2>', 'Ċ', 'ĠĠĠ', 'Ġstars', ':', '<extra_id_3>', 'Ċ', 'ĠĠĠ', 'Ġfor', 'ks', ':', '<extra_id_4>', 'Ċ', 'Ċ', 'ĠĠĠ', 'Ġdef', 'Ġauthor', 'name', '(', 'self', '):', 'Ċ', 'ĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġself', '.', 'author', 'Ġ+', 'Ġ\"__', '\"', 'Ġ+', 'Ġself', '.', 'name', 'Ċ', 'Ċ', 'ĠĠĠ', 'Ġdef', 'Ġrepo', '_', 'dir', '(', 'self', ',', 'Ġrepos', '_', 'dir', ':', '<extra_id_5>', ')', 'Ġ->', '<extra_id_6>', ':', 'Ċ', 'ĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġrepos', '_', 'dir', 'Ġ/', 'Ġ\"', 'down', 'loaded', '\"', 'Ġ/', 'Ġself', '.', 'author', 'name', '()', 'Ċ', 'Ċ', 'ĠĠĠ', 'Ġdef', 'Ġdownload', '(', 'self', ',', 'Ġrepos', '_', 'dir', ':', '<extra_id_7>', ',', 'Ġtimeout', '=', 'None', ')', 'Ġ->', '<extra_id_8>', ':', 'Ċ', 'ĠĠĠĠĠĠĠ', 'Ġpass', 'Ċ', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize using Byte Pair Encoding (BPE)\n",
    "print(tokenizer.convert_ids_to_tokens(result[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<s>', '<extra_id_0>', 'str', '<extra_id_1>', 'str', '<extra_id_2>', 'str', '<extra_id_3>', 'List', '[', 'str', ']', 'Ġ+', 'ĠList', '[', 'str', ']', '<extra_id_4>', 'List', '[', 'str', ']', 'Ġ+', 'ĠList', '[', 'str', ']', 'Ġ+', 'ĠList', '[', 'str', ']', '<extra_id_5>', 'Path', '<extra_id_6>', 'Path', 'Ġ.', 'ĠPath', '<extra_id_7>', 'Path', 'Ġ.', 'ĠPath', '<extra_id_8>', 'Path', 'Ġ.', 'ĠPath', 'Ġ[', 'Ġstr', ']', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Let model predict a sequence of types using BPE\n",
    "print(tokenizer.convert_ids_to_tokens(result[\"output_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[str, str, str, Any, Any, Path, Path.Path, Path.Path, Path.Path[str]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Extract the predicted types\n",
    "print(result[\"predicted_types\"])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
