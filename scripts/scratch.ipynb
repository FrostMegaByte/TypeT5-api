{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def gen_stub(m, rm_comments=..., rm_imports=...): ...\n",
      "def remove_empty_lines(m): ...\n",
      "OMIT = ...\n",
      "@dataclass\n",
      "class ClassNamespace:\n",
      "    all_elems: ... = ...\n",
      "    declared_elems: ... = ...\n",
      "class StubGenerator(cst.CSTTransformer):\n",
      "    def __init__(self): ...\n",
      "    def register_elem(self, name, declared): ...\n",
      "    def visit_ClassDef(self, node): ...\n",
      "    def leave_ClassDef(self, node, updated): ...\n",
      "    def leave_FunctionDef(self, node, updated): ...\n",
      "    def leave_Annotation(self, node, updated): ...\n",
      "    def leave_Param(self, node, updated): ...\n",
      "    def leave_AnnAssign(self, node, updated): ...\n",
      "    def leave_Assign(self, node, updated): ...\n",
      "    def leave_Attribute(self, node, updated): ...\n",
      "    ns_stack: ...\n",
      "class EmptyLineRemove(cst.CSTTransformer):\n",
      "    def on_leave(self, node, updated): ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, stub_from_module\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''\n",
    "from .utils import *\n",
    "from .data import remove_comments, remove_imports\n",
    "import spot\n",
    "\n",
    "\n",
    "def gen_stub(m: cst.Module, rm_comments=True, rm_imports=True) -> cst.Module:\n",
    "    \"\"\"Removes all comments and docstrings.\"\"\"\n",
    "    if rm_comments:\n",
    "        m = remove_comments(m)\n",
    "    if rm_imports:\n",
    "        m, _ = remove_imports(m)\n",
    "    m = m.visit(StubGenerator())\n",
    "    m = remove_empty_lines(m)\n",
    "    spot.fly.attach(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "def remove_empty_lines(m: cst.Module) -> cst.Module:\n",
    "    m = m.visit(EmptyLineRemove())\n",
    "    return m\n",
    "\n",
    "\n",
    "OMIT = cst.SimpleStatementSuite([cst.Expr(cst.Ellipsis())])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClassNamespace:\n",
    "    all_elems: set[str] = field(default_factory=set)\n",
    "    declared_elems: set[str] = field(default_factory=set)\n",
    "\n",
    "\n",
    "class StubGenerator(cst.CSTTransformer):\n",
    "    \"\"\"Generate a stub module from a Python module.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ns_stack = list[ClassNamespace]()\n",
    "        self.ns_stack = 1\n",
    "\n",
    "    def register_elem(self, name: str, declared: bool):\n",
    "        if self.ns_stack:\n",
    "            s = self.ns_stack[-1]\n",
    "            s.all_elems.add(name)\n",
    "            if declared:\n",
    "                s.declared_elems.add(name)\n",
    "\n",
    "    def visit_ClassDef(self, node: cst.ClassDef):\n",
    "        self.ns_stack.append(ClassNamespace())\n",
    "\n",
    "    def leave_ClassDef(self, node, updated: cst.ClassDef):\n",
    "        s = self.ns_stack.pop()\n",
    "        to_declare = s.all_elems.difference(s.declared_elems)\n",
    "        if to_declare:\n",
    "            more_stmts = [cst.parse_statement(f\"{n}: ...\") for n in to_declare]\n",
    "            new_stmts = list(updated.body.body) + more_stmts\n",
    "            updated = updated.with_changes(\n",
    "                body=updated.body.with_changes(body=new_stmts)\n",
    "            )\n",
    "        return updated\n",
    "\n",
    "    def leave_FunctionDef(self, node, updated: cst.FunctionDef):\n",
    "        self.register_elem(updated.name.value, True)\n",
    "        return updated.with_changes(body=OMIT, returns=None)\n",
    "\n",
    "    def leave_Annotation(self, node, updated: cst.Annotation):\n",
    "        return updated.with_changes(annotation=cst.Ellipsis())\n",
    "\n",
    "    def leave_Param(self, node, updated: cst.Param):\n",
    "        if updated.default is not None:\n",
    "            updated = updated.with_changes(default=cst.Ellipsis())\n",
    "        return updated.with_changes(annotation=None)\n",
    "\n",
    "    def leave_AnnAssign(self, node, updated: cst.AnnAssign):\n",
    "        if updated.value is not None:\n",
    "            updated = updated.with_changes(value=cst.Ellipsis())\n",
    "        return updated\n",
    "\n",
    "    def leave_Assign(self, node, updated: cst.AnnAssign):\n",
    "        return updated.with_changes(value=cst.Ellipsis())\n",
    "\n",
    "    def leave_Attribute(self, node, updated: cst.Assign):\n",
    "        match updated:\n",
    "            case cst.Attribute(\n",
    "                value=cst.Name(value=\"self\"),\n",
    "                attr=cst.Name(value=elem_name),\n",
    "            ):\n",
    "                self.register_elem(elem_name, False)\n",
    "        return updated\n",
    "\n",
    "\n",
    "class EmptyLineRemove(cst.CSTTransformer):\n",
    "    def on_leave(self, node, updated):\n",
    "        if hasattr(updated, \"leading_lines\") and updated.leading_lines:\n",
    "            return updated.with_changes(leading_lines=[])\n",
    "        return updated\n",
    "\n",
    "'''\n",
    "\n",
    "ex_m = cst.parse_module(ex_code)\n",
    "print(stub_from_module(ex_m).code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests.test_static_analysis/test_import_normalization\n",
      "\t spot.static_analysis/to_abs_import_path \n",
      "tests.test_static_analysis/test_namespace_resolution\n",
      "\t spot.static_analysis/ModuleNamespace.from_modules \n",
      "\t spot.static_analysis/ModuleNamespace.resolve_path   (maybe)\n",
      "tests.test_static_analysis/test_usage_analysis\n",
      "\t spot.static_analysis/PythonProject.from_modules \n",
      "\t spot.static_analysis/PythonModule.from_cst \n",
      "\t spot.static_analysis/UsageAnalysis.__init__ \n",
      "\t spot.static_analysis/ProjectPath.from_str \n",
      "\t spot.utils/assert_eq \n"
     ]
    }
   ],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).caller2callees.items():\n",
    "    if caller.module == \"tests.test_static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.callee, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.tokenized_src import PreprocessArgs, proj_root\n",
    "from spot.function_dataset import repo_to_tk_srcs, dataset_from_repos\n",
    "\n",
    "srcs = repo_to_tk_srcs(proj_root(), PreprocessArgs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk_srcs_per_file: 100%|██████████| 328/328 [00:02<00:00, 118.92it/s]\n",
      "verify_labels: 100%|██████████| 333/333 [00:00<00:00, 73852.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from spot.data import SrcDataset, CtxArgs\n",
    "\n",
    "sdata = SrcDataset(proj_root(), srcs)\n",
    "ctx_args = CtxArgs(1024, 128, 256, 512)\n",
    "cdata = sdata.to_chunks(ctx_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.utils import DefaultTokenizer\n",
    "\n",
    "DefaultTokenizer.encode(\"abs = def(x)\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= file: spot.train/TrainingConfig.get_model_name ========\n",
      "# spot.train/TrainingConfig\n",
      "def as_name(self):\n",
      "    if len(self.modified_params()) > 0:\n",
      "        return \"-\".join(\n",
      "            f\"{str(k)}={str(v)}\" for k, v in self.modified_params().items()\n",
      "        )\n",
      "    else:\n",
      "        return \"default\"\n",
      "# spot.train/TrainingConfig\n",
      "def get_model_name(self) -> <mask>:\n",
      "    return \"model-v3--\" + self.as_name()\n",
      "\n",
      "======= file: spot.train/TrainingConfig.train_ctx_args ========\n",
      "# spot.train/TrainingConfig\n",
      "def train_ctx_args(self) -> <mask>:\n",
      "    return CtxArgs(\n",
      "        ctx_size=self.ctx_size,\n",
      "        preamble_size=self.preamble_size,\n",
      "        left_margin=self.left_margin,\n",
      "        right_margin=self.right_margin,\n",
      "        max_labels=self.train_max_labels,\n",
      "        inline_prev_gold=self.inline_prev_gold,\n",
      "    )\n",
      "# spot.train/TrainingConfig\n",
      "def dec_ctx_args(self):\n",
      "    r = self.train_ctx_args()\n",
      "    r.max_labels = self.dec_max_labels\n",
      "    return r\n",
      "\n",
      "======= file: spot.train/TrainingConfig.dec_ctx_args ========\n",
      "# spot.train/TrainingConfig\n",
      "def train_ctx_args(self):\n",
      "    return CtxArgs(\n",
      "        ctx_size=self.ctx_size,\n",
      "        preamble_size=self.preamble_size,\n",
      "        left_margin=self.left_margin,\n",
      "        right_margin=self.right_margin,\n",
      "        max_labels=self.train_max_labels,\n",
      "        inline_prev_gold=self.inline_prev_gold,\n",
      "    )\n",
      "# spot.train/TrainingConfig\n",
      "def dec_ctx_args(self) -> <mask>:\n",
      "    r = self.train_ctx_args()\n",
      "    r.max_labels = self.dec_max_labels\n",
      "    return r\n",
      "\n",
      "======= file: spot.train/TrainModelWrapper.__init__ ========\n",
      "# spot.train/TrainModelWrapper\n",
      "def __init__(\n",
      "    self, model_checkpoint: <mask>, *, model_saving_path: <mask>\n",
      ") -> <mask>:\n",
      "    super().__init__()\n",
      "    self.save_hyperparameters()\n",
      "    self.model: <mask> = load_model_spot(model_checkpoint)\n",
      "    self.tokenizer: <mask> = TokenizerSPOT.from_pretrained(model_checkpoint)\n",
      "    self.model_saving_path = model_saving_path\n",
      "    self.model_saving_interval: <mask> = None\n",
      "    self.avg_loss = MovingAvg(alpha=0.01)\n",
      "    self.labels_trained = 0\n",
      "# spot.train\n",
      "def train_spot_model(\n",
      "    src_datasets,\n",
      "    model_name,\n",
      "    train_args,\n",
      "    record_batches,\n",
      "    gpus,\n",
      "    quicktest=False,\n",
      "    use_early_stop=True,\n",
      "    use_small_model=False,\n",
      "):\n",
      "    os.chdir(proj_root())\n",
      "    train_ctx_args = train_args.train_ctx_args\n",
      "    dec_args = train_args.dec_args\n",
      "\n",
      "    datadir = Path(os.getenv(\"datadir\", \"data\"))\n",
      "\n",
      "    running_dir = datadir / \"checkpoints/lit-running\" / model_name\n",
      "    if running_dir.exists():\n",
      "        shutil.rmtree(running_dir)\n",
      "    running_dir.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "    model_path = (\n",
      "        \"Salesforce/codet5-small\" if use_small_model else \"Salesforce/codet5-base\"\n",
      "    )\n",
      "    lit_model = TrainModelWrapper(model_path, model_saving_path=running_dir / \"models\")\n",
      "    tokenizer:... = lit_model.tokenizer\n",
      "    wrapper = ModelWrapper(lit_model.model, tokenizer, dec_args)\n",
      "\n",
      "    chunks:... = {}\n",
      "    with run_long_task(\"Preparing chunked datasets\", notify=False):\n",
      "        for n in [\"valid\", \"train\"]:\n",
      "            src = src_datasets[n]\n",
      "            chunks[n] = src.to_chunks(train_ctx_args)\n",
      "\n",
      "    wandb_logger = WandbLogger()  \n",
      "\n",
      "    collate_fn = DataCollatorForSeq2Seq(lit_model.tokenizer, lit_model.model)\n",
      "    train_dataloader = dynamic_dataloader(\n",
      "        cast(Any, chunks[\"train\"].data),\n",
      "        max_tokens=train_args.train_max_tokens,\n",
      "        collate_fn=collate_fn,\n",
      "        shuffle=True,\n",
      "    )\n",
      "    valid_dataloader = dynamic_dataloader(\n",
      "        cast(Any, chunks[\"valid\"].data),\n",
      "        max_tokens=train_args.eval_max_tokens,\n",
      "        collate_fn=collate_fn,\n",
      "        shuffle=True,  \n",
      "    )\n",
      "\n",
      "    ckpt_interval = max(1, len(train_dataloader) // 10)\n",
      "    val_interval = 1 if quicktest else max(500, ckpt_interval)\n",
      "\n",
      "    if record_batches:\n",
      "        lit_model.model_saving_interval = ckpt_interval\n",
      "\n",
      "    checkpoint_cb = ModelCheckpoint(\n",
      "        dirpath=running_dir,\n",
      "        save_top_k=3,\n",
      "        monitor=\"valid/loss\",\n",
      "        mode=\"min\",\n",
      "        save_on_train_epoch_end=False,\n",
      "        verbose=quicktest,\n",
      "    )\n",
      "\n",
      "    trainer = pl.Trainer(\n",
      "        default_root_dir=str(running_dir),\n",
      "        accelerator=\"gpu\" if gpus else \"cpu\",\n",
      "        gpus=gpus,\n",
      "        precision=16,\n",
      "        max_epochs=train_args.max_epochs,\n",
      "        logger=wandb_logger,\n",
      "        val_check_interval=val_interval,\n",
      "        callbacks=(\n",
      "            [checkpoint_cb, EarlyStopping(\"valid/loss\", mode=\"min\", verbose=quicktest)]\n",
      "            if use_early_stop\n",
      "            else []\n",
      "        ),\n",
      "        gradient_clip_val=1.0,\n",
      "        gradient_clip_algorithm=\"norm\",\n",
      "        accumulate_grad_batches=train_args.accumulate_grad_batches,\n",
      "    )\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", \"The dataloader.*does not have many workers.*\")\n",
      "\n",
      "    with run_long_task(f\"Training {model_name}\", notify=False):\n",
      "...\n",
      "======= file: spot.train/TrainModelWrapper.on_fit_start ========\n",
      "# spot.model/ModelWrapper\n",
      "def save_pretrained(self, path):\n",
      "    self.model.save_pretrained(str(path))\n",
      "    self.tokenizer.save_pretrained(str(path))\n",
      "    with open(path / \"args.pkl\", \"wb\") as f:\n",
      "        pickle.dump(self.args, f)\n",
      "# spot.train/TrainModelWrapper\n",
      "def on_fit_start(self):\n",
      "    if self.model_saving_interval is not None:\n",
      "        self.batch_ids: <mask> = []\n",
      "        self.saving_counter = 0\n",
      "        self.model.save_pretrained(self.model_saving_path / f\"n_batches=0\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for src in srcs[-5:]:\n",
    "    print(f\"======= file: {src.file} ========\")\n",
    "    src.print_code(max_lines=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(proj'root.file2/usage1', CodeRange(start=CodePosition(line=9, column=4), end=CodePosition(line=9, column=9)), QualifiedName(name='.file1.gf', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage1', CodeRange(start=CodePosition(line=9, column=12), end=CodePosition(line=9, column=27)), QualifiedName(name='root.file1.C', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage2', CodeRange(start=CodePosition(line=14, column=12), end=CodePosition(line=14, column=28)), QualifiedName(name='root.file1.gf_with_inner', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage2', CodeRange(start=CodePosition(line=15, column=11), end=CodePosition(line=15, column=18)), QualifiedName(name='usage2.<locals>.inner', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/usage_method1', CodeRange(start=CodePosition(line=18, column=8), end=CodePosition(line=18, column=15)), QualifiedName(name='root.file1.C', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage_method1', CodeRange(start=CodePosition(line=19, column=8), end=CodePosition(line=19, column=16)), QualifiedName(name='usage_method1.<locals>.x.foo', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/usage_method2', CodeRange(start=CodePosition(line=22, column=4), end=CodePosition(line=22, column=24)), QualifiedName(name='<method>.foo', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/usage_method2', CodeRange(start=CodePosition(line=22, column=9), end=CodePosition(line=22, column=16)), QualifiedName(name='root.file1.C', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/usage_local', CodeRange(start=CodePosition(line=25, column=4), end=CodePosition(line=25, column=13)), QualifiedName(name='usage1', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/usage_local', CodeRange(start=CodePosition(line=26, column=4), end=CodePosition(line=26, column=17)), QualifiedName(name='UsageClass', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/UsageClass.__init__', CodeRange(start=CodePosition(line=34, column=17), end=CodePosition(line=34, column=33)), QualifiedName(name='root.file1.gf_with_inner', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/UsageClass.__init__', CodeRange(start=CodePosition(line=35, column=17), end=CodePosition(line=35, column=28)), QualifiedName(name='UsageClass.__init__.<locals>.self.foo', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/UsageClass.foo', CodeRange(start=CodePosition(line=38, column=15), end=CodePosition(line=38, column=36)), QualifiedName(name='usage_local', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/UsageClass.foo', CodeRange(start=CodePosition(line=38, column=27), end=CodePosition(line=38, column=35)), QualifiedName(name='root.file1.gf', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file2/SubClass.use', CodeRange(start=CodePosition(line=46, column=8), end=CodePosition(line=46, column=19)), QualifiedName(name='SubClass.use.<locals>.self.foo', source=<QualifiedNameSource.LOCAL: 3>))\n",
      "(proj'root.file2/SubClass.use', CodeRange(start=CodePosition(line=47, column=8), end=CodePosition(line=47, column=24)), QualifiedName(name='root.file1.C.s_method', source=<QualifiedNameSource.IMPORT: 1>))\n"
     ]
    }
   ],
   "source": [
    "from spot.static_analysis import cst, PythonModule, compute_module_usages, PythonProject\n",
    "\n",
    "code1 = \"\"\"\n",
    "# root.file1\n",
    "\n",
    "# global function\n",
    "def gf(x):\n",
    "    return x * x\n",
    "\n",
    "# with inner function\n",
    "def gf_with_inner(x):\n",
    "    def inner(y):\n",
    "        return y * y\n",
    "    return inner(x)\n",
    "\n",
    "# class\n",
    "class C:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def foo(self, y):\n",
    "        return self.x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x + 1\n",
    "    \n",
    "\"\"\"\n",
    "code2 = \"\"\"\n",
    "# root.file2\n",
    "from .file1 import gf\n",
    "from root.file1 import gf_with_inner\n",
    "import root.file1\n",
    "import root.file1 as f1\n",
    "\n",
    "def usage1(x):\n",
    "    gf(x) + root.file1.C(5)\n",
    "    foo(5)\n",
    "\n",
    "def usage2(x):\n",
    "    def inner():\n",
    "        1 + gf_with_inner(x)\n",
    "    return inner()\n",
    "\n",
    "def usage_method1(x):\n",
    "    x = f1.C(5)\n",
    "    1 + x.foo(3)\n",
    "\n",
    "def usage_method2(x):\n",
    "    (1 + f1.C(5)).foo(3)\n",
    "\n",
    "def usage_local():\n",
    "    usage1(3)\n",
    "    UsageClass(4)\n",
    "\n",
    "@f1.C(1)\n",
    "def usage_dec():\n",
    "    pass\n",
    "\n",
    "class UsageClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = gf_with_inner(x)\n",
    "        self.y = self.foo(5)\n",
    "\n",
    "    def foo(self, y):\n",
    "        return usage_local(f1.gf(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x\n",
    "\n",
    "class SubClass(UsageClass):\n",
    "    def use(self):\n",
    "        self.foo(5)\n",
    "        f1.C.s_method(5)\n",
    "\"\"\"\n",
    "\n",
    "project = PythonProject.from_modules(\n",
    "    [\n",
    "        PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for u in compute_module_usages(project.modules[\"root.file2\"]):\n",
    "    print(str(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local name: gf_with_inner.<locals>.inner\n",
      "Segs: ['gf_with_inner', '<locals>', 'inner']\n",
      "Local name: usage2.<locals>.inner\n",
      "Segs: ['usage2', '<locals>', 'inner']\n",
      "Local name: usage_method1.<locals>.x.foo\n",
      "Segs: ['usage_method1', '<locals>', 'x', 'foo']\n",
      "Case 3\n",
      "Local name: <method>.foo\n",
      "Local name: usage1\n",
      "Segs: ['usage1']\n",
      "Case 1\n",
      "Local name: UsageClass\n",
      "Segs: ['UsageClass']\n",
      "Case 2\n",
      "Local name: UsageClass.__init__.<locals>.self.foo\n",
      "Segs: ['UsageClass', 'foo']\n",
      "Case 1\n",
      "Local name: usage_local\n",
      "Segs: ['usage_local']\n",
      "Case 1\n",
      "Local name: SubClass.use.<locals>.self.foo\n",
      "Segs: ['SubClass', 'foo']\n",
      "Case 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FunctionUsage(caller=proj'root.file2/SubClass.use', callee=proj'root.file1/C.foo', call_site=CodeRange(start=CodePosition(line=42, column=8), end=CodePosition(line=42, column=19)), is_certain=False),\n",
       " FunctionUsage(caller=proj'root.file2/SubClass.use', callee=proj'root.file2/UsageClass.foo', call_site=CodeRange(start=CodePosition(line=42, column=8), end=CodePosition(line=42, column=19)), is_certain=False)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.static_analysis import UsageAnalysis\n",
    "\n",
    "analysis = UsageAnalysis(project)\n",
    "analysis.caller2callees[ProjectPath(\"root.file2\", \"SubClass.use\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@wraps(function)\n",
      "def catch_permission_denied(function):\n",
      "    import some.inner.imports\n",
      "    @wraps(function)\n",
      "    def decorated(x: <mask>, y: <mask>) -> <mask>:\n",
      "        try:\n",
      "            return function(*args, **kwargs)\n",
      "\n",
      "        except InsufficientPrivilege as error:\n",
      "            LOG.error(\"Forbidden: %s\", error) \n",
      "            raise Forbidden()\n",
      "\n",
      "    return decorated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc4db6cf4c24dc09970460392d6c744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='preamble', min=1), IntSlider(value=100, description='le…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(ctx_size, preamble, left, right, max_labels=max_labels, inline_prev_gold=inline_prev)\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0,1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.data import GitRepo, ModuleRemapUnpickler\n",
    "from spot.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    collect_annotations,\n",
    "    mypy_checker,\n",
    ")\n",
    "from spot.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"spot.data\" if n == \"spot.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "from spot.utils import get_data_dir\n",
    "\n",
    "model_dir = \"Salesforce/codet5-base\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9077, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from spot.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- model output ----\n",
      "<pad><s><extra_id_0>int<extra_id_1>int<extra_id_2>int<extra_id_3>int<extra_id_4>int, y : int<extra_id_5>int<extra_id_6>Optional[int]<extra_id_7>int<extra_id_8>int<extra_id_9>Bar[int, int, int, float, float]</s>\n",
      "---- checker_feedback ----\n",
      "env_code_2.py:20:14: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n",
      "env_code_2.py:32:29: error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  [arg-type]\n",
      "env_code_2.py:35:6: error: \"Bar\" expects no type arguments, but 5 given  [type-arg]\n",
      "Found 3 errors in 1 file (checked 1 source file)\n",
      "\n",
      "---- new input ----\n",
      "# Env example 2: some existing annotations\n",
      "\n",
      "from typing import *\n",
      "\n",
      "\n",
      "def fib(n: /* int */<extra_id_0>):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 1) + fib(n - 2)\n",
      "\n",
      "\n",
      "def foo(bar: /* int */<extra_id_1>):\n",
      "    return fib(bar)\n",
      "\n",
      "\n",
      "class Bar:\n",
      "    z: /* int */<extra_id_2> = /* error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  */\"hello\"\n",
      "    w: /* int */<extra_id_3>\n",
      "\n",
      "    def __init__(self, x: /* Any */<extra_id_4>):\n",
      "        self.x: /* int */<extra_id_5> = x\n",
      "        self.y: /* Optional[int] */<extra_id_6> = None\n",
      "        self.reset(self.z)\n",
      "\n",
      "    def reset(self, w0):\n",
      "        self.w = w0\n",
      "\n",
      "    def foo(self, z: /* int */<extra_id_7>) -> /* int */<extra_id_8>:\n",
      "        return self.x + len(/* error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  */z)\n",
      "\n",
      "\n",
      "bar: /* Bar[int, int, int, float, float] *//* error: \"Bar\" expects no type arguments, but 5 given  */<extra_id_9> = Bar(3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import patch_code_with_extra\n",
    "\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r[\"model_result\"][\"predicted_types\"])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountedAcc(16.23%, count=573)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from spot.utils import Path, run_long_task, DefaultTokenizer, not_none, CountedAcc\n",
    "from spot import proj_root\n",
    "from spot.function_dataset import guess_src_root\n",
    "\n",
    "datadir = Path(not_none(os.getenv(\"datadir\")))\n",
    "repos_dir = datadir / \"SPOT-data/repos/\"\n",
    "\n",
    "repos_split_path = proj_root() /  \"data/repos_split.pkl\"\n",
    "with repos_split_path.open(\"rb\") as f:\n",
    "    repos_split = pickle.load(f)\n",
    "\n",
    "root_is_src = list[bool]()\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd = repo.repo_dir(repos_dir)\n",
    "    root_is_src.append(guess_src_root(rd).name == \"src\")\n",
    "\n",
    "CountedAcc(sum(root_is_src), len(root_is_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_projects: 573\n",
      "src_in_root: 93\n",
      "package_in_root: 203\n",
      "setup_in_root: 107\n",
      "weird_repos: 170\n"
     ]
    }
   ],
   "source": [
    "src_in_root = 0\n",
    "package_in_root = 0\n",
    "setup_in_root = 0\n",
    "n_proj = 0\n",
    "\n",
    "weird_repos = []\n",
    "setup_files = []\n",
    "\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    n_proj += 1\n",
    "    files = list(rd.iterdir())\n",
    "    if rd / \"src\" in files:\n",
    "        src_in_root += 1\n",
    "    elif rd / (pname := rd.name.split(\"__\")[-1]) in files:\n",
    "        package_in_root += 1\n",
    "    elif rd / \"setup.cfg\" in files:\n",
    "        setup_in_root += 1\n",
    "        setup_files.append(rd / \"setup.cfg\")\n",
    "    else:\n",
    "        weird_repos.append(repo)\n",
    "\n",
    "print(\"n_projects:\", n_proj)\n",
    "print(\"src_in_root:\", src_in_root)\n",
    "print(\"package_in_root:\", package_in_root)\n",
    "print(\"setup_in_root:\", setup_in_root)\n",
    "print(\"weird_repos:\", len(weird_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: downloaded/tiangolo__uvicorn-gunicorn-docker\n",
      "scripts\n",
      ".gitignore\n",
      "mypy.ini\n",
      "README.md\n",
      "tests\n",
      ".github\n",
      ".mypy_cache\n",
      ".git\n",
      "docker-images\n",
      "pyproject.toml\n",
      "LICENSE\n",
      "Repo: downloaded/uwbmrb__BMRBDep\n",
      ".gitignore\n",
      "install.sh\n",
      "ADIT-NMR Testing.ods\n",
      "README.md\n",
      "FrontEnd\n",
      "deploy.sh\n",
      "BackEnd\n",
      "nginx_configuration_example.conf\n",
      "upgrade.sh\n",
      "apache_configuration_example.conf\n",
      ".mypy_cache\n",
      ".git\n",
      "wsgi.conf\n",
      "installation.md\n",
      ".editorconfig\n",
      "Dockerfile\n",
      "build_docker.sh\n",
      "run_locally.sh\n",
      ".dockerignore\n",
      "Repo: downloaded/jfcherng__Sublime-VisualizeZeroWidthChars\n",
      "messages\n",
      "dependencies.json\n",
      "docs\n",
      ".flake8\n",
      "boot.py\n",
      "scripts\n",
      ".gitignore\n",
      "typings\n",
      "mypy.ini\n",
      ".python-version\n",
      "messages.json\n",
      "README.md\n",
      "menus\n",
      "plugin\n",
      ".github\n",
      ".mypy_cache\n",
      ".git\n",
      "pyproject.toml\n",
      "VisualizeZeroWidthChars.sublime-settings\n",
      "CHANGELOG.md\n",
      "requirements.txt\n",
      ".gitattributes\n",
      "LICENSE\n",
      ".editorconfig\n",
      "Repo: downloaded/chaosdorf__mpd-mqtt-gateway\n",
      ".gitignore\n",
      "gateway.py\n",
      ".github\n",
      "Pipfile\n",
      ".mypy_cache\n",
      "server.py\n",
      ".git\n",
      "Dockerfile\n",
      "Pipfile.lock\n",
      "Repo: downloaded/Celeo__Preston\n",
      "preston\n",
      ".gitignore\n",
      "README.md\n",
      "pytest.ini\n",
      "tests\n",
      ".github\n",
      ".mypy_cache\n",
      "poetry.lock\n",
      ".git\n",
      "pyproject.toml\n",
      "LICENSE\n",
      ".vscode\n",
      "Repo: downloaded/andrewscwei__rbc-statement-parser\n",
      "parse_csv.py\n",
      ".gitignore\n",
      "__pycache__\n",
      "README.md\n",
      "parse_tbl.py\n",
      "Pipfile\n",
      ".mypy_cache\n",
      ".git\n",
      "config\n",
      "LICENSE\n",
      "parse_pdf.py\n",
      "utils.py\n",
      "Pipfile.lock\n",
      "Repo: downloaded/devonhollowood__adventofcode\n",
      "2016\n",
      "README.md\n",
      "2019\n",
      "2017\n",
      ".mypy_cache\n",
      "2015\n",
      ".git\n",
      "LICENSE\n",
      "2018\n",
      "2021\n",
      "2020\n",
      "Repo: downloaded/tedle__uitabot\n",
      "scripts\n",
      ".gitignore\n",
      "web-client\n",
      "README.md\n",
      "CONFIG.md\n",
      "config.example.json\n",
      "COMMANDS.md\n",
      ".github\n",
      "bot\n",
      ".mypy_cache\n",
      ".git\n",
      "CHANGELOG.md\n",
      ".gitattributes\n",
      "LICENSE\n",
      "Repo: downloaded/JohnStrunk__ocs-monkey\n",
      "build.sh\n",
      "chaos_runner.py\n",
      ".gitignore\n",
      "mypy.ini\n",
      "log_gather.py\n",
      ".travis\n",
      "README.md\n",
      "setup-env.sh\n",
      "kube.py\n",
      ".github\n",
      "util.py\n",
      "event.py\n",
      ".mypy_cache\n",
      ".git\n",
      "tox.ini\n",
      "osio-workload\n",
      "workload_runner.py\n",
      "requirements.txt\n",
      "oc_in_cluster.sh\n",
      "test_kube.py\n",
      "osio.py\n",
      "LICENSE\n",
      "Dockerfile\n",
      "failure.py\n",
      "helm\n",
      "conftest.py\n",
      "log_gather_ocs.py\n",
      "failure_ocs.py\n",
      ".dockerignore\n",
      "Repo: downloaded/jmanuel1__material-search\n",
      "scripts\n",
      ".gitignore\n",
      "elements\n",
      "README.md\n",
      "package.json\n",
      "images\n",
      "manifest.json\n",
      "robots.txt\n",
      ".mypy_cache\n",
      "polymer.json\n",
      ".git\n",
      "gulpfile.js\n",
      "requirements.txt\n",
      "index.html\n",
      ".gitattributes\n",
      "yarn.lock\n",
      "favicon.ico\n",
      "styles\n",
      "test\n",
      "LICENSE.md\n"
     ]
    }
   ],
   "source": [
    "for repo in weird_repos[:10]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    print(\"Repo:\", rd.relative_to(repos_dir))\n",
    "    for f in rd.iterdir():\n",
    "        print(f.relative_to(rd))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
