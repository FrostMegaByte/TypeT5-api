{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "    body=[\n",
       "        ClassDef(\n",
       "            name=Name(\n",
       "                value='A',\n",
       "                lpar=[],\n",
       "                rpar=[],\n",
       "            ),\n",
       "            body=IndentedBlock(\n",
       "                body=[\n",
       "                    SimpleStatementLine(\n",
       "                        body=[\n",
       "                            Assign(\n",
       "                                targets=[\n",
       "                                    AssignTarget(\n",
       "                                        target=Name(\n",
       "                                            value='x',\n",
       "                                            lpar=[],\n",
       "                                            rpar=[],\n",
       "                                        ),\n",
       "                                        whitespace_before_equal=SimpleWhitespace(\n",
       "                                            value=' ',\n",
       "                                        ),\n",
       "                                        whitespace_after_equal=SimpleWhitespace(\n",
       "                                            value=' ',\n",
       "                                        ),\n",
       "                                    ),\n",
       "                                ],\n",
       "                                value=Integer(\n",
       "                                    value='5',\n",
       "                                    lpar=[],\n",
       "                                    rpar=[],\n",
       "                                ),\n",
       "                                semicolon=MaybeSentinel.DEFAULT,\n",
       "                            ),\n",
       "                        ],\n",
       "                        leading_lines=[],\n",
       "                        trailing_whitespace=TrailingWhitespace(\n",
       "                            whitespace=SimpleWhitespace(\n",
       "                                value='',\n",
       "                            ),\n",
       "                            comment=None,\n",
       "                            newline=Newline(\n",
       "                                value=None,\n",
       "                            ),\n",
       "                        ),\n",
       "                    ),\n",
       "                ],\n",
       "                header=TrailingWhitespace(\n",
       "                    whitespace=SimpleWhitespace(\n",
       "                        value='',\n",
       "                    ),\n",
       "                    comment=None,\n",
       "                    newline=Newline(\n",
       "                        value=None,\n",
       "                    ),\n",
       "                ),\n",
       "                indent=None,\n",
       "                footer=[],\n",
       "            ),\n",
       "            bases=[],\n",
       "            keywords=[],\n",
       "            decorators=[],\n",
       "            lpar=MaybeSentinel.DEFAULT,\n",
       "            rpar=MaybeSentinel.DEFAULT,\n",
       "            leading_lines=[],\n",
       "            lines_after_decorators=[],\n",
       "            whitespace_after_class=SimpleWhitespace(\n",
       "                value=' ',\n",
       "            ),\n",
       "            whitespace_after_name=SimpleWhitespace(\n",
       "                value='',\n",
       "            ),\n",
       "            whitespace_before_colon=SimpleWhitespace(\n",
       "                value='',\n",
       "            ),\n",
       "        ),\n",
       "        ClassDef(\n",
       "            name=Name(\n",
       "                value='B',\n",
       "                lpar=[],\n",
       "                rpar=[],\n",
       "            ),\n",
       "            body=IndentedBlock(\n",
       "                body=[\n",
       "                    SimpleStatementLine(\n",
       "                        body=[\n",
       "                            Pass(\n",
       "                                semicolon=MaybeSentinel.DEFAULT,\n",
       "                            ),\n",
       "                        ],\n",
       "                        leading_lines=[],\n",
       "                        trailing_whitespace=TrailingWhitespace(\n",
       "                            whitespace=SimpleWhitespace(\n",
       "                                value='',\n",
       "                            ),\n",
       "                            comment=None,\n",
       "                            newline=Newline(\n",
       "                                value=None,\n",
       "                            ),\n",
       "                        ),\n",
       "                    ),\n",
       "                ],\n",
       "                header=TrailingWhitespace(\n",
       "                    whitespace=SimpleWhitespace(\n",
       "                        value='',\n",
       "                    ),\n",
       "                    comment=None,\n",
       "                    newline=Newline(\n",
       "                        value=None,\n",
       "                    ),\n",
       "                ),\n",
       "                indent=None,\n",
       "                footer=[],\n",
       "            ),\n",
       "            bases=[\n",
       "                Arg(\n",
       "                    value=Name(\n",
       "                        value='A',\n",
       "                        lpar=[],\n",
       "                        rpar=[],\n",
       "                    ),\n",
       "                    keyword=None,\n",
       "                    equal=MaybeSentinel.DEFAULT,\n",
       "                    comma=MaybeSentinel.DEFAULT,\n",
       "                    star='',\n",
       "                    whitespace_after_star=SimpleWhitespace(\n",
       "                        value='',\n",
       "                    ),\n",
       "                    whitespace_after_arg=SimpleWhitespace(\n",
       "                        value='',\n",
       "                    ),\n",
       "                ),\n",
       "            ],\n",
       "            keywords=[],\n",
       "            decorators=[\n",
       "                Decorator(\n",
       "                    decorator=Name(\n",
       "                        value='dataclass',\n",
       "                        lpar=[],\n",
       "                        rpar=[],\n",
       "                    ),\n",
       "                    leading_lines=[],\n",
       "                    whitespace_after_at=SimpleWhitespace(\n",
       "                        value='',\n",
       "                    ),\n",
       "                    trailing_whitespace=TrailingWhitespace(\n",
       "                        whitespace=SimpleWhitespace(\n",
       "                            value='',\n",
       "                        ),\n",
       "                        comment=None,\n",
       "                        newline=Newline(\n",
       "                            value=None,\n",
       "                        ),\n",
       "                    ),\n",
       "                ),\n",
       "            ],\n",
       "            lpar=LeftParen(\n",
       "                whitespace_after=SimpleWhitespace(\n",
       "                    value='',\n",
       "                ),\n",
       "            ),\n",
       "            rpar=RightParen(\n",
       "                whitespace_before=SimpleWhitespace(\n",
       "                    value='',\n",
       "                ),\n",
       "            ),\n",
       "            leading_lines=[\n",
       "                EmptyLine(\n",
       "                    indent=True,\n",
       "                    whitespace=SimpleWhitespace(\n",
       "                        value='',\n",
       "                    ),\n",
       "                    comment=None,\n",
       "                    newline=Newline(\n",
       "                        value=None,\n",
       "                    ),\n",
       "                ),\n",
       "            ],\n",
       "            lines_after_decorators=[],\n",
       "            whitespace_after_class=SimpleWhitespace(\n",
       "                value=' ',\n",
       "            ),\n",
       "            whitespace_after_name=SimpleWhitespace(\n",
       "                value='',\n",
       "            ),\n",
       "            whitespace_before_colon=SimpleWhitespace(\n",
       "                value='',\n",
       "            ),\n",
       "        ),\n",
       "    ],\n",
       "    header=[\n",
       "        EmptyLine(\n",
       "            indent=True,\n",
       "            whitespace=SimpleWhitespace(\n",
       "                value='',\n",
       "            ),\n",
       "            comment=None,\n",
       "            newline=Newline(\n",
       "                value=None,\n",
       "            ),\n",
       "        ),\n",
       "    ],\n",
       "    footer=[],\n",
       "    encoding='utf-8',\n",
       "    default_indent='    ',\n",
       "    default_newline='\\n',\n",
       "    has_trailing_newline=True,\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.utils import cst\n",
    "\n",
    "import_code = '''\n",
    "class A:\n",
    "    x = 5\n",
    "\n",
    "@dataclass\n",
    "class B(A):\n",
    "    pass\n",
    "'''\n",
    "\n",
    "cst.parse_module(import_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModuleName = str\n",
      "ModulePath = str\n",
      "class ProjectPath(NamedTuple):\n",
      "    module: ...\n",
      "    path: ...\n",
      "    def __str__(self): ...\n",
      "    def __repr__(self): ...\n",
      "    def append(self, path): ...\n",
      "    def pop(self): ...\n",
      "    @staticmethod\n",
      "    def from_str(s): ...\n",
      "    path: ...\n",
      "    module: ...\n",
      "ProjNamespace = dict[str, ProjectPath]\n",
      "@dataclass\n",
      "class PythonFunction:\n",
      "    name: ...\n",
      "    path: ...\n",
      "    in_class: ...\n",
      "    tree: ...\n",
      "    def __repr__(self): ...\n",
      "    path: ...\n",
      "@dataclass\n",
      "class PythonVariable:\n",
      "    name: ...\n",
      "    path: ...\n",
      "    in_class: ...\n",
      "    assignments: ...  \n",
      "    def __repr__(self): ...\n",
      "    path: ...\n",
      "PythonElem = PythonFunction | PythonVariable\n",
      "@dataclass\n",
      "class PythonClass:\n",
      "    name: ...\n",
      "    path: ...\n",
      "    attributes: ...\n",
      "    methods: ...\n",
      "    tree: ...\n",
      "    superclasses: ... = ...\n",
      "    def __repr__(self): ...\n",
      "    path: ...\n",
      "    attributes: ...\n",
      "    methods: ...\n",
      "@dataclass\n",
      "class PythonModule:\n",
      "    functions: ...\n",
      "    global_vars: ...\n",
      "    classes: ...\n",
      "    name: ...\n",
      "    imported_modules: ...\n",
      "    defined_symbols: ...\n",
      "    tree: ...\n",
      "    @staticmethod\n",
      "    def from_cst(module, name): ...\n",
      "    def __repr__(self): ...\n",
      "    def all_funcs(self): ...\n",
      "    def all_vars(self): ...\n",
      "    def all_elements(self): ...\n",
      "    classes: ...\n",
      "    functions: ...\n",
      "    global_vars: ...\n",
      "@dataclass\n",
      "class PythonProject:\n",
      "    modules: ...\n",
      "    symlinks: ...\n",
      "    @staticmethod\n",
      "    def from_modules(modules): ...\n",
      "    @staticmethod\n",
      "    def from_root(\n",
      "        root,\n",
      "        discard_bad_files = ...,\n",
      "        src_filter = ...,\n",
      "        drop_comments = ...,\n",
      "        ignore_dirs = ...,\n",
      "    ): ...\n",
      "    def all_funcs(self): ...\n",
      "    def all_vars(self): ...\n",
      "    def all_elems(self): ...\n",
      "    @staticmethod\n",
      "    def rel_path_to_module_name(rel_path): ...\n",
      "    modules: ...\n",
      "def to_abs_import_path(current_mod, path): ...\n",
      "_path_segs_cache = dict[str, list[str]]()\n",
      "def split_import_path(path): ...\n",
      "@dataclass\n",
      "class ProjectUsage:\n",
      "    user: ...\n",
      "    used: ...\n",
      "    call_site: ...\n",
      "    is_certain: ...  \n",
      "    def __str__(self): ...\n",
      "    is_certain: ...\n",
      "    user: ...\n",
      "    used: ...\n",
      "class ModuleHierarchy:\n",
      "    def __init__(self): ...\n",
      "    def __repr__(self): ...\n",
      "    def add_module(self, segs): ...\n",
      "    def resolve_path(self, segs): ...\n",
      "    @staticmethod\n",
      "    def from_modules(modules): ...\n",
      "    children: ...\n",
      "def sort_modules_by_imports(project): ...\n",
      "def build_project_namespaces(\n",
      "    project,\n",
      "): ...\n",
      "class _NsBuilder(cst.CSTVisitor):\n",
      "    def __init__(self, module_path, module2ns): ...\n",
      "    def visit_ImportFrom(self, node): ...\n",
      "    namespace: ...\n",
      "    module_path: ...\n",
      "    module2ns: ...\n",
      "class UsageAnalysis:\n",
      "    all_usages: ...\n",
      "    path2elem: ...\n",
      "    user2used: ...\n",
      "    used2user: ...\n",
      "    def get_var(self, path): ...\n",
      "    def get_func(self, path): ...\n",
      "    def __init__(self, project): ...\n",
      "    def find_class(self, mname, qname): ...\n",
      "    def generate_usages(\n",
      "        self,\n",
      "        mname,\n",
      "        caller,\n",
      "        span,\n",
      "        qname,\n",
      "    ): ...\n",
      "    def assert_usages(self, caller, *callees): ...\n",
      "    sorted_modules: ...\n",
      "    name2class_member: ...\n",
      "    all_usages: ...\n",
      "    ns_hier: ...\n",
      "    path2elem: ...\n",
      "    project: ...\n",
      "    user2used: ...\n",
      "    used2user: ...\n",
      "    path2class: ...\n",
      "def compute_module_usages(mod): ...\n",
      "class _VisitType(enum.Enum):\n",
      "    Root = ...\n",
      "    Class = ...\n",
      "    Function = ...\n",
      "class PythonModuleBuilder(cst.CSTVisitor):\n",
      "    def __init__(self, module_name): ...\n",
      "    def get_module(self): ...\n",
      "    def visit_FunctionDef(self, node): ...\n",
      "    def leave_FunctionDef(self, node): ...\n",
      "    def visit_ClassDef(self, node): ...\n",
      "    def leave_ClassDef(self, node): ...\n",
      "    def visit_AnnAssign(self, node): ...\n",
      "    def visit_Assign(self, node): ...\n",
      "    def visit_Import(self, node): ...\n",
      "    def visit_ImportFrom(self, node): ...\n",
      "    def visit_Module(self, node): ...\n",
      "    def generate_init_(self, cls): ...\n",
      "    functions: ...\n",
      "    defined_symbols: ...\n",
      "    classes: ...\n",
      "    global_vars: ...\n",
      "    current_class: ...\n",
      "    visit_stack: ...\n",
      "    module: ...\n",
      "    module_name: ...\n",
      "    imported_modules: ...\n",
      "def parse_module_path(\n",
      "    path_ex, cur_mod, dots\n",
      "): ...\n",
      "class UsageRecorder(cst.CSTVisitor):\n",
      "    def __init__(\n",
      "        self,\n",
      "        name_mapping,\n",
      "        span_mapping,\n",
      "    ): ...\n",
      "    def _resolve(self, name): ...\n",
      "    def record_name_use(self, name): ...\n",
      "    def visit_Attribute(self, node): ...\n",
      "    def on_visit(self, node): ...\n",
      "    def visit_Decorator(self, node): ...\n",
      "    def visit_Annotation(self, node): ...\n",
      "    name_mapping: ...\n",
      "    span_mapping: ...\n",
      "    usages: ...\n",
      "@lru_cache(...)\n",
      "def is_access_chain(node): ...\n",
      "def stub_from_module(m, rm_comments=..., rm_imports=...): ...\n",
      "CNode = TypeVar(\"CNode\", bound=cst.CSTNode)\n",
      "def remove_imports(\n",
      "    m,\n",
      "): ...\n",
      "def remove_comments(m): ...\n",
      "def remove_empty_lines(m): ...\n",
      "def remove_types(m, type_mask=...): ...\n",
      "@dataclass\n",
      "class ClassNamespace:\n",
      "    all_elems: ... = ...\n",
      "    declared_elems: ... = ...\n",
      "class StubGenerator(cst.CSTTransformer):\n",
      "    OMIT = ...\n",
      "    def __init__(self): ...\n",
      "    def register_elem(self, name, declared): ...\n",
      "    def visit_ClassDef(self, node): ...\n",
      "    def leave_ClassDef(self, node, updated): ...\n",
      "    def visit_FunctionDef(self, node): ...\n",
      "    def leave_FunctionDef(self, node, updated): ...\n",
      "    def leave_Annotation(self, node, updated): ...\n",
      "    def leave_Param(self, node, updated): ...\n",
      "    def leave_AnnAssign(self, node, updated): ...\n",
      "    def leave_Assign(self, node, updated): ...\n",
      "    def leave_Attribute(self, node, updated): ...\n",
      "    def leave_Decorator(self, node, updated): ...\n",
      "    ns_stack: ...\n",
      "    nest_level: ...\n",
      "class EmptyLineRemove(cst.CSTTransformer):\n",
      "    def on_leave(self, node, updated): ...\n",
      "class CommentRemover(cst.CSTTransformer):\n",
      "    def leave_IndentedBlock(\n",
      "        self, node, updated\n",
      "    ): ...\n",
      "    def leave_Module(self, node, updated): ...\n",
      "    def leave_EmptyLine(self, node, updated): ...\n",
      "    def leave_TrailingWhitespace(self, node, updated): ...\n",
      "    @staticmethod\n",
      "    def is_doc_string(node): ...\n",
      "class ImportsRemover(cst.CSTTransformer):\n",
      "    def __init__(self): ...\n",
      "    def leave_Import(self, node, updated): ...\n",
      "    def leave_ImportFrom(self, node, updated): ...\n",
      "    def visit_FunctionDef(self, node): ...\n",
      "    import_stmts: ...\n",
      "class AnnotRemover(cst.CSTTransformer):\n",
      "    def __init__(self, type_mask = ...): ...\n",
      "    def leave_FunctionDef(self, node, updated): ...\n",
      "    def leave_Param(self, node, updated): ...\n",
      "    def leave_AnnAssign(self, node, updated): ...\n",
      "    type_mask: ...\n",
      "def guess_src_root(proj_root): ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import cst, read_file, proj_root\n",
    "from spot.static_analysis import stub_from_module\n",
    "\n",
    "ex_m = cst.parse_module(read_file(proj_root() / \"src/spot/static_analysis.py\"))\n",
    "print(stub_from_module(ex_m).code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spot.static_analysis/to_abs_import_path\n",
      "\t spot.static_analysis/split_import_path \n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "spot.static_analysis/split_import_path\n",
      "\t spot.static_analysis/_path_segs_cache \n",
      "spot.static_analysis/sort_modules_by_imports\n",
      "\t spot.static_analysis/PythonProject.modules   (maybe)\n",
      "\t spot.static_analysis/PythonModule.imported_modules   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.imported_modules   (maybe)\n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "spot.static_analysis/build_project_namespaces\n",
      "\t spot.static_analysis/sort_modules_by_imports \n",
      "\t spot.static_analysis/ModuleName \n",
      "\t spot.static_analysis/ProjNamespace \n",
      "\t spot.static_analysis/_NsBuilder.__init__ \n",
      "\t spot.static_analysis/PythonFunction.tree   (maybe)\n",
      "\t spot.static_analysis/PythonClass.tree   (maybe)\n",
      "\t spot.static_analysis/PythonModule.tree   (maybe)\n",
      "\t spot.static_analysis/PythonProject.modules   (maybe)\n",
      "\t spot.static_analysis/_NsBuilder.namespace   (maybe)\n",
      "\t spot.utils/MovingAvg.update   (maybe)\n",
      "\t spot.static_analysis/PythonModule.defined_symbols   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.defined_symbols   (maybe)\n",
      "spot.static_analysis/compute_module_usages\n",
      "\t spot.static_analysis/PythonFunction.tree   (maybe)\n",
      "\t spot.static_analysis/PythonClass.tree   (maybe)\n",
      "\t spot.static_analysis/PythonModule.tree   (maybe)\n",
      "\t spot.static_analysis/UsageRecorder.__init__ \n",
      "\t spot.static_analysis/PythonModule.all_funcs   (maybe)\n",
      "\t spot.static_analysis/PythonProject.all_funcs   (maybe)\n",
      "\t spot.static_analysis/UsageRecorder.usages   (maybe)\n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "\t spot.static_analysis/ProjectPath.path   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.path   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.path   (maybe)\n",
      "\t spot.static_analysis/PythonClass.path   (maybe)\n",
      "\t spot.type_env/AnnotInfo.path   (maybe)\n",
      "\t spot.type_env/CodePathManager.path   (maybe)\n",
      "\t spot.type_env/TypeInfAction.path   (maybe)\n",
      "\t spot.utils/TimeLogger.clear   (maybe)\n",
      "\t spot.utils/PickleCache.clear   (maybe)\n",
      "spot.static_analysis/parse_module_path\n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "spot.static_analysis/is_access_chain\n",
      "\t spot.static_analysis/is_access_chain \n",
      "spot.static_analysis/stub_from_module\n",
      "\t spot.static_analysis/remove_comments \n",
      "\t spot.static_analysis/remove_imports \n",
      "\t spot.static_analysis/StubGenerator.__init__ \n",
      "\t spot.static_analysis/remove_empty_lines \n",
      "spot.static_analysis/remove_imports\n",
      "\t spot.static_analysis/ImportsRemover.__init__ \n",
      "\t spot.static_analysis/ImportsRemover.import_stmts   (maybe)\n",
      "spot.static_analysis/remove_comments\n",
      "\t spot.static_analysis/CNode \n",
      "spot.static_analysis/remove_empty_lines\n",
      "\t spot.static_analysis/CNode \n",
      "spot.static_analysis/remove_types\n",
      "\t spot.static_analysis/CNode \n",
      "\t spot.static_analysis/AnnotRemover.__init__ \n",
      "spot.static_analysis/ProjectPath.__str__\n",
      "\t spot.static_analysis/ProjectPath.module \n",
      "\t spot.static_analysis/ProjectPath.path \n",
      "spot.static_analysis/ProjectPath.append\n",
      "\t spot.static_analysis/ProjectPath.module \n",
      "\t spot.static_analysis/ProjectPath.path \n",
      "spot.static_analysis/PythonFunction.__repr__\n",
      "\t spot.static_analysis/PythonFunction.path \n",
      "spot.static_analysis/PythonVariable.__repr__\n",
      "\t spot.static_analysis/PythonVariable.path \n",
      "spot.static_analysis/PythonClass.__repr__\n",
      "\t spot.static_analysis/PythonClass.path \n",
      "\t spot.static_analysis/PythonClass.attributes \n",
      "\t spot.static_analysis/PythonClass.methods \n",
      "spot.static_analysis/PythonModule.from_cst\n",
      "\t spot.static_analysis/PythonModuleBuilder.__init__ \n",
      "\t spot.static_analysis/PythonModuleBuilder.get_module   (maybe)\n",
      "spot.static_analysis/PythonModule.__repr__\n",
      "\t spot.static_analysis/PythonModule.functions \n",
      "\t spot.static_analysis/PythonModule.classes \n",
      "spot.static_analysis/PythonModule.all_funcs\n",
      "\t spot.static_analysis/PythonModule.functions \n",
      "\t spot.static_analysis/PythonModule.classes \n",
      "spot.static_analysis/PythonModule.all_vars\n",
      "\t spot.static_analysis/PythonModule.global_vars \n",
      "\t spot.static_analysis/PythonModule.classes \n",
      "spot.static_analysis/PythonProject.from_modules\n",
      "\t spot.static_analysis/PythonFunction.name   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.name   (maybe)\n",
      "\t spot.static_analysis/PythonClass.name   (maybe)\n",
      "\t spot.static_analysis/PythonModule.name   (maybe)\n",
      "\t spot.data/GitRepo.name   (maybe)\n",
      "spot.static_analysis/PythonProject.from_root\n",
      "\t spot.static_analysis/PythonProject.rel_path_to_module_name \n",
      "\t spot.static_analysis/remove_comments \n",
      "\t spot.static_analysis/PythonModule.from_cst \n",
      "spot.static_analysis/PythonProject.all_funcs\n",
      "\t spot.static_analysis/PythonModule.all_funcs   (maybe)\n",
      "\t spot.static_analysis/PythonProject.all_funcs   (maybe)\n",
      "spot.static_analysis/PythonProject.all_vars\n",
      "\t spot.static_analysis/PythonModule.all_vars   (maybe)\n",
      "\t spot.static_analysis/PythonProject.all_vars   (maybe)\n",
      "spot.static_analysis/ProjectUsage.__str__\n",
      "\t spot.static_analysis/ProjectUsage.user \n",
      "\t spot.static_analysis/ProjectUsage.is_certain \n",
      "\t spot.static_analysis/ProjectUsage.used \n",
      "spot.static_analysis/ModuleHierarchy.__init__\n",
      "\t spot.static_analysis/ModuleHierarchy.children \n",
      "spot.static_analysis/ModuleHierarchy.__repr__\n",
      "\t spot.static_analysis/ModuleHierarchy.children \n",
      "spot.static_analysis/ModuleHierarchy.add_module\n",
      "\t spot.static_analysis/ModuleHierarchy.children   (maybe)\n",
      "\t spot.static_analysis/ModuleHierarchy.__init__ \n",
      "spot.static_analysis/ModuleHierarchy.resolve_path\n",
      "\t spot.static_analysis/ModuleHierarchy.children   (maybe)\n",
      "spot.static_analysis/ModuleHierarchy.from_modules\n",
      "\t spot.static_analysis/ModuleHierarchy.__init__ \n",
      "\t spot.static_analysis/ModuleHierarchy.add_module   (maybe)\n",
      "\t spot.static_analysis/split_import_path \n",
      "spot.static_analysis/_NsBuilder.__init__\n",
      "\t spot.static_analysis/_NsBuilder.module_path \n",
      "\t spot.static_analysis/_NsBuilder.module2ns \n",
      "\t spot.static_analysis/_NsBuilder.namespace \n",
      "\t spot.static_analysis/ProjNamespace \n",
      "spot.static_analysis/_NsBuilder.visit_ImportFrom\n",
      "\t spot.static_analysis/parse_module_path \n",
      "\t spot.static_analysis/ProjectPath.module   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.module   (maybe)\n",
      "\t spot.type_env/TypeInfState.module   (maybe)\n",
      "\t spot.static_analysis/_NsBuilder.module_path \n",
      "\t spot.static_analysis/_NsBuilder.module2ns \n",
      "\t spot.utils/MovingAvg.update   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.name   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.name   (maybe)\n",
      "\t spot.static_analysis/PythonClass.name   (maybe)\n",
      "\t spot.static_analysis/PythonModule.name   (maybe)\n",
      "\t spot.data/GitRepo.name   (maybe)\n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "\t spot.static_analysis/_NsBuilder.namespace \n",
      "spot.static_analysis/UsageAnalysis.get_var\n",
      "\t spot.static_analysis/UsageAnalysis.path2elem \n",
      "spot.static_analysis/UsageAnalysis.get_func\n",
      "\t spot.static_analysis/UsageAnalysis.path2elem \n",
      "spot.static_analysis/UsageAnalysis.__init__\n",
      "\t spot.static_analysis/ProjectPath.path   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.path   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.path   (maybe)\n",
      "\t spot.static_analysis/PythonClass.path   (maybe)\n",
      "\t spot.type_env/AnnotInfo.path   (maybe)\n",
      "\t spot.type_env/CodePathManager.path   (maybe)\n",
      "\t spot.type_env/TypeInfAction.path   (maybe)\n",
      "\t spot.static_analysis/PythonModule.all_funcs   (maybe)\n",
      "\t spot.static_analysis/PythonProject.all_funcs   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.in_class   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.in_class   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.name   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.name   (maybe)\n",
      "\t spot.static_analysis/PythonClass.name   (maybe)\n",
      "\t spot.static_analysis/PythonModule.name   (maybe)\n",
      "\t spot.data/GitRepo.name   (maybe)\n",
      "\t spot.static_analysis/ProjectPath.module   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.module   (maybe)\n",
      "\t spot.type_env/TypeInfState.module   (maybe)\n",
      "\t spot.static_analysis/PythonModule.all_vars   (maybe)\n",
      "\t spot.static_analysis/PythonProject.all_vars   (maybe)\n",
      "\t data.code.good_code_1/Foo.x   (maybe)\n",
      "\t data.code.env_code_2/Bar.x   (maybe)\n",
      "\t data.code_output.inference.env_code_2/Bar.x   (maybe)\n",
      "\t spot.utils/groupby \n",
      "\t spot.static_analysis/ModuleHierarchy.from_modules \n",
      "\t spot.static_analysis/build_project_namespaces \n",
      "\t spot.static_analysis/to_abs_import_path \n",
      "\t spot.static_analysis/ModuleHierarchy.resolve_path   (maybe)\n",
      "\t spot.static_analysis/compute_module_usages \n",
      "\t spot.static_analysis/ProjectUsage.user   (maybe)\n",
      "\t spot.static_analysis/ProjectUsage.used   (maybe)\n",
      "\t spot.static_analysis/ProjectUsage.is_certain   (maybe)\n",
      "\t spot.static_analysis/UsageAnalysis.path2elem \n",
      "\t spot.static_analysis/UsageAnalysis.all_usages \n",
      "\t spot.static_analysis/UsageAnalysis.user2used \n",
      "\t spot.static_analysis/UsageAnalysis.used2user \n",
      "spot.static_analysis/PythonModuleBuilder.__init__\n",
      "\t spot.static_analysis/PythonModuleBuilder.functions \n",
      "\t spot.static_analysis/PythonModuleBuilder.global_vars \n",
      "\t spot.static_analysis/PythonModuleBuilder.classes \n",
      "\t spot.static_analysis/PythonModuleBuilder.current_class \n",
      "\t spot.static_analysis/PythonModuleBuilder.visit_stack \n",
      "\t spot.static_analysis/_VisitType.Root \n",
      "\t spot.static_analysis/PythonModuleBuilder.module \n",
      "\t spot.static_analysis/PythonModuleBuilder.imported_modules \n",
      "\t spot.static_analysis/PythonModuleBuilder.defined_symbols \n",
      "\t spot.static_analysis/PythonModuleBuilder.module_name \n",
      "spot.static_analysis/PythonModuleBuilder.get_module\n",
      "\t spot.static_analysis/PythonModuleBuilder.module \n",
      "\t spot.static_analysis/PythonModuleBuilder.module_name \n",
      "\t spot.static_analysis/PythonModuleBuilder.imported_modules \n",
      "\t spot.static_analysis/PythonModuleBuilder.defined_symbols \n",
      "spot.static_analysis/PythonModuleBuilder.visit_FunctionDef\n",
      "\t spot.static_analysis/PythonModuleBuilder.visit_stack \n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "\t spot.static_analysis/_VisitType.Function \n",
      "\t spot.static_analysis/_VisitType.Class \n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.name   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.name   (maybe)\n",
      "\t spot.static_analysis/PythonClass.name   (maybe)\n",
      "\t spot.static_analysis/PythonModule.name   (maybe)\n",
      "\t spot.data/GitRepo.name   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.current_class \n",
      "\t spot.static_analysis/PythonModuleBuilder.module_name \n",
      "\t spot.static_analysis/PythonClass.methods   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.functions \n",
      "\t spot.static_analysis/_VisitType.Root \n",
      "\t spot.static_analysis/PythonModuleBuilder.defined_symbols \n",
      "\t spot.static_analysis/ProjectPath.path   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.path   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.path   (maybe)\n",
      "\t spot.static_analysis/PythonClass.path   (maybe)\n",
      "\t spot.type_env/AnnotInfo.path   (maybe)\n",
      "\t spot.type_env/CodePathManager.path   (maybe)\n",
      "\t spot.type_env/TypeInfAction.path   (maybe)\n",
      "spot.static_analysis/PythonModuleBuilder.leave_FunctionDef\n",
      "\t spot.static_analysis/PythonModuleBuilder.visit_stack \n",
      "\t spot.static_analysis/_VisitType.Function \n",
      "\t spot.type_env/AnnotPath.pop   (maybe)\n",
      "spot.static_analysis/PythonModuleBuilder.visit_ClassDef\n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "\t spot.static_analysis/_VisitType.Class \n",
      "\t spot.static_analysis/PythonModuleBuilder.current_class \n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.module_name \n",
      "spot.static_analysis/PythonModuleBuilder.leave_ClassDef\n",
      "\t spot.static_analysis/PythonModuleBuilder.visit_stack \n",
      "\t spot.static_analysis/_VisitType.Class \n",
      "\t spot.static_analysis/PythonModuleBuilder.current_class \n",
      "\t spot.static_analysis/PythonModuleBuilder.classes \n",
      "\t spot.static_analysis/PythonFunction.name   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.name   (maybe)\n",
      "\t spot.static_analysis/PythonClass.name   (maybe)\n",
      "\t spot.static_analysis/PythonModule.name   (maybe)\n",
      "\t spot.data/GitRepo.name   (maybe)\n",
      "\t spot.static_analysis/PythonClass.methods   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.defined_symbols \n",
      "\t spot.static_analysis/ProjectPath.path   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.path   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.path   (maybe)\n",
      "\t spot.static_analysis/PythonClass.path   (maybe)\n",
      "\t spot.type_env/AnnotInfo.path   (maybe)\n",
      "\t spot.type_env/CodePathManager.path   (maybe)\n",
      "\t spot.type_env/TypeInfAction.path   (maybe)\n",
      "\t spot.type_env/AnnotPath.pop   (maybe)\n",
      "spot.static_analysis/PythonModuleBuilder.visit_AnnAssign\n",
      "\t spot.static_analysis/PythonModuleBuilder.current_class \n",
      "\t spot.static_analysis/PythonModuleBuilder.visit_stack \n",
      "\t spot.static_analysis/_VisitType.Root \n",
      "\t spot.static_analysis/PythonModuleBuilder.module_name \n",
      "\t spot.static_analysis/_VisitType.Class \n",
      "\t spot.static_analysis/PythonClass.attributes   (maybe)\n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "\t spot.static_analysis/_VisitType.Function \n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "spot.static_analysis/PythonModuleBuilder.visit_Assign\n",
      "\t spot.static_analysis/PythonModuleBuilder.current_class \n",
      "\t spot.static_analysis/PythonModuleBuilder.visit_stack \n",
      "\t spot.static_analysis/_VisitType.Root \n",
      "\t spot.static_analysis/PythonModuleBuilder.module_name \n",
      "\t spot.static_analysis/_VisitType.Class \n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "\t spot.static_analysis/_VisitType.Function \n",
      "\t spot.static_analysis/PythonClass.attributes   (maybe)\n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "spot.static_analysis/PythonModuleBuilder.visit_Import\n",
      "\t spot.static_analysis/parse_module_path \n",
      "\t spot.static_analysis/PythonFunction.name   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.name   (maybe)\n",
      "\t spot.static_analysis/PythonClass.name   (maybe)\n",
      "\t spot.static_analysis/PythonModule.name   (maybe)\n",
      "\t spot.data/GitRepo.name   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.module_name \n",
      "spot.static_analysis/PythonModuleBuilder.visit_ImportFrom\n",
      "\t spot.static_analysis/parse_module_path \n",
      "\t spot.static_analysis/ProjectPath.module   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.module   (maybe)\n",
      "\t spot.type_env/TypeInfState.module   (maybe)\n",
      "\t spot.static_analysis/PythonModuleBuilder.module_name \n",
      "spot.static_analysis/PythonModuleBuilder.visit_Module\n",
      "\t spot.static_analysis/PythonModuleBuilder.module \n",
      "spot.static_analysis/UsageRecorder.__init__\n",
      "\t spot.static_analysis/UsageRecorder.name_mapping \n",
      "\t spot.static_analysis/UsageRecorder.span_mapping \n",
      "\t spot.static_analysis/UsageRecorder.usages \n",
      "spot.static_analysis/UsageRecorder._resolve\n",
      "\t spot.static_analysis/is_access_chain \n",
      "\t spot.static_analysis/UsageRecorder.name_mapping \n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "\t spot.static_analysis/PythonFunction.name   (maybe)\n",
      "\t spot.static_analysis/PythonVariable.name   (maybe)\n",
      "\t spot.static_analysis/PythonClass.name   (maybe)\n",
      "\t spot.static_analysis/PythonModule.name   (maybe)\n",
      "\t spot.data/GitRepo.name   (maybe)\n",
      "spot.static_analysis/UsageRecorder.record_name_use\n",
      "\t spot.static_analysis/UsageRecorder._resolve \n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "\t spot.static_analysis/UsageRecorder.span_mapping \n",
      "spot.static_analysis/UsageRecorder.visit_Attribute\n",
      "\t spot.static_analysis/UsageRecorder._resolve \n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "\t spot.static_analysis/UsageRecorder.span_mapping \n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "spot.static_analysis/UsageRecorder.on_visit\n",
      "\t spot.static_analysis/UsageRecorder.record_name_use \n",
      "\t spot.static_analysis/UsageRecorder.on_visit   (maybe)\n",
      "\t spot.type_env/CodePathManager.on_visit   (maybe)\n",
      "\t spot.type_env/AnnotCollector.on_visit   (maybe)\n",
      "\t spot.type_env/AnnotApplier.on_visit   (maybe)\n",
      "spot.static_analysis/StubGenerator.__init__\n",
      "\t spot.static_analysis/StubGenerator.ns_stack \n",
      "spot.static_analysis/StubGenerator.register_elem\n",
      "\t spot.static_analysis/StubGenerator.ns_stack \n",
      "spot.static_analysis/StubGenerator.visit_ClassDef\n",
      "\t spot.static_analysis/ProjectPath.append   (maybe)\n",
      "\t spot.type_env/AnnotPath.append   (maybe)\n",
      "spot.static_analysis/StubGenerator.leave_ClassDef\n",
      "\t spot.type_env/AnnotPath.pop   (maybe)\n",
      "\t spot.static_analysis/ClassNamespace.declared_elems   (maybe)\n",
      "spot.static_analysis/StubGenerator.leave_FunctionDef\n",
      "\t spot.static_analysis/StubGenerator.register_elem \n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "\t spot.static_analysis/StubGenerator.OMIT \n",
      "spot.static_analysis/StubGenerator.leave_AnnAssign\n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "spot.static_analysis/StubGenerator.leave_Assign\n",
      "\t spot.utils/MovingAvg.value   (maybe)\n",
      "\t spot.type_env/AnnotPath.value   (maybe)\n",
      "spot.static_analysis/StubGenerator.leave_Attribute\n",
      "\t spot.static_analysis/StubGenerator.register_elem \n",
      "spot.static_analysis/CommentRemover.leave_IndentedBlock\n",
      "\t spot.static_analysis/CommentRemover.is_doc_string \n",
      "spot.static_analysis/CommentRemover.leave_Module\n",
      "\t spot.static_analysis/CommentRemover.leave_IndentedBlock \n",
      "spot.static_analysis/ImportsRemover.__init__\n",
      "\t spot.static_analysis/ImportsRemover.import_stmts \n",
      "spot.static_analysis/AnnotRemover.__init__\n",
      "\t spot.static_analysis/AnnotRemover.type_mask \n",
      "spot.static_analysis/AnnotRemover.leave_AnnAssign\n",
      "\t spot.static_analysis/AnnotRemover.type_mask \n"
     ]
    }
   ],
   "source": [
    "from spot import proj_root\n",
    "from spot.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).user2used.items():\n",
    "    if caller.module == \"spot.static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.used, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spot.tokenized_src import PreprocessArgs, proj_root\n",
    "from spot.function_dataset import repo_to_tk_srcs, dataset_from_repos\n",
    "\n",
    "srcs = repo_to_tk_srcs(proj_root(), PreprocessArgs(drop_env_types=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "map srcs_to_chunks: 100%|██████████| 555/555 [00:00<00:00, 969.82it/s] \n",
      "verify_labels: 100%|██████████| 560/560 [00:00<00:00, 45464.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from spot.data import SrcDataset, CtxArgs\n",
    "\n",
    "sdata = SrcDataset(proj_root(), srcs)\n",
    "ctx_args = CtxArgs(1024, 128, 256, 512)\n",
    "cdata = sdata.to_chunks(ctx_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= file: spot.utils/raise_error ========\n",
      "\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def raise_error(msg: <mask>) -> <mask>:  \n",
      "    raise RuntimeError(msg)\n",
      "\n",
      "# END\n",
      "\n",
      "\n",
      "======= file: spot.utils/load_model_spot ========\n",
      "# spot.model\n",
      "@dataclass\n",
      "class ModelWrapper:\n",
      "\n",
      "    @staticmethod\n",
      "    def from_pretrained(path: Path) -> \"ModelWrapper\":\n",
      "        model = cast(ModelSPOT, ModelSPOT.from_pretrained(str(path)))\n",
      "        tokenizer = TokenizerSPOT.from_pretrained(str(path))\n",
      "        with open(path / \"args.pkl\", \"rb\") as f:\n",
      "            args = pickle.load(f)\n",
      "        return ModelWrapper(\n",
      "            model=model,\n",
      "            tokenizer=tokenizer,\n",
      "            args=args,\n",
      "            monitor=TaskLoggingMonitor(path.name),\n",
      "        )\n",
      "    \n",
      "\n",
      "# spot.utils\n",
      "ModelSPOT = T5ForConditionalGeneration\n",
      "# spot.utils\n",
      "TestCounter = 1\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def load_model_spot(path) -> <mask>:\n",
      "    TestCounter * 2\n",
      "    return cast(ModelSPOT, ModelSPOT.from_pretrained(path))\n",
      "\n",
      "# END\n",
      "# scripts.train_dagger\n",
      "model = load_model_spot(\"Salesforce/codet5-base\")\n",
      "# spot.train\n",
      "def R1_srcs_from_ckpts(\n",
      "    tokenizer: TokenizerSPOT,\n",
      "    dec_args: DecodingArgs,\n",
      "    r0_src: SrcDataset,\n",
      "    cdata: ChunkedDataset,\n",
      "    chunk_ids: list[list[int]],\n",
      "    tc_args: TypeCheckArgs,\n",
      "    ckpt_dir: Path,\n",
      "    ckpt_interval: int,\n",
      "    max_workers: int,\n",
      "    device,\n",
      "    tqdm_args={},\n",
      "):\n",
      "    if (n_got := sum(len(x) for x in chunk_ids))!= len(cdata.chunks_info):\n",
      "        logging.warning(\n",
      "            f\"Some chunks are missing. Got {n_got} chunks, but expected {len(cdata.chunks_info)}\"\n",
      "        )\n",
      "    chunks_info = list[SrcChunkInfo]()\n",
      "    model_preds = list[list[PythonType]]()\n",
      "    for i in tqdm(\n",
      "        range(0, len(chunk_ids), ckpt_interval),\n",
      "        desc=\"R1_srcs_from_ckpts\",\n",
      "        **tqdm_args,\n",
      "    ):\n",
      "        ids = list(seq_flatten(chunk_ids[i : i + ckpt_interval]))\n",
      "        model = load_model_spot(ckpt_dir / f\"n_batches={i}\")\n",
      "        wrapper = ModelWrapper(model, tokenizer, dec_args)\n",
      "        wrapper = wrapper.to(device)\n",
      "        try:\n",
      "            data_sub = cdata[ids]\n",
      "        except IndexError as e:\n",
      "            raise IndexError(\n",
      "                f\"ids: {ids},\\nchunk_ids: {cdata.data['chunk_id']}\\ncdata: {cdata}\"\n",
      "            ) from e\n",
      "        chunks_info.extend(data_sub.chunks_info)\n",
      "        preds = wrapper.predict(data_sub.data, tqdm_args=tqdm_args)\n",
      "        model_preds.extend(preds)\n",
      "    srcs = R1_srcs_from_preds(\n",
      "        r0_src,\n",
      "        chunks_info,\n",
      "        cdata.files,\n",
      "        model_preds,\n",
      "        tc_args=tc_args,\n",
      "        max_workers=max_workers,\n",
      "        tqdm_args=tqdm_args,\n",
      "    )\n",
      "    return srcs, chunks_info, model_preds\n",
      "\n",
      "# spot.train\n",
      "class TrainModelWrapper(pl.LightningModule):\n",
      "\n",
      "    def __init__(\n",
      "        self, model_checkpoint: str | Path, *, model_saving_path: Path\n",
      "    ) -> None:\n",
      "        super().__init__()\n",
      "        self.save_hyperparameters()\n",
      "        self.model: ModelSPOT = load_model_spot(model_checkpoint)\n",
      "        self.tokenizer: TokenizerSPOT = TokenizerSPOT.from_pretrained(model_checkpoint)\n",
      "        self.model_saving_path = model_saving_path\n",
      "        self.model_saving_interval: Optional[int] = None\n",
      "        self.avg_loss = MovingAvg(alpha=0.01)\n",
      "        self.labels_trained = 0\n",
      "    \n",
      "\n",
      "\n",
      "======= file: spot.utils/load_tokenizer_spot ========\n",
      "# spot.model\n",
      "@dataclass\n",
      "class ModelWrapper:\n",
      "\n",
      "    @staticmethod\n",
      "    def from_pretrained(path: Path) -> \"ModelWrapper\":\n",
      "        model = cast(ModelSPOT, ModelSPOT.from_pretrained(str(path)))\n",
      "        tokenizer = TokenizerSPOT.from_pretrained(str(path))\n",
      "        with open(path / \"args.pkl\", \"rb\") as f:\n",
      "            args = pickle.load(f)\n",
      "        return ModelWrapper(\n",
      "            model=model,\n",
      "            tokenizer=tokenizer,\n",
      "            args=args,\n",
      "            monitor=TaskLoggingMonitor(path.name),\n",
      "        )\n",
      "    \n",
      "\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def load_tokenizer_spot() -> <mask>:\n",
      "    return TokenizerSPOT.from_pretrained(\"Salesforce/codet5-base\")\n",
      "\n",
      "# END\n",
      "# spot.utils\n",
      "DefaultTokenizer = load_tokenizer_spot()\n",
      "# spot.critic\n",
      "class CriticModel(nn.Module):\n",
      "    def __init__(self, t5enc: T5EncoderModel):\n",
      "        super().__init__()\n",
      "\n",
      "        self.t5enc = t5enc\n",
      "        config = t5enc.config\n",
      "\n",
      "        self.critic_classifier = nn.Linear(config.d_model, 1)\n",
      "        self.tokenizer = load_tokenizer_spot()\n",
      "        self.labels_trained = 0\n",
      "    \n",
      "\n",
      "\n",
      "======= file: spot.utils/_turn_off_tokenizer_warning ========\n",
      "\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def _turn_off_tokenizer_warning(tokenizer: <mask>):\n",
      "    tokenizer.deprecation_warnings[\n",
      "        \"sequence-length-is-longer-than-the-specified-maximum\"\n",
      "    ] = True\n",
      "\n",
      "# END\n",
      "\n",
      "\n",
      "======= file: spot.utils/with_default_workers ========\n",
      "\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "@contextmanager\n",
      "def with_default_workers(workers: <mask>):\n",
      "    global DefaultWorkers\n",
      "    old_workers = DefaultWorkers\n",
      "    DefaultWorkers = workers\n",
      "    try:\n",
      "        yield\n",
      "    finally:\n",
      "        DefaultWorkers = old_workers\n",
      "\n",
      "# END\n",
      "\n",
      "\n",
      "======= file: spot.utils/pmap ========\n",
      "# spot.type_env\n",
      "@dataclass(order=True, unsafe_hash=True)\n",
      "class AnnotPath:\n",
      "\n",
      "    def append(self, seg: str, id: int) -> \"AnnotPath\":\n",
      "        seg = seg if id == 0 else f\"{seg}[{id}]\"\n",
      "        return AnnotPath(self.value.cons(seg))\n",
      "    \n",
      "\n",
      "# spot.static_analysis\n",
      "class ProjectPath(NamedTuple):\n",
      "\n",
      "    def append(self, path: ModulePath) -> \"ProjectPath\":\n",
      "        return ProjectPath(self.module, self.path + \".\" + path)\n",
      "    \n",
      "\n",
      "# spot.utils\n",
      "def assert_eq(x: T1, *xs: T1, extra_message: Callable[[], str] = lambda: \"\") -> None:\n",
      "    for i in range(len(xs)):\n",
      "        x = xs[i - 1] if i > 0 else x\n",
      "        y = xs[i]\n",
      "        assert x == y, (\n",
      "            f\"{x} (of type {type(x).__name__})!= {y} (of type {type(y).__name__}) at equality {i}.\\n\"\n",
      "            + extra_message()\n",
      "        )\n",
      "\n",
      "# spot.utils\n",
      "T1 = TypeVar(\"T1\")\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def pmap(\n",
      "    f: <mask>,\n",
      "    *f_args: <mask>,\n",
      "    desc: <mask>,\n",
      "    max_workers: <mask> = None,\n",
      "    tqdm_args: <mask> = {},\n",
      ") -> <mask>:\n",
      "    n = len(f_args[0])\n",
      "    assert_eq(n, *(len(xs) for xs in f_args))\n",
      "\n",
      "    if max_workers is None:\n",
      "        max_workers = DefaultWorkers\n",
      "    if max_workers <= 1:\n",
      "        outs = list[T1]()\n",
      "        for i in tqdm(range(n), desc=desc, **tqdm_args):\n",
      "            outs.append(f(*(a[i] for a in f_args)))\n",
      "        return outs\n",
      "\n",
      "    chunksize = max(1, n // (10 * max_workers))\n",
      "    r = process_map(\n",
      "        f,\n",
      "        *f_args,\n",
      "        chunksize=chunksize,\n",
      "        max_workers=max_workers,\n",
      "        desc=desc,\n",
      "        tqdm_class=tqdm,\n",
      "        **tqdm_args,\n",
      "    )\n",
      "    assert isinstance(r, list)\n",
      "    return r\n",
      "\n",
      "# END\n",
      "# spot.data\n",
      "def chunk_srcs_per_file(\n",
      "    repos_root: Path,\n",
      "    srcs: Sequence[TokenizedSrc],\n",
      "    ctx_args: \"CtxArgs\",\n",
      "    tqdm_args: dict,\n",
      ") -> \"ChunkedDataset\":\n",
      "\n",
      "    srcs = [s for s in srcs if len(s.types) > 0]\n",
      "    label_ranges = [(0, len(s.types)) for s in srcs]\n",
      "    chunk_rs = pmap(\n",
      "        src_to_chunks,\n",
      "        srcs,\n",
      "        label_ranges,\n",
      "        [ctx_args] * len(srcs),\n",
      "        desc=\"map srcs_to_chunks\",\n",
      "        tqdm_args=tqdm_args,\n",
      "    )\n",
      "\n",
      "    data: dict[str, list] = {\n",
      "        \"input_ids\": [],\n",
      "        \"labels\": [],\n",
      "        \"n_labels\": [],\n",
      "        \"chunk_id\": [],\n",
      "    }\n",
      "    chunks = [chunk for r in chunk_rs for chunk in r[0]]\n",
      "    chunks_info = [info for r in chunk_rs for info in r[1]]\n",
      "\n",
      "    chunk_id = 0\n",
      "    for chunk in chunks:\n",
      "        data[\"input_ids\"].append(chunk[\"input_ids\"])\n",
      "        data[\"labels\"].append(chunk[\"labels\"])\n",
      "        data[\"n_labels\"].append(chunk[\"n_labels\"])\n",
      "        data[\"chunk_id\"].append(chunk_id)\n",
      "        chunk_id += 1\n",
      "\n",
      "    files = [(repos_root / s.file).resolve() for s in srcs]\n",
      "    return ChunkedDataset(\n",
      "        data=Dataset.from_dict(data),\n",
      "        chunks_info=chunks_info,\n",
      "        files=files,\n",
      "        file2src={f: s.main_code for f, s in zip(files, srcs)},\n",
      "        file2repo={f: (repos_root / s.repo).resolve() for f, s in zip(files, srcs)},\n",
      "    )\n",
      "\n",
      "# spot.decode\n",
      "def select_candidates_by_type_errors(\n",
      "    src_data: SrcDataset,\n",
      "    chunks: ChunkedDataset,\n",
      "    pred_candidates: list[list[list[PythonType]]],\n",
      "    only_same_file_error: bool = False,\n",
      ") -> DatasetPredResult:\n",
      "    file2src = src_data.file2src(resolve=False)\n",
      "    srcs_to_check = src_data.all_srcs\n",
      "\n",
      "    with src_data.setup_typechecking(srcs_to_check, skip_pre_fdbks=True) as env:\n",
      "        to_check = dict[tuple[int, int], tuple[TokenizedSrc, dict[int, str], Path]]()\n",
      "        for i in range(len(chunks.data)):\n",
      "            info = chunks.chunks_info[i]\n",
      "            file = info.src_file\n",
      "            src = file2src[file.relative_to(src_data.repos_root)]\n",
      "            proj_root = env.template_root / src.repo\n",
      "            for j, candidates in enumerate(pred_candidates[i]):\n",
      "                preds_dict = {\n",
      "                    l_id: str(pred) for l_id, pred in zip(info.label_ids, candidates)\n",
      "                }\n",
      "                to_check[(i, j)] = (src, preds_dict, proj_root)\n",
      "\n",
      "        to_check_values = to_check.values()\n",
      "        check_rs: list[int] = pmap(\n",
      "            count_type_errors_in_project,\n",
      "            [[x[0]] for x in to_check_values],\n",
      "            [[x[1]] for x in to_check_values],\n",
      "            [x[2] for x in to_check_values],\n",
      "            [only_same_file_error for _ in to_check_values],\n",
      "            desc=\"map count_type_errors_in_project\",\n",
      "        )\n",
      "\n",
      "    n_errors = dict(zip(to_check.keys(), check_rs))\n",
      "    final_preds = list[list[PythonType]]()\n",
      "    extra_info = list[dict]()\n",
      "    for i in range(len(chunks.data)):\n",
      "        candidates = pred_candidates[i]\n",
      "        es = [n_errors[(i, j)] for j in range(len(candidates))]\n",
      "        sample_id = int(np.argmin(es))\n",
      "        final_preds.append(candidates[sample_id])\n",
      "        extra_info.append({\"n_errors\": es[sample_id]})\n",
      "    return DatasetPredResult(chunks, final_preds, extra_info)\n",
      "\n",
      "# spot.decode\n",
      "def select_candidates_using_critic(\n",
      "    critic: CriticModel,\n",
      "    no_feedback: bool,\n",
      "    src_data: SrcDataset,\n",
      "    chunks: ChunkedDataset,\n",
      "    pred_candidates: list[list[list[PythonType]]],\n",
      "    dec_args: DecodingArgs,\n",
      "    tqdm_args: dict = {},\n",
      "    score_transform: Callable[[float], float] = lambda x: x,\n",
      ") -> DatasetPredResult[CriticAssesInfo]:\n",
      "    file2src = src_data.file2src(resolve=False)\n",
      "    srcs_to_check = src_data.all_srcs\n",
      "\n",
      "    with src_data.setup_typechecking(srcs_to_check) as env:\n",
      "        to_check = dict[tuple[int, int], tuple[TokenizedSrc, dict[int, str], Path]]()\n",
      "        for i in range(len(chunks.data)):\n",
      "            info = chunks.chunks_info[i]\n",
      "            file = info.src_file\n",
      "            src = file2src[file.relative_to(src_data.repos_root)]\n",
      "            proj_root = env.template_root / src.repo\n",
      "            for j, candidates in enumerate(pred_candidates[i]):\n",
      "                preds_dict = {\n",
      "                    l_id: str(pred) for l_id, pred in zip(info.label_ids, candidates)\n",
      "                }\n",
      "                to_check[(i, j)] = (src, preds_dict, proj_root)\n",
      "\n",
      "        to_check_values = to_check.values()\n",
      "        if no_feedback:\n",
      "            check_rs = [\n",
      "                SrcCheckResult(\"no_feedback=True\", x[0].main_code)\n",
      "                for x in to_check_values\n",
      "            ]\n",
      "        else:\n",
      "            check_rs: list[SrcCheckResult] = pmap(\n",
      "                type_check_src_in_project,\n",
      "                [x[0] for x in to_check_values],\n",
      "                [x[1] for x in to_check_values],\n",
      "                [x[2] for x in to_check_values],\n",
      "                [env.pre_fdbks[x[0].file] for x in to_check_values],\n",
      "                desc=\"map type_check_src_in_project\",\n",
      "                tqdm_args=tqdm_args,\n",
      "            )\n",
      "\n",
      "    all_fdbks = [r.feedbacks for r in check_rs if isinstance(r.feedbacks, list)]\n",
      "    success_rate = len(all_fdbks) / len(check_rs)\n",
      "    print(f\"Type checking success rate: {success_rate:.2%}\")\n",
      "\n",
      "    n_errors_map = dict[tuple[int, int], int]()\n",
      "    for k, v in zip(to_check.keys(), [r.feedbacks for r in check_rs]):\n",
      "        if isinstance(v, list):\n",
      "            n_errors_map[k] = len(v)\n",
      "        else:\n",
      "            n_errors_map[k] = 0\n",
      "\n",
      "    avg_n_fdbks = sum(n_errors_map.values()) / len(n_errors_map)\n",
      "    print(f\"Average number of feedbacks per check: {avg_n_fdbks:.2f}\")\n",
      "\n",
      "    file2id = {s.file: i for i, s in enumerate(src_data.all_srcs)}\n",
      "    src_ids = [file2id[x[0].file] for x in to_check_values]\n",
      "    critic_inputs_metas = pmap(\n",
      "        to_critic_inputs,\n",
      "        [x[0] for x in to_check_values],\n",
      "        [x[1] for x in to_check_values],\n",
      "        check_rs,\n",
      "        [dec_args.ctx_args] * len(to_check_values),\n",
      "        desc=\"map to_critic_inputs\",\n",
      "    )\n",
      "    all_inputs = [x for xs, _ in critic_inputs_metas for x in xs]\n",
      "    all_meta = [x for _, xs in critic_inputs_metas for x in xs]\n",
      "    for i, (x, info) in enumerate(zip(all_inputs, all_meta)):\n",
      "        x[\"chunk_id\"] = i\n",
      "        x[\"prediction_spans\"] = [\n",
      "            (s.start, s.stop) for s in not_none(info.inlined_spans)\n",
      "        ]\n",
      "\n",
      "    critic_dataset = Dataset.from_dict(merge_dicts(all_inputs))\n",
      "\n",
      "    dataloader = dynamic_dataloader(\n",
      "        critic_dataset,\n",
      "        max_tokens=dec_args.sampling_max_tokens,\n",
      "        collate_fn=CriticCollator(),\n",
      "    )\n",
      "    chunk2preds = critic.classify_data(\n",
      "        dataloader, len(critic_dataset), tqdm_args=tqdm_args\n",
      "    )\n",
      "    critic_preds = list[list[float]]()\n",
      "    critic_scores = list[float]()\n",
      "    for inputs, metas in critic_inputs_metas:\n",
      "        all_preds = []\n",
      "        for x, meta in zip(inputs, metas):\n",
      "            preds = chunk2preds[x[\"chunk_id\"]]\n",
      "            assert_eq(len(preds), len(not_none(meta.prev_types)))\n",
      "            all_preds.extend(preds)\n",
      "        critic_preds.append(all_preds)\n",
      "        score = np.mean([score_transform(p) for p in all_preds])\n",
      "        critic_scores.append(float(score))\n",
      "\n",
      "...\n",
      "======= file: spot.utils/read_file ========\n",
      "\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def read_file(path) -> <mask>:\n",
      "    with open(path, \"r\") as f:\n",
      "        return f.read()\n",
      "\n",
      "# END\n",
      "# tests.test_type_env\n",
      "parsed = cst.parse_module(read_file(\"data/code/bad_code_1.py\"))\n",
      "# tests.test_type_env\n",
      "def test_annotation_collection():\n",
      "    parsed = cst.parse_module(read_file(\"data/code/env_code_2.py\"))\n",
      "    annots = collect_annots_info(parsed)\n",
      "    annot_paths = [(a.path, a.cat) for a in annots]\n",
      "    correct_annot_paths: list[tuple[AnnotPath, AnnotCat]] = [\n",
      "        (annot_path(\"fib\", \"n\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"fib\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"foo\", \"bar\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"foo\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"Bar\", \"z\"), AnnotCat.ClassAtribute),\n",
      "        (annot_path(\"Bar\", \"w\"), AnnotCat.ClassAtribute),\n",
      "        (annot_path(\"Bar\", \"__init__\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"Bar\", \"__init__\", \"x\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"Bar\", \"__init__\", \"self.x\"), AnnotCat.ClassAtribute),\n",
      "        (annot_path(\"Bar\", \"__init__\", \"self.y\"), AnnotCat.ClassAtribute),\n",
      "        (annot_path(\"Bar\", \"reset\", \"w0\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"Bar\", \"reset\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"Bar\", \"foo\", \"z\"), AnnotCat.FuncArg),\n",
      "        (annot_path(\"Bar\", \"foo\", SpecialNames.Return), AnnotCat.FuncReturn),\n",
      "        (annot_path(\"bar\"), AnnotCat.GlobalVar),\n",
      "    ]\n",
      "    for pair in correct_annot_paths:\n",
      "        assert pair in annot_paths\n",
      "    for pair in annot_paths:\n",
      "        assert pair in correct_annot_paths\n",
      "\n",
      "# spot.type_env\n",
      "class TypeInfEnv:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        checker: IncrementalChekcer,\n",
      "        src_file,\n",
      "        select_annotations: Callable[..., list[AnnotInfo]],\n",
      "        check_any=False,\n",
      "        print_mypy_output=False,\n",
      "    ):\n",
      "        self.checker = checker\n",
      "        self.src_file = realpath(src_file)\n",
      "        self.original_src = read_file(src_file)\n",
      "        self.check_any = check_any\n",
      "        self.print_mypy_output = print_mypy_output\n",
      "        if TypeInfEnv.SpecialComment in self.original_src:\n",
      "            raise RuntimeError(\n",
      "                f\"The file {src_file} has already been modified by SPOT since it contains the special comment.\"\n",
      "            )\n",
      "        self.select_annotations = select_annotations\n",
      "        self.state: TypeInfState = None  \n",
      "        self.preamble = cst.parse_module(TypeInfEnv.Preamble).body\n",
      "    \n",
      "\n",
      "# spot.data\n",
      "@dataclass\n",
      "class GitRepo:\n",
      "\n",
      "    def src_files(self, repos_dir):\n",
      "        for fpath in self.repo_dir(repos_dir).glob(\"**/*.py\"):\n",
      "            yield (fpath, read_file(fpath))\n",
      "    \n",
      "\n",
      "    def collect_annotations(\n",
      "        self, repos_dir, silent=True\n",
      "    ) -> dict[Path, dict[AnnotPath, tuple[Optional[PythonType], AnnotCat]]]:\n",
      "        n_paths, n_annots = 0, 0\n",
      "        file_to_annots = dict[\n",
      "            Path, dict[AnnotPath, tuple[Optional[PythonType], AnnotCat]]\n",
      "        ]()\n",
      "        for src in self.repo_dir(repos_dir).glob(\"**/*.py\"):\n",
      "            rpath = src.relative_to(self.repo_dir(repos_dir))\n",
      "            m = cst.parse_module(read_file(src))\n",
      "            paths = collect_annots_info(m)\n",
      "            path_to_cat = {pinfo.path: pinfo.cat for pinfo in paths}\n",
      "            n_paths += len(paths)\n",
      "            annots = (info for info in paths if info.annot is not None)\n",
      "            n_annots += sum(1 for _ in annots)\n",
      "            file_to_annots[rpath] = {\n",
      "                (k := info.path): (\n",
      "                    parse_type_expr(\n",
      "                        m, cast(cst.Annotation, info.annot).annotation, silent\n",
      "                    ),\n",
      "                    path_to_cat[k],\n",
      "                )\n",
      "                for info in annots\n",
      "            }\n",
      "        self.n_type_annots = n_annots\n",
      "        self.n_type_places = n_paths\n",
      "        return file_to_annots\n",
      "    \n",
      "\n",
      "\n",
      "======= file: spot.utils/write_file ========\n",
      "\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def write_file(path, content: <mask>) -> <mask>:\n",
      "    with open(path, \"w\") as f:\n",
      "        f.write(content)\n",
      "\n",
      "# END\n",
      "# spot.visualization\n",
      "def export_preds_on_code(\n",
      "    dataset: SrcDataset | ChunkedDataset,\n",
      "    preds: list[dict] | list[list],\n",
      "    export_to: Path,\n",
      "):\n",
      "    if export_to.exists():\n",
      "        shutil.rmtree(export_to)\n",
      "    (export_to / \"chunks\").mkdir(parents=True)\n",
      "    for i in tqdm(range(len(dataset)), desc=\"Exporting\"):\n",
      "        preds_dict = preds[i]\n",
      "        if isinstance(preds_dict, list):\n",
      "            preds_dict = {k: v for k, v in enumerate(preds_dict)}\n",
      "        page = (\n",
      "            visualize_chunk(\n",
      "                dataset.data[i][\"input_ids\"],\n",
      "                preds_dict,\n",
      "                dataset.chunks_info[i].types,\n",
      "                dataset.chunks_info[i].src_file,\n",
      "            )\n",
      "            if isinstance(dataset, ChunkedDataset)\n",
      "            else visualize_chunk(\n",
      "                dataset.all_srcs[i].tokenized_code,\n",
      "                preds_dict,\n",
      "                dataset.all_srcs[i].types,\n",
      "                dataset.all_srcs[i].file,\n",
      "                contain_extra_id=False,\n",
      "            )\n",
      "        )\n",
      "        assert isinstance(page.value, str)\n",
      "        write_file(export_to / \"chunks\" / f\"chunk{i}.html\", page.value)\n",
      "\n",
      "    chunk_accs = list[CountedAcc]()\n",
      "    with tqdm(total=len(preds), desc=\"Computing accuracies\") as pbar:\n",
      "        labels_list = (\n",
      "            [info.types for info in dataset.chunks_info]\n",
      "            if isinstance(dataset, ChunkedDataset)\n",
      "            else [s.types for s in dataset.all_srcs]\n",
      "        )\n",
      "        for labels, ps in zip(labels_list, preds):\n",
      "            if isinstance(ps, list):\n",
      "                ps = {k: v for k, v in enumerate(ps)}\n",
      "            n_correct = sum(\n",
      "                normalize_type(ps[t]) == normalize_type(labels[t]) for t in ps\n",
      "            )\n",
      "            chunk_accs.append(CountedAcc(n_correct, len(ps)))\n",
      "            pbar.update()\n",
      "\n",
      "    chunk_sorted = sorted(range(len(chunk_accs)), key=lambda i: chunk_accs[i].acc)\n",
      "    links = \"\\n\".join(\n",
      "        f\"<li><a href='chunks/chunk{i}.html#prediction-0'>chunk{i} (Acc: {chunk_accs[i]})</a></li>\"\n",
      "        for i in chunk_sorted\n",
      "    )\n",
      "    index = f\"\"\" Chunks sorted by accuracy (from low to high).\n",
      "    <ol> {links} </ol>\n",
      "    \"\"\"\n",
      "    write_file(export_to / \"index.html\", index)\n",
      "    return None\n",
      "\n",
      "# tests.test_type_env\n",
      "def test_mypy_checker_2():\n",
      "    with mypy_checker(Path(\"data/code_output\"), wait_before_check=0.0) as checker:\n",
      "        if Path(\"data/code_output/bad_code_1.py\").exists():\n",
      "            os.remove(\"data/code_output/bad_code_1.py\")\n",
      "        oe = checker.recheck_project().num_errors\n",
      "        write_file(\"data/code_output/bad_code_1.py\", parsed.code)\n",
      "        assert checker.recheck_project().num_errors > oe\n",
      "        new_code = apply_annotations(parsed, code_1_patch).code\n",
      "        write_file(\n",
      "            \"data/code_output/bad_code_1.py\",\n",
      "            new_code,\n",
      "        )\n",
      "        c_r = checker.recheck_project()\n",
      "        assert c_r.num_errors == oe, f\"mypy_output: {c_r.output_str}\\ncode: {new_code}\"\n",
      "\n",
      "# spot.type_env\n",
      "class TypeInfEnv:\n",
      "\n",
      "    def restore_file(self) -> None:\n",
      "        write_file(self.src_file, self.original_src)\n",
      "    \n",
      "\n",
      "    def step(self, action: TypeInfAction) -> bool:\n",
      "        state = self.state\n",
      "        assert state is not None, \"Did you forget to call reset()?\"\n",
      "        assert (\n",
      "            action.path in state.to_annot\n",
      "        ), f\"Invalid action: path {action.path} not in `to_annot`.\"\n",
      "        type = action.type\n",
      "        mod = apply_annotations(state.module, {action.path: cst.Annotation(type)})\n",
      "        write_file(self.src_file, mod.code)\n",
      "        out = self.checker.recheck_files(self.src_file)\n",
      "        if isinstance(out, str):\n",
      "            raise RuntimeError(out)\n",
      "\n",
      "        if self.print_mypy_output:\n",
      "            print(\"action: \", action)\n",
      "            print(\"mypy output:\", out.output_str)\n",
      "\n",
      "        rejected = out.num_errors > state.num_errors\n",
      "        if rejected:\n",
      "            type = cst.parse_expression(f\"Annotated[Any, {mod.code_for_node(type)}]\")\n",
      "            mod = apply_annotations(mod, {action.path: cst.Annotation(type)})\n",
      "            write_file(self.src_file, mod.code)\n",
      "            if self.check_any:\n",
      "                out = self.checker.recheck_files(self.src_file)\n",
      "                if isinstance(out, str):\n",
      "                    raise RuntimeError(out)\n",
      "                assert out.num_errors == state.num_errors, (\n",
      "                    \"Adding Any should not trigger more type errors.\\n\"\n",
      "                    f\"original errors: {state.num_errors}, new errors: {out.num_errors}\\n\"\n",
      "                    f\"action: {action}\\n\"\n",
      "                    f\"mypy output: {out.output_str}\\n\"\n",
      "                    f\"---------Code---------\\n {mod.code}\\n\"\n",
      "                )\n",
      "        state.to_annot.pop(action.path)\n",
      "        state.annotated[action.path] = type\n",
      "        state.module = mod\n",
      "        return rejected\n",
      "    \n",
      "\n",
      "\n",
      "======= file: spot.utils/proj_root ========\n",
      "\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def proj_root() -> <mask>:\n",
      "    return Path(__file__).parent.parent.parent\n",
      "\n",
      "# END\n",
      "# spot.utils\n",
      "def get_data_dir() -> Path:\n",
      "    if (v := os.getenv(\"datadir\")) is not None:\n",
      "        return Path(v)\n",
      "    else:\n",
      "        return proj_root() / \"data\"\n",
      "\n",
      "# spot.utils\n",
      "def pushover_alert(\n",
      "    title: str, message: str, print_to_console: bool = True, notify: bool = True\n",
      ") -> None:\n",
      "\n",
      "    conn = http.client.HTTPSConnection(\"api.pushover.net:443\")\n",
      "    config_file = proj_root() / \"config/pushover.json\"\n",
      "    if print_to_console:\n",
      "        print(f\"Pushover: ({title}) {message}\")\n",
      "    if not config_file.exists():\n",
      "        print(\n",
      "            f\"No pushover config file found at {config_file}. Not able to push message.\"\n",
      "        )\n",
      "    elif notify:\n",
      "        config = json.loads(config_file.read_text())\n",
      "        conn.request(\n",
      "            \"POST\",\n",
      "            \"/1/messages.json\",\n",
      "            urllib.parse.urlencode(  \n",
      "                {\n",
      "                    \"token\": config[\"token\"],\n",
      "                    \"user\": config[\"user\"],\n",
      "                    \"title\": title,\n",
      "                    \"message\": message,\n",
      "                }\n",
      "            ),\n",
      "            {\"Content-type\": \"application/x-www-form-urlencoded\"},\n",
      "        )\n",
      "        conn.getresponse()\n",
      "\n",
      "# spot.type_check\n",
      "@contextmanager\n",
      "def mypy_checker(code_dir: Path, dmypy_path: Path | None = None, wait_before_check=1.0):\n",
      "    try:\n",
      "        if dmypy_path is None:\n",
      "            dmypy_path = proj_root() / \".venv/bin/dmypy\"\n",
      "        yield (\n",
      "            checker := IncrementalChekcer(\n",
      "                dmypy_path, code_dir, wait_before_check=wait_before_check\n",
      "            )\n",
      "        )\n",
      "    finally:\n",
      "        checker.close()\n",
      "\n",
      "# spot.train\n",
      "def train_spot_model(\n",
      "    src_datasets: dict[str, SrcDataset],\n",
      "    model_name: str,\n",
      "    train_args: ModelTrainingArgs,\n",
      "    record_batches: bool,\n",
      "    gpus: list[int],\n",
      "    quicktest=False,\n",
      "    use_early_stop=True,\n",
      "    use_small_model=False,\n",
      ") -> tuple[ModelWrapper, dict]:\n",
      "    os.chdir(proj_root())\n",
      "    train_ctx_args = train_args.train_ctx_args\n",
      "    dec_args = train_args.dec_args\n",
      "\n",
      "    datadir = Path(os.getenv(\"datadir\", \"data\"))\n",
      "\n",
      "    running_dir = datadir / \"checkpoints/lit-running\" / model_name\n",
      "    if running_dir.exists():\n",
      "        shutil.rmtree(running_dir)\n",
      "    running_dir.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "    model_path = (\n",
      "        \"Salesforce/codet5-small\" if use_small_model else \"Salesforce/codet5-base\"\n",
      "    )\n",
      "    lit_model = TrainModelWrapper(model_path, model_saving_path=running_dir / \"models\")\n",
      "    tokenizer: TokenizerSPOT = lit_model.tokenizer\n",
      "    wrapper = ModelWrapper(lit_model.model, tokenizer, dec_args)\n",
      "\n",
      "    chunks: dict[str, ChunkedDataset] = {}\n",
      "    with run_long_task(\"Preparing chunked datasets\", notify=False):\n",
      "        for n in [\"valid\", \"train\"]:\n",
      "            src = src_datasets[n]\n",
      "            chunks[n] = src.to_chunks(train_ctx_args)\n",
      "\n",
      "    wandb_logger = WandbLogger()  \n",
      "\n",
      "    collate_fn = DataCollatorForSeq2Seq(lit_model.tokenizer, lit_model.model)\n",
      "    train_dataloader = dynamic_dataloader(\n",
      "        cast(Any, chunks[\"train\"].data),\n",
      "        max_tokens=train_args.train_max_tokens,\n",
      "        collate_fn=collate_fn,\n",
      "        shuffle=True,\n",
      "    )\n",
      "    valid_dataloader = dynamic_dataloader(\n",
      "        cast(Any, chunks[\"valid\"].data),\n",
      "        max_tokens=train_args.eval_max_tokens,\n",
      "        collate_fn=collate_fn,\n",
      "        shuffle=True,  \n",
      "    )\n",
      "\n",
      "    ckpt_interval = max(1, len(train_dataloader) // 10)\n",
      "    val_interval = 1 if quicktest else max(500, ckpt_interval)\n",
      "\n",
      "    if record_batches:\n",
      "        lit_model.model_saving_interval = ckpt_interval\n",
      "\n",
      "    checkpoint_cb = ModelCheckpoint(\n",
      "        dirpath=running_dir,\n",
      "        save_top_k=3,\n",
      "        monitor=\"valid/loss\",\n",
      "        mode=\"min\",\n",
      "        save_on_train_epoch_end=False,\n",
      "        verbose=quicktest,\n",
      "    )\n",
      "\n",
      "    trainer = pl.Trainer(\n",
      "        default_root_dir=str(running_dir),\n",
      "        accelerator=\"gpu\" if gpus else \"cpu\",\n",
      "        gpus=gpus,\n",
      "        precision=16,\n",
      "        max_epochs=train_args.max_epochs,\n",
      "        logger=wandb_logger,\n",
      "        val_check_interval=val_interval,\n",
      "        callbacks=(\n",
      "            [checkpoint_cb, EarlyStopping(\"valid/loss\", mode=\"min\", verbose=quicktest)]\n",
      "            if use_early_stop\n",
      "            else []\n",
      "        ),\n",
      "        gradient_clip_val=1.0,\n",
      "        gradient_clip_algorithm=\"norm\",\n",
      "        accumulate_grad_batches=train_args.accumulate_grad_batches,\n",
      "    )\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", \"The dataloader.*does not have many workers.*\")\n",
      "\n",
      "    with run_long_task(f\"Training {model_name}\", notify=False):\n",
      "        trainer.fit(\n",
      "            model=lit_model,\n",
      "            train_dataloaders=train_dataloader,\n",
      "            val_dataloaders=valid_dataloader,\n",
      "        )\n",
      "\n",
      "    extra = dict[str, Any]()\n",
      "    save_dir = datadir / \"checkpoints/lit-saved\" / model_name\n",
      "\n",
      "    final_eval = trainer.validate(model=lit_model, dataloaders=valid_dataloader)[0]\n",
      "\n",
      "    try:\n",
      "        if (\n",
      "            use_early_stop\n",
      "            and (best_loss := checkpoint_cb.best_model_score) is not None\n",
      "            and best_loss < final_eval[\"valid/loss\"]\n",
      "        ):\n",
      "            print(\n",
      "                f\"Loading best model with score {best_loss} from: {checkpoint_cb.best_model_path}\"\n",
      "            )\n",
      "            wrapper.model = TrainModelWrapper.load_from_checkpoint(\n",
      "                checkpoint_cb.best_model_path\n",
      "            ).model\n",
      "        if save_dir.exists():\n",
      "            shutil.rmtree(save_dir)\n",
      "        save_dir.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "        wrapper.save_pretrained(save_dir)\n",
      "        if record_batches:\n",
      "            device = torch.device(f\"cuda:{gpus[0]}\" if gpus else \"cpu\")\n",
      "            wrapper = wrapper.to(device)\n",
      "            extra[\"batch_ids\"] = lit_model.batch_ids\n",
      "            with run_long_task(\"Generating R1 datasets\", notify=False):\n",
      "                R1_src_datasets = R1_srcs_from_extra(\n",
      "                    wrapper,\n",
      "                    src_datasets,\n",
      "                    extra,\n",
      "                    tc_args=train_args.tc_args,\n",
      "                    ckpt_dir=running_dir / \"models\",\n",
      "                    ckpt_interval=ckpt_interval,\n",
      "                )\n",
      "\n",
      "            extra[\"R1-src_datasets\"] = R1_src_datasets\n",
      "            pickle_dump(save_dir / \"extra.pkl\", extra)\n",
      "\n",
      "            shutil.rmtree(running_dir)\n",
      "    except Exception as e:\n",
      "        logging.error(\n",
      "            \"Error encountered after training, returning partial results... Error:\\n\", e\n",
      "        )\n",
      "\n",
      "    return wrapper, extra\n",
      "\n",
      "# spot.critic\n",
      "def train_critic_model(\n",
      "    critic_datasets: dict[str, SrcDataset],\n",
      "    train_args: CriticTrainArgs,\n",
      "    model_name: str,\n",
      "    gpus: list[int],\n",
      "    quicktest=False,\n",
      "    use_early_stop=True,\n",
      "    use_small_model=False,\n",
      ") -> tuple[CriticModel, dict]:\n",
      "    os.chdir(proj_root())\n",
      "\n",
      "    datadir = Path(os.getenv(\"datadir\", \"data\"))\n",
      "\n",
      "    running_dir = datadir / \"checkpoints/lit-running\" / model_name\n",
      "    if running_dir.exists():\n",
      "        shutil.rmtree(running_dir)\n",
      "    running_dir.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "    datasets: dict[str, Dataset] = {}\n",
      "    for n in [\"valid\", \"test\", \"train\"]:\n",
      "        sdata = critic_datasets[n]\n",
      "        cdata = sdata.to_chunks(train_args.ctx_args)\n",
      "        datasets[n] = to_critic_dataset(cdata)\n",
      "\n",
      "    pos_weight = 1.0\n",
      "    assert math.isfinite(pos_weight), f\"pos_weight = {pos_weight}\"\n",
      "\n",
      "    model_path = (\n",
      "        \"Salesforce/codet5-small\" if use_small_model else \"Salesforce/codet5-base\"\n",
      "    )\n",
      "    model = CriticModel.from_code_t5(model_path)\n",
      "    lit_model = TrainCriticModelWrapper(model, pos_weight, running_dir)\n",
      "\n",
      "    wandb_logger = WandbLogger()  \n",
      "    wandb_logger.log_hyperparams({\"pos_weight\": pos_weight})\n",
      "    print(f\"pos_weight = {pos_weight}\")\n",
      "\n",
      "    collate_fn = CriticCollator()\n",
      "    dataloaders = dict[str, DataLoader]()\n",
      "    for n, data in datasets.items():\n",
      "        dataloaders[n] = dynamic_dataloader(\n",
      "            data,\n",
      "            max_tokens=(\n",
      "                train_args.train_max_tokens\n",
      "                if n == \"train\"\n",
      "                else train_args.eval_max_tokens\n",
      "            ),\n",
      "            collate_fn=collate_fn,\n",
      "            shuffle=True,\n",
      "        )\n",
      "...\n",
      "======= file: spot.utils/get_data_dir ========\n",
      "# spot.utils\n",
      "def proj_root() -> Path:\n",
      "    return Path(__file__).parent.parent.parent\n",
      "\n",
      "# BEGIN\n",
      "# spot.utils\n",
      "def get_data_dir() -> <mask>:\n",
      "    if (v := os.getenv(\"datadir\")) is not None:\n",
      "        return Path(v)\n",
      "    else:\n",
      "        return proj_root() / \"data\"\n",
      "\n",
      "# END\n",
      "# scripts.train_dagger\n",
      "datadir = get_data_dir()\n",
      "# scripts.train_model\n",
      "datadir = get_data_dir()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for src in srcs[:10]:\n",
    "    print(f\"======= file: {src.file} ========\")\n",
    "    src.print_code(max_lines=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quicktest': bool,\n",
       " 'func_only': bool,\n",
       " 'pre_args': spot.tokenized_src.PreprocessArgs,\n",
       " 'data_reduction': int,\n",
       " 'check_in_isolation': bool,\n",
       " 'all_labels': bool,\n",
       " 'inline_prev_gold': bool,\n",
       " 'ctx_size': int,\n",
       " 'left_margin': int,\n",
       " 'preamble_size': int,\n",
       " 'right_margin': int,\n",
       " 'train_max_labels': int,\n",
       " 'dec_max_labels': int,\n",
       " 'use_small_model': bool,\n",
       " 'modifications': str}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.train import TrainingConfig\n",
    "\n",
    "TrainingConfig(quicktest=True).__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TrainingConfig(imports_in_preamble=False)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.data import PreprocessArgs, repr_modified_args\n",
    "\n",
    "repr_modified_args(TrainingConfig(pre_args=PreprocessArgs(imports_in_preamble=False)), flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(proj'root.file3/usage1', CodeRange(start=CodePosition(line=6, column=4), end=CodePosition(line=6, column=9)), QualifiedName(name='gf', source=<QualifiedNameSource.IMPORT: 1>))\n",
      "(proj'root.file3/usage1', CodeRange(start=CodePosition(line=7, column=4), end=CodePosition(line=7, column=8)), QualifiedName(name='C', source=<QualifiedNameSource.IMPORT: 1>))\n"
     ]
    }
   ],
   "source": [
    "from spot.static_analysis import cst, PythonModule, compute_module_usages, PythonProject\n",
    "\n",
    "code1 = \"\"\"\n",
    "# root.file1\n",
    "\n",
    "# global function\n",
    "def gf(x):\n",
    "    return x * x\n",
    "\n",
    "# with inner function\n",
    "def gf_with_inner(x):\n",
    "    def inner(y):\n",
    "        return y * y\n",
    "    return inner(x)\n",
    "\n",
    "# class\n",
    "class C:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def foo(self, y):\n",
    "        return self.x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x + 1\n",
    "    \n",
    "\"\"\"\n",
    "code2 = \"\"\"\n",
    "# root.file2\n",
    "from .file1 import gf\n",
    "from root.file1 import gf_with_inner\n",
    "import root.file1\n",
    "import root.file1 as f1\n",
    "\n",
    "def usage1(x):\n",
    "    gf(x) + root.file1.C(5)\n",
    "    foo(5)\n",
    "\n",
    "def usage2(x):\n",
    "    def inner():\n",
    "        1 + gf_with_inner(x)\n",
    "    return inner()\n",
    "\n",
    "def usage_method1(x):\n",
    "    x = f1.C(5)\n",
    "    1 + x.foo(3)\n",
    "\n",
    "def usage_method2(x):\n",
    "    (1 + f1.C(5)).foo(3)\n",
    "\n",
    "def usage_local():\n",
    "    usage1(3)\n",
    "    UsageClass(4)\n",
    "\n",
    "@f1.C(1)\n",
    "def usage_dec():\n",
    "    pass\n",
    "\n",
    "class UsageClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = gf_with_inner(x)\n",
    "        self.y = self.foo(5)\n",
    "\n",
    "    def foo(self, y):\n",
    "        return usage_local(f1.gf(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x\n",
    "\n",
    "class SubClass(UsageClass):\n",
    "    def use(self):\n",
    "        self.foo(5)\n",
    "        f1.C.s_method(5)\n",
    "\"\"\"\n",
    "\n",
    "code3 = \"\"\"\n",
    "# root.file3\n",
    "from .file1 import *\n",
    "\n",
    "def usage1(x):\n",
    "    gf(5)\n",
    "    C(5)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "project = PythonProject.from_modules(\n",
    "        [\n",
    "            PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
    "            PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
    "            PythonModule.from_cst(cst.parse_module(code3), \"root.file3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "for u in compute_module_usages(project.modules[\"root.file3\"]):\n",
    "    print(str(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root.file1': {'gf': proj'root.file1/gf',\n",
       "  'C': proj'root.file1/C',\n",
       "  'gf_with_inner': proj'root.file1/gf_with_inner'},\n",
       " 'root.file2': {'gf': proj'root.file1/gf',\n",
       "  'gf_with_inner': proj'root.file1/gf_with_inner',\n",
       "  'usage2': proj'root.file2/usage2',\n",
       "  'UsageClass': proj'root.file2/UsageClass',\n",
       "  'usage_method1': proj'root.file2/usage_method1',\n",
       "  'usage_method2': proj'root.file2/usage_method2',\n",
       "  'usage_local': proj'root.file2/usage_local',\n",
       "  'SubClass': proj'root.file2/SubClass',\n",
       "  'usage1': proj'root.file2/usage1',\n",
       "  'usage_dec': proj'root.file2/usage_dec'},\n",
       " 'root.file3': {'gf': proj'root.file1/gf',\n",
       "  'C': proj'root.file1/C',\n",
       "  'gf_with_inner': proj'root.file1/gf_with_inner',\n",
       "  'usage1': proj'root.file3/usage1'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.static_analysis import build_project_namespaces\n",
    "build_project_namespaces(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(proj'root.file3/usage1',\n",
       "  CodeRange(start=CodePosition(line=6, column=4), end=CodePosition(line=6, column=9)),\n",
       "  QualifiedName(name='gf', source=<QualifiedNameSource.IMPORT: 1>)),\n",
       " (proj'root.file3/usage1',\n",
       "  CodeRange(start=CodePosition(line=7, column=4), end=CodePosition(line=7, column=8)),\n",
       "  QualifiedName(name='C', source=<QualifiedNameSource.IMPORT: 1>))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.static_analysis import compute_module_usages\n",
    "\n",
    "compute_module_usages(project.modules[\"root.file3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local name: gf_with_inner.<locals>.inner\n",
      "Segs: ['gf_with_inner', '<locals>', 'inner']\n",
      "Local name: usage2.<locals>.inner\n",
      "Segs: ['usage2', '<locals>', 'inner']\n",
      "Local name: usage_method1.<locals>.x.foo\n",
      "Segs: ['usage_method1', '<locals>', 'x', 'foo']\n",
      "Case 3\n",
      "Local name: <method>.foo\n",
      "Local name: usage1\n",
      "Segs: ['usage1']\n",
      "Case 1\n",
      "Local name: UsageClass\n",
      "Segs: ['UsageClass']\n",
      "Case 2\n",
      "Local name: UsageClass.__init__.<locals>.self.foo\n",
      "Segs: ['UsageClass', 'foo']\n",
      "Case 1\n",
      "Local name: usage_local\n",
      "Segs: ['usage_local']\n",
      "Case 1\n",
      "Local name: SubClass.use.<locals>.self.foo\n",
      "Segs: ['SubClass', 'foo']\n",
      "Case 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FunctionUsage(caller=proj'root.file2/SubClass.use', callee=proj'root.file1/C.foo', call_site=CodeRange(start=CodePosition(line=42, column=8), end=CodePosition(line=42, column=19)), is_certain=False),\n",
       " FunctionUsage(caller=proj'root.file2/SubClass.use', callee=proj'root.file2/UsageClass.foo', call_site=CodeRange(start=CodePosition(line=42, column=8), end=CodePosition(line=42, column=19)), is_certain=False)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.static_analysis import UsageAnalysis, build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n",
    "\n",
    "analysis = UsageAnalysis(project)\n",
    "analysis.caller2callees[ProjectPath(\"root.file2\", \"SubClass.use\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@wraps(function)\n",
      "def catch_permission_denied(function):\n",
      "    import some.inner.imports\n",
      "    @wraps(function)\n",
      "    def decorated(x: <mask>, y: <mask>) -> <mask>:\n",
      "        try:\n",
      "            return function(*args, **kwargs)\n",
      "\n",
      "        except InsufficientPrivilege as error:\n",
      "            LOG.error(\"Forbidden: %s\", error) \n",
      "            raise Forbidden()\n",
      "\n",
      "    return decorated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from spot.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc4db6cf4c24dc09970460392d6c744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='preamble', min=1), IntSlider(value=100, description='le…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spot.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(ctx_size, preamble, left, right, max_labels=max_labels, inline_prev_gold=inline_prev)\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0,1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.data import GitRepo, ModuleRemapUnpickler\n",
    "from spot.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    mypy_checker,\n",
    ")\n",
    "from spot.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"spot.data\" if n == \"spot.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "from spot.utils import get_data_dir\n",
    "\n",
    "model_dir = \"Salesforce/codet5-base\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9077, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from spot.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- model output ----\n",
      "<pad><s><extra_id_0>int<extra_id_1>int<extra_id_2>int<extra_id_3>int<extra_id_4>int, y : int<extra_id_5>int<extra_id_6>Optional[int]<extra_id_7>int<extra_id_8>int<extra_id_9>Bar[int, int, int, float, float]</s>\n",
      "---- checker_feedback ----\n",
      "env_code_2.py:20:14: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n",
      "env_code_2.py:32:29: error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  [arg-type]\n",
      "env_code_2.py:35:6: error: \"Bar\" expects no type arguments, but 5 given  [type-arg]\n",
      "Found 3 errors in 1 file (checked 1 source file)\n",
      "\n",
      "---- new input ----\n",
      "# Env example 2: some existing annotations\n",
      "\n",
      "from typing import *\n",
      "\n",
      "\n",
      "def fib(n: /* int */<extra_id_0>):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 1) + fib(n - 2)\n",
      "\n",
      "\n",
      "def foo(bar: /* int */<extra_id_1>):\n",
      "    return fib(bar)\n",
      "\n",
      "\n",
      "class Bar:\n",
      "    z: /* int */<extra_id_2> = /* error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  */\"hello\"\n",
      "    w: /* int */<extra_id_3>\n",
      "\n",
      "    def __init__(self, x: /* Any */<extra_id_4>):\n",
      "        self.x: /* int */<extra_id_5> = x\n",
      "        self.y: /* Optional[int] */<extra_id_6> = None\n",
      "        self.reset(self.z)\n",
      "\n",
      "    def reset(self, w0):\n",
      "        self.w = w0\n",
      "\n",
      "    def foo(self, z: /* int */<extra_id_7>) -> /* int */<extra_id_8>:\n",
      "        return self.x + len(/* error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  */z)\n",
      "\n",
      "\n",
      "bar: /* Bar[int, int, int, float, float] *//* error: \"Bar\" expects no type arguments, but 5 given  */<extra_id_9> = Bar(3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import patch_code_with_extra\n",
    "\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r[\"model_result\"][\"predicted_types\"])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountedAcc(16.23%, count=573)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from spot.utils import Path, run_long_task, DefaultTokenizer, not_none, CountedAcc\n",
    "from spot import proj_root\n",
    "from spot.function_dataset import guess_src_root\n",
    "\n",
    "datadir = Path(not_none(os.getenv(\"datadir\")))\n",
    "repos_dir = datadir / \"SPOT-data/repos/\"\n",
    "\n",
    "repos_split_path = proj_root() /  \"data/repos_split.pkl\"\n",
    "with repos_split_path.open(\"rb\") as f:\n",
    "    repos_split = pickle.load(f)\n",
    "\n",
    "root_is_src = list[bool]()\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd = repo.repo_dir(repos_dir)\n",
    "    root_is_src.append(guess_src_root(rd).name == \"src\")\n",
    "\n",
    "CountedAcc(sum(root_is_src), len(root_is_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_projects: 573\n",
      "src_in_root: 93\n",
      "package_in_root: 203\n",
      "setup_in_root: 107\n",
      "weird_repos: 170\n"
     ]
    }
   ],
   "source": [
    "src_in_root = 0\n",
    "package_in_root = 0\n",
    "setup_in_root = 0\n",
    "n_proj = 0\n",
    "\n",
    "weird_repos = []\n",
    "setup_files = []\n",
    "\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    n_proj += 1\n",
    "    files = list(rd.iterdir())\n",
    "    if rd / \"src\" in files:\n",
    "        src_in_root += 1\n",
    "    elif rd / (pname := rd.name.split(\"__\")[-1]) in files:\n",
    "        package_in_root += 1\n",
    "    elif rd / \"setup.cfg\" in files:\n",
    "        setup_in_root += 1\n",
    "        setup_files.append(rd / \"setup.cfg\")\n",
    "    else:\n",
    "        weird_repos.append(repo)\n",
    "\n",
    "print(\"n_projects:\", n_proj)\n",
    "print(\"src_in_root:\", src_in_root)\n",
    "print(\"package_in_root:\", package_in_root)\n",
    "print(\"setup_in_root:\", setup_in_root)\n",
    "print(\"weird_repos:\", len(weird_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: downloaded/tiangolo__uvicorn-gunicorn-docker\n",
      "scripts\n",
      ".gitignore\n",
      "mypy.ini\n",
      "README.md\n",
      "tests\n",
      ".github\n",
      ".mypy_cache\n",
      ".git\n",
      "docker-images\n",
      "pyproject.toml\n",
      "LICENSE\n",
      "Repo: downloaded/uwbmrb__BMRBDep\n",
      ".gitignore\n",
      "install.sh\n",
      "ADIT-NMR Testing.ods\n",
      "README.md\n",
      "FrontEnd\n",
      "deploy.sh\n",
      "BackEnd\n",
      "nginx_configuration_example.conf\n",
      "upgrade.sh\n",
      "apache_configuration_example.conf\n",
      ".mypy_cache\n",
      ".git\n",
      "wsgi.conf\n",
      "installation.md\n",
      ".editorconfig\n",
      "Dockerfile\n",
      "build_docker.sh\n",
      "run_locally.sh\n",
      ".dockerignore\n",
      "Repo: downloaded/jfcherng__Sublime-VisualizeZeroWidthChars\n",
      "messages\n",
      "dependencies.json\n",
      "docs\n",
      ".flake8\n",
      "boot.py\n",
      "scripts\n",
      ".gitignore\n",
      "typings\n",
      "mypy.ini\n",
      ".python-version\n",
      "messages.json\n",
      "README.md\n",
      "menus\n",
      "plugin\n",
      ".github\n",
      ".mypy_cache\n",
      ".git\n",
      "pyproject.toml\n",
      "VisualizeZeroWidthChars.sublime-settings\n",
      "CHANGELOG.md\n",
      "requirements.txt\n",
      ".gitattributes\n",
      "LICENSE\n",
      ".editorconfig\n",
      "Repo: downloaded/chaosdorf__mpd-mqtt-gateway\n",
      ".gitignore\n",
      "gateway.py\n",
      ".github\n",
      "Pipfile\n",
      ".mypy_cache\n",
      "server.py\n",
      ".git\n",
      "Dockerfile\n",
      "Pipfile.lock\n",
      "Repo: downloaded/Celeo__Preston\n",
      "preston\n",
      ".gitignore\n",
      "README.md\n",
      "pytest.ini\n",
      "tests\n",
      ".github\n",
      ".mypy_cache\n",
      "poetry.lock\n",
      ".git\n",
      "pyproject.toml\n",
      "LICENSE\n",
      ".vscode\n",
      "Repo: downloaded/andrewscwei__rbc-statement-parser\n",
      "parse_csv.py\n",
      ".gitignore\n",
      "__pycache__\n",
      "README.md\n",
      "parse_tbl.py\n",
      "Pipfile\n",
      ".mypy_cache\n",
      ".git\n",
      "config\n",
      "LICENSE\n",
      "parse_pdf.py\n",
      "utils.py\n",
      "Pipfile.lock\n",
      "Repo: downloaded/devonhollowood__adventofcode\n",
      "2016\n",
      "README.md\n",
      "2019\n",
      "2017\n",
      ".mypy_cache\n",
      "2015\n",
      ".git\n",
      "LICENSE\n",
      "2018\n",
      "2021\n",
      "2020\n",
      "Repo: downloaded/tedle__uitabot\n",
      "scripts\n",
      ".gitignore\n",
      "web-client\n",
      "README.md\n",
      "CONFIG.md\n",
      "config.example.json\n",
      "COMMANDS.md\n",
      ".github\n",
      "bot\n",
      ".mypy_cache\n",
      ".git\n",
      "CHANGELOG.md\n",
      ".gitattributes\n",
      "LICENSE\n",
      "Repo: downloaded/JohnStrunk__ocs-monkey\n",
      "build.sh\n",
      "chaos_runner.py\n",
      ".gitignore\n",
      "mypy.ini\n",
      "log_gather.py\n",
      ".travis\n",
      "README.md\n",
      "setup-env.sh\n",
      "kube.py\n",
      ".github\n",
      "util.py\n",
      "event.py\n",
      ".mypy_cache\n",
      ".git\n",
      "tox.ini\n",
      "osio-workload\n",
      "workload_runner.py\n",
      "requirements.txt\n",
      "oc_in_cluster.sh\n",
      "test_kube.py\n",
      "osio.py\n",
      "LICENSE\n",
      "Dockerfile\n",
      "failure.py\n",
      "helm\n",
      "conftest.py\n",
      "log_gather_ocs.py\n",
      "failure_ocs.py\n",
      ".dockerignore\n",
      "Repo: downloaded/jmanuel1__material-search\n",
      "scripts\n",
      ".gitignore\n",
      "elements\n",
      "README.md\n",
      "package.json\n",
      "images\n",
      "manifest.json\n",
      "robots.txt\n",
      ".mypy_cache\n",
      "polymer.json\n",
      ".git\n",
      "gulpfile.js\n",
      "requirements.txt\n",
      "index.html\n",
      ".gitattributes\n",
      "yarn.lock\n",
      "favicon.ico\n",
      "styles\n",
      "test\n",
      "LICENSE.md\n"
     ]
    }
   ],
   "source": [
    "for repo in weird_repos[:10]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    print(\"Repo:\", rd.relative_to(repos_dir))\n",
    "    for f in rd.iterdir():\n",
    "        print(f.relative_to(rd))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
